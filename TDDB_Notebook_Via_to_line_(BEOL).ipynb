{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guasong/CS125FinalProject/blob/master/TDDB_Notebook_Via_to_line_(BEOL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blcyXJ_J49XR"
      },
      "source": [
        "# Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45uF1F5k6CyR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "5x3L7fZpvkbq",
        "outputId": "b6a8ad5c-85dd-4103-adb0-3b4cd029910f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting reliability\n",
            "  Downloading reliability-0.8.6-py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.8/dist-packages (from reliability) (1.21.6)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.8/dist-packages (from reliability) (0.17.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from reliability) (1.7.3)\n",
            "Collecting mplcursors>=0.3\n",
            "  Downloading mplcursors-0.5.2.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.0/89.0 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from reliability) (1.3.5)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.8/dist-packages (from reliability) (1.5)\n",
            "Collecting autograd-gamma>=0.5.0\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib>=3.5.0\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.3->reliability) (0.16.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (7.1.2)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.0/296.0 KB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (1.4.4)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->reliability) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.2->reliability) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->reliability) (1.15.0)\n",
            "Building wheels for collected packages: autograd-gamma, mplcursors\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=6c05a2a4e223329aa7bf0801357bbb7701fc3fdc8792e426c71801ccb2d2909c\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/a2/b6/582cfdfbeeccd469504a01af3bb952fd9e7eccba40995eafea\n",
            "  Building wheel for mplcursors (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mplcursors: filename=mplcursors-0.5.2-py3-none-any.whl size=21069 sha256=965dc6067d7eeabf26bd55b2fc24c37982d73e68d70925e34ff12ffe4fdca12c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/02/b1/7d5c397daf88f75d3432168edddd8b02d3ae2c050ffa7ca26f\n",
            "Successfully built autograd-gamma mplcursors\n",
            "Installing collected packages: fonttools, contourpy, matplotlib, autograd-gamma, mplcursors, reliability\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed autograd-gamma-0.5.0 contourpy-1.0.6 fonttools-4.38.0 matplotlib-3.6.2 mplcursors-0.5.2 reliability-0.8.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install reliability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g7plArDy6DvD"
      },
      "outputs": [],
      "source": [
        "import numpy as np                             # for fast array manipulation\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch                                   # Pytorch\n",
        "import torchvision                             # contains image datasets and many functions to manipulate images\n",
        "import torchvision.transforms as transforms    # to normalize, scale etc the dataset\n",
        "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
        "from torch.utils.data import Dataset        # to load data into batches (for SGD)\n",
        "from torchvision.utils import make_grid        # Plotting. Makes a grid of tensors\n",
        "from torchvision.datasets import MNIST         # the classic handwritten digits dataset\n",
        "import matplotlib.pyplot as plt                # to plot our images\n",
        "import torch.nn as nn                          # Class that implements a model (such as a Neural Network)\n",
        "import torch.nn.functional as F                # contains activation functions, sampling layers and more \"functional\" stuff\n",
        "import torch.optim as optim                    # For optimization routines such as SGD, ADAM, ADAGRAD, etc\n",
        "from scipy import stats\n",
        "from torch.distributions import Weibull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5DMPv_2-Ur"
      },
      "source": [
        "## Set up the Device (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iry_rIc13BM-",
        "outputId": "d5b1c960-c742-43c9-e3d4-394a47b1c941"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XWufAQTE5FfC"
      },
      "outputs": [],
      "source": [
        "# # Set up Transformations (0-1 Scaling only)\n",
        "# # DO NOT NORMALIZE WITH VAE. NORMALIZING WITH MEAN 0 AND STD 1 MEANS WE WILL HAVE\n",
        "# # NEGATIVE PIXEL VALUES AND THIS IS NOT ALLOWED SINCE WE ARE USING A BINARY\n",
        "# # CROSS ENTROPY LOSS (I.E. BERNOULLI). WE NEED THE PIXEL VALUES TO BE INTERPRETABLE AS \n",
        "# # PROBABILITIES \n",
        "# t = transforms.Compose([\n",
        "#                         transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "# # Use transformation for both training and test set\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=t)\n",
        "# testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=t)\n",
        "\n",
        "# # Load train and test set\n",
        "# trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "# testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtN_6xbn5G8J"
      },
      "source": [
        "## Load MNIST DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "cQ-L63FJtpjG",
        "outputId": "3374f320-4b31-45fa-83f2-2a5c3e308368"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e394989-99e7-45cb-b268-f0884c5dcc3d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e394989-99e7-45cb-b268-f0884c5dcc3d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset4.csv to dataset4.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V12LI8Fw5311"
      },
      "source": [
        "## Global Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2])\n",
        "x = a.reshape([2,1])\n",
        "b = torch.tensor([[1,2,3,4],[1,2,3,4]])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYwMrpJJy-W",
        "outputId": "a17a47fa-b10f-498d-ec8e-1a5df018d659"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "MhsJ6wWG55z-"
      },
      "outputs": [],
      "source": [
        "batch_size = 2      # How many images to use for a SGD update\n",
        "L = 1                 # Samples per data point. See section \"Likelihood Lower Bound\".\n",
        "e_hidden = 20        # Number of hidden units in the encoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
        "e2_hidden = 25       # Second layer hidden in encoder\n",
        "e3_hidden = 20        # Third layer\n",
        "d_hidden = 500        # Number of hidden units in the decoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
        "d2_hidden = 250       # Second layer hidden in decoder\n",
        "d3_hidden = 50        # Third layer\n",
        "latent_dim = 10        # Chosen based on AEVB paper, page 7, section \"Marginal Likelihood\"\n",
        "gaussian_num = 2      # Number of gaussian distributions to mix\n",
        "learning_rate = 0.0003 # For SGD\n",
        "weight_decay = 1e-5   # For SGD\n",
        "epochs = 300          # Number of sweeps through the whole dataset, also called epochs\n",
        "input_size = 7\n",
        "# area1_point = 718\n",
        "# area2_point = 723\n",
        "# area3_point = 729\n",
        "# censor_point_0=22\n",
        "# censor_point_1=7\n",
        "# censor_point_2=6\n",
        "\n",
        "area1_point = 800\n",
        "area2_point = 800\n",
        "area3_point = 800\n",
        "censor_point_0=6\n",
        "censor_point_1=0\n",
        "censor_point_2=0\n",
        "\n",
        "censor_percent_0=censor_point_0/area1_point\n",
        "censor_percent_1=censor_point_1/area2_point\n",
        "censor_percent_2=censor_point_2/area3_point\n",
        "m=20\n",
        "loss_tuning = 3\n",
        "mu_p=torch.tensor([18.8, 18.7])\n",
        "var_p = torch.tensor([0.2, 1])\n",
        "logvar_p=torch.log(var_p)\n",
        "pi_p = torch.tensor([0.5, 0.5])\n",
        "logpi_p=torch.log(torch.tensor([0.5, 0.5]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "from math import floor\n",
        "rng = np.random.default_rng()"
      ],
      "metadata": {
        "id": "aMPJ6y7usnyr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "pxi8vVD82uJx"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "df = pd.read_csv(io.StringIO(uploaded['dataset4.csv'].decode('utf-8')), index_col=None, header=None)\n",
        "tbd_max = df.iloc[:,:3].max().max()\n",
        "tbd_min = df.iloc[:,:3].min().min()\n",
        "i_max = df.iloc[:,3:6].max().max()\n",
        "i_min = df.iloc[:,3:6].min().min()\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        " \n",
        "    area1=df.iloc[censor_point_0:area1_point,0].values.flatten()\n",
        "    current1=df.iloc[censor_point_0:area1_point,3].values.flatten()\n",
        "    x=np.stack((area1, current1), axis=1)[:area1_point]\n",
        "    np.random.shuffle(x)\n",
        "    random_point_num = ceil(len(x)/input_size)*input_size-len(x)\n",
        "    random_sample = rng.choice(x, random_point_num, replace=False)\n",
        "    x=np.append(random_sample, x, axis=0)\n",
        "    current1=x[:,1].reshape(-1,input_size)\n",
        "    area1=x[:,0].reshape(-1,input_size)\n",
        "\n",
        "    area2=df.iloc[censor_point_1:area2_point,1].values.flatten()\n",
        "    current2=df.iloc[censor_point_1:area2_point,4].values.flatten()\n",
        "    x=np.stack((area2, current2), axis=1)\n",
        "    np.random.shuffle(x)\n",
        "    random_point_num = ceil(len(x)/input_size)*input_size-len(x)\n",
        "    random_sample = rng.choice(x, random_point_num, replace=False)\n",
        "    x=np.append(random_sample, x, axis=0)\n",
        "    current2=x[:,1].reshape(-1,input_size)\n",
        "    area2=x[:,0].reshape(-1,input_size)\n",
        "\n",
        "\n",
        "    area3=df.iloc[censor_point_2:area3_point,2].values.flatten()\n",
        "    current3=df.iloc[censor_point_2:area3_point,5].values.flatten()\n",
        "    x=np.stack((area3, current3), axis=1)\n",
        "    np.random.shuffle(x)\n",
        "    random_point_num = ceil(len(x)/input_size)*input_size-len(x)\n",
        "    random_sample = rng.choice(x, random_point_num, replace=False)\n",
        "    x=np.append(random_sample, x, axis=0)\n",
        "    current3=x[:,1].reshape(-1,input_size)\n",
        "    area3=x[:,0].reshape(-1,input_size)\n",
        "\n",
        "    y1=np.full(area1.shape[0], 100)\n",
        "    y2=np.full(area2.shape[0], 10)\n",
        "    y3=np.full(area3.shape[0], 1)\n",
        "    \n",
        "    x = np.append(area1, area2, axis=0)\n",
        "    y = np.append(y1, y2, axis=0)\n",
        "    w = np.append(current1, current2, axis=0)\n",
        "    \n",
        "    x = np.append(x, area3, axis=0)\n",
        "    y = np.append(y, y3, axis=0)\n",
        "    w = np.append(w, current3, axis=0)\n",
        "\n",
        "#   Train without area1\n",
        "    # x = np.append(area2, area3, axis=0)\n",
        "    # y = np.append(y2, y3, axis=0)\n",
        "    # w = np.append(current2, current3, axis=0)\n",
        "    \n",
        "#   adjust size to match epoch\n",
        "\n",
        "    x = np.append(x, x, axis=0)\n",
        "    y = np.append(y, y, axis=0)\n",
        "    w = np.append(w, w, axis=0)\n",
        "\n",
        "\n",
        "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
        "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
        "    self.w_train=torch.tensor(w,dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx],self.y_train[idx],self.w_train[idx]\n",
        "\n",
        "myDs=MyDataset(df)\n",
        "trainloader=DataLoader(myDs,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjCB4POb6RYO"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "i3P2Axt_DXB2"
      },
      "outputs": [],
      "source": [
        "# My custom layer\n",
        "class constant_layer(nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
        "  def forward(self, input):\n",
        "    return self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "w0gSmpGh6Aqf"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Encoder Network. Must inherit from `nn.Module` provided by Pytorch. We only need to define 2 things:\n",
        "    \n",
        "    1) The components of the network (layers, activation functions, etc). This is done in __init__().\n",
        "    2) How the network uses such components to transform the network input into an output. This is done in a method called `forward()`.\n",
        "    \"\"\"\n",
        "    super(Encoder, self).__init__()\n",
        "    # Define Fully-Connected FeedForward Connections\n",
        "    self.hidden = nn.Linear(in_features=input_size*2+1, out_features=e_hidden)\n",
        "    self.hidden2 = nn.Linear(in_features=e_hidden, out_features=e2_hidden)\n",
        "    self.hidden3 = nn.Linear(in_features=e2_hidden, out_features=e3_hidden)\n",
        "    self.area_hidden = nn.Linear(in_features=2, out_features=e_hidden)\n",
        "    self.area_hidden2 = nn.Linear(in_features=e_hidden, out_features=e2_hidden)\n",
        "    self.area_hidden3 = nn.Linear(in_features=e2_hidden, out_features=e3_hidden)\n",
        "    # We need two separate layers. One is used for mu one is used for logvar.\n",
        "    self.logmu1_layer     = nn.Linear(in_features=e3_hidden, out_features=1)\n",
        "    self.logvar1_layer = nn.Linear(in_features=e3_hidden, out_features=1)\n",
        "    self.logmu2_layer     = nn.Linear(in_features=e3_hidden, out_features=1)\n",
        "    self.logvar2_layer = nn.Linear(in_features=e3_hidden, out_features=1)\n",
        "    self.pi_layer     = nn.Linear(in_features=e3_hidden, out_features=1)   # only need one mixing parameter for bimodal GMM\n",
        "    self.loga_layer     = nn.Linear(in_features=e3_hidden, out_features=1)                # two out features for a and b\n",
        "    self.logb_layer     = nn.Linear(in_features=e3_hidden, out_features=1)                # two out features for a and b\n",
        "  def forward(self, x, area, current):\n",
        "    \"\"\"Defines how the network transforms the input x into an encoded representation.\"\"\"\n",
        "    # Pass input through the first set of connections\n",
        "    x = (x-tbd_min)/(tbd_max-tbd_min)\n",
        "    current = (current-i_min)/(i_max-i_min)\n",
        "    x, indices = torch.sort(x)\n",
        "    for i in range(len(x)):\n",
        "      current[i] = current[i][indices[i]]\n",
        "    x = torch.cat((x,current,torch.log10(area).reshape(batch_size,1)), 1)\n",
        "    x1 = F.relu(self.hidden(x))\n",
        "    x2 = F.relu(self.hidden2(x1))\n",
        "    x3 = F.relu(self.hidden3(x2))\n",
        "    # a = F.relu(self.area_hidden(torch.log10(a)))\n",
        "    # a = F.relu(self.area_hidden2(a))\n",
        "    # a = F.relu(self.area_hidden3(a))\n",
        "    y = x3\n",
        "    # Now pass it to one set of connections to get mu, and to another set of connections \n",
        "    # to get logvar\n",
        "    pi = self.pi_layer(y)\n",
        "\n",
        "    a = self.loga_layer(y).repeat(1,input_size)\n",
        "    a = torch.exp(a)\n",
        "    b = self.logb_layer(y).repeat(1,input_size)\n",
        "    b = torch.exp(b)\n",
        "    # a = self.loga_layer(y).repeat(1,input_size)\n",
        "    # a = F.sigmoid(a)\n",
        "    # b = self.logb_layer(y).repeat(1,input_size)\n",
        "    # b = F.sigmoid(b)\n",
        "    logmu1 = self.logmu1_layer(y).repeat(1,input_size)+3\n",
        "    logvar1 = self.logvar1_layer(y).repeat(1,input_size)+1.5\n",
        "    logmu2 = self.logmu2_layer(y).repeat(1,input_size)+3\n",
        "    logvar2 = self.logvar2_layer(y).repeat(1,input_size)+1.5\n",
        "    return logmu1, logvar1, logmu2, logvar2, torch.sigmoid(pi), a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bXNbMiRBZSK"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "eEk2RsnCBYHV"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, T_630, A0, m=20.0):\n",
        "    \"\"\"\n",
        "    Decoder Network. Works similarly to the encoder, except it takes an input from the latent space\n",
        "    and then outputs an image.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "    # Define Fully-Connected FeedForward Connections\n",
        "    # self.hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
        "    # self.batchnorm = nn.Batchnorm1d(num_features=d3_hidden)\n",
        "    # self.hidden2 = nn.Linear(in_features=d_hidden, out_features=d2_hidden)\n",
        "    # self.hidden3 = nn.Linear(in_features=d2_hidden, out_features=d3_hidden)\n",
        "    # Second set of FC connections. Here we only want one output\n",
        "    # self.output_layer = nn.Linear(in_features=d_hidden, out_features=input_size)\n",
        "    self.T_630 = T_630\n",
        "    self.m = 20.0\n",
        "    self.A0 = A0\n",
        "\n",
        "  def forward(self, z, a, b, mu, A):\n",
        "    \"\"\"Defines how the network transforms the latent input z into a flatten image.\"\"\"\n",
        "    # Notice that we use a sigmoid function at the end to restrict output values between \n",
        "    # 0 and 1 so they can be interpreted as probabilities (?)\n",
        "    # Reason for unphysical!!!!\n",
        "    beta = a+b*z*0.3*0.5\n",
        "    area = A.repeat(input_size)\n",
        "    area = area.reshape((-1, batch_size)).T\n",
        "    T63 = self.T_630*(self.A0/area)**(1/beta)*(z/mu)**self.m\n",
        "    return beta, T63"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkstd46tCy-N"
      },
      "source": [
        "## Variational Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "f7O0F1sNC0eI"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, T_630, A0):\n",
        "    \"\"\"Puts together Encoder & Decoder with the reparametrization trick.\"\"\"\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder(T_630, A0, m=20.0)\n",
        "\n",
        "  def sample_latent(self, logmu1, logvar1, logmu2, logvar2, pi):\n",
        "    # Get standard normal in the shape of mu  \n",
        "    eps1 = torch.randn(batch_size, input_size)\n",
        "    eps2 = torch.randn(batch_size, input_size)\n",
        "    # Use logarithmic properties to transform logvar to std. Then multiply\n",
        "    # and sum by latent mu\n",
        "    # mu = batch*1\n",
        "    mu1 = torch.exp(logmu1)\n",
        "    mu2 = torch.exp(logmu2)\n",
        "    first_z = eps1.mul(torch.exp(0.5*logvar1)).add_(mu1)\n",
        "    second_z = eps2.mul(torch.exp(0.5*logvar2)).add_(mu2)\n",
        "    mixed_z = torch.tensor([])\n",
        "    select = torch.rand(batch_size, 1)\n",
        "    for i in range(batch_size):\n",
        "      if select[i][0] < pi[i][0]:\n",
        "        mixed_z = torch.cat((mixed_z, first_z[i]), 0)\n",
        "      else:\n",
        "        mixed_z = torch.cat((mixed_z, second_z[i]), 0)\n",
        "    mixed_z = mixed_z.reshape(batch_size, input_size)\n",
        "    gmm_mu = mu1*pi+mu2*(1-pi)\n",
        "    var1 = torch.exp(logvar1)\n",
        "    var2 = torch.exp(logvar2)\n",
        "    gmm_var = var1.mul(pi)+var2.mul(1-pi)+pi*(1-pi)*((mu1-mu2)**2)\n",
        "    gmm_logvar = torch.log(gmm_var)\n",
        "    return mixed_z, gmm_mu, gmm_logvar\n",
        "\n",
        "  def forward(self, x, A, current):\n",
        "    \"\"\"Transforms image into latent and then back to its reconstruction.\"\"\"\n",
        "    # Feed image to encoder. Obtain mean and logvar for the latent space\n",
        "    logmu1, logvar1, logmu2, logvar2, latent_pi, a, b = self.encoder(x.view(-1, input_size), A, current)\n",
        "    # Sample from the latent space with the give mean and variance using the \n",
        "    # reparametrization trick\n",
        "    z, gmm_mu, gmm_logvar = self.sample_latent(logmu1, logvar1, logmu2, logvar2, latent_pi)\n",
        "    # problem\n",
        "    # Decode the latent representation\n",
        "    beta, T63 = self.decoder(z.view(-1, input_size), a, b, gmm_mu, A)\n",
        "    return beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi, a, b   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAFLfInLFTvu"
      },
      "source": [
        "## VAE Loss Function (KL + Reconstruction Loss)\n",
        "Remember from the [slides](https://neuralnetworksbristol.netlify.app/VAE_Mauro.pdf) that maximizing the evidence can be approximately done by maximizing the ELBO (Evidence Lower Bound) which can be written as a difference of two terms:\n",
        "\n",
        "$$\n",
        "\\newcommand{\\Ebb}{\\mathbb{E}}\n",
        "\\newcommand{\\vect}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
        "\\newcommand{\\vphi}{\\vect{\\phi}}\n",
        "\\newcommand{\\vtheta}{\\vect{\\theta}}\n",
        "\\newcommand{\\vx}{\\vect{x}}\n",
        "\\newcommand{\\vz}{\\vect{z}}\n",
        "\\newcommand{\\kl}[2]{\\text{KL}(#1\\,\\,||\\,\\,#2)}\n",
        "\\newcommand{\\encoder}{q_{\\vphi}(\\vz \\mid \\vx)}\n",
        "\\newcommand{\\elbo}{\\mathcal{L}_{\\theta, \\vphi}(\\vx)}\n",
        "\\newcommand{\\vepsilon}{\\vect{\\epsilon}}\n",
        "\\newcommand{\\iid}{\\overset{\\text{i.i.d.}}{\\sim}}\n",
        "\\elbo =\n",
        "\\underbrace{\\Ebb_{q_{\\vphi}}\\left[\\log p_{\\vtheta}(\\vx\\mid \\vz)\\right]}_{\\text{Expected Log-Likelihood}} - \\underbrace{\\kl{\\encoder}{p_{\\vtheta}(\\vz)}}_{\\text{Regularization Term}}\n",
        "$$\n",
        "\n",
        "In the case of both $q_{\\vphi}$ and $\\encoder$ being Gaussians the [AEVB paper](https://arxiv.org/pdf/1312.6114.pdf) shows that the following formula is available\n",
        "\n",
        "$$\n",
        "-\\kl{q_{\\vphi}}{\\encoder} = \\frac{1}{2}\\sum_{j=1}^J \\left[1 + \\log\\sigma_j^2  - \\mu_j^2 - \\sigma_j^2\\right]\n",
        "$$\n",
        "where $J$ is the dimensionality of $\\vz$ which, in our case, is denoted `latent_dim`.\n",
        "\n",
        "Then an unbiased estimate for the Objective is given below:\n",
        "\n",
        "$$\n",
        "\\widetilde{\\mathcal{L}}_{\\vtheta, \\vphi}(\\vx) = \\displaystyle \\frac{1}{L}\\sum_{i=1}^L \\left[\\log p_{\\vtheta}(\\vx \\mid g_{\\vphi}(\\vepsilon^{(i)}, \\vx))\\right] - \\frac{1}{2}\\sum_{j=1}^J \\left[1 + \\log\\sigma_j^2  - \\mu_j^2 - \\sigma_j^2\\right] \\qquad \\text{where } \\vepsilon^{(i)} \\iid \\mathcal{N}(\\vect{0}, \\vect{I})\n",
        "$$\n",
        "\n",
        "Where $L=$ `L` and $J=$ `latent_dim`. \n",
        "\n",
        "Recall from Slide $14$ that in the case of Bernoulli variables the log-likelihood becomes\n",
        "\n",
        "$$\n",
        "\\log p_{\\vtheta}(\\vx\\mid \\vz) = \\sum_{j} x_j \\log p_j + (1 - x_j) \\log(1 - p_j)\n",
        "$$\n",
        "Since we want our pixel values to be between `0` and `1` this is exactly what we are looking for and thankfully is already implemented in Pytorch under the name of `binary_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "_QCUmQ-gFSEA"
      },
      "outputs": [],
      "source": [
        "def vae_loss(image, beta, T63, logmu1, logmu2, logvar1, logvar2, pi):\n",
        "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
        "  # Compute the binary_crossentropy.\n",
        "  # Notice that we reshape them because in practice we don't receive just 1 image and 1 reconstruction, but we receive a whole batch!\n",
        "  # reconstruction_loss = F.cross_entropy(input=reconstruction.view(-1, input_size), target=image.view(-1, input_size), reduction='sum')\n",
        "  reconstruction_loss = torch.sum(torch.log(beta)-beta*torch.log(T63)+(beta-1)*torch.log(image)-(image/T63)**beta)\n",
        "  # Compute KL divergence using formula (closed-form)\n",
        "  mu1=torch.exp(logmu1)\n",
        "  mu2=torch.exp(logmu2)\n",
        "  pi1 = pi\n",
        "  pi2 = 1-pi\n",
        "  var1 = torch.exp(logvar1)\n",
        "  var2 = torch.exp(logvar2)\n",
        "  kl1 = logvar_p[0]+logpi_p[0]-logvar1-torch.log(pi1)-0.5+0.5*(var1**2+(mu1-mu_p[0])**2)/var_p[0]**2\n",
        "  kl2 = logvar_p[1]+logpi_p[1]-logvar2-torch.log(pi2)-0.5+0.5*(var2**2+(mu2-mu_p[1])**2)/var_p[1]**2\n",
        "  kl = torch.sum(kl1*pi+kl2*(1-pi))  # upperbound mentioned by Olsen's paper\n",
        "  MSE_loss = torch.sum((mu1*pi1+mu2*pi2-torch.tensor([32]).repeat(batch_size*input_size).reshape(batch_size, input_size)))**2\n",
        "  total_loss = abs(reconstruction_loss)+10*MSE_loss\n",
        "  return total_loss, MSE_loss\n",
        "  # return total_loss, reconstruction_loss, kl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzEMDG6YnaGZ"
      },
      "source": [
        "## VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adQadEJSnBpk",
        "outputId": "1a511463-b42a-4f51-d44f-156cb1877427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): Linear(in_features=15, out_features=20, bias=True)\n",
              "    (hidden2): Linear(in_features=20, out_features=25, bias=True)\n",
              "    (hidden3): Linear(in_features=25, out_features=20, bias=True)\n",
              "    (area_hidden): Linear(in_features=2, out_features=20, bias=True)\n",
              "    (area_hidden2): Linear(in_features=20, out_features=25, bias=True)\n",
              "    (area_hidden3): Linear(in_features=25, out_features=20, bias=True)\n",
              "    (logmu1_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (logvar1_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (logmu2_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (logvar2_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (pi_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (loga_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "    (logb_layer): Linear(in_features=20, out_features=1, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "# Instantiate VAE\n",
        "vae = VAE(T_630=217, A0=10.0)\n",
        "# vae = VAE(T_630=657.5, A0=10.0)\n",
        "# vae = VAE(T_630=1163.0, A0=1.0)\n",
        "# Pass VAE to the device (GPU or CPU)\n",
        "vae = vae.to(device)\n",
        "\n",
        "# Use Stochastic Gradient Descent as optimizer\n",
        "optimizer = optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# Set VAE to training mode\n",
        "vae.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwlTIQu-pHKX",
        "outputId": "dd9d5424-a61b-4c0e-d26e-3ac5917fcb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 300] average total error: 1259179844775.709717 , average MSE loss: 11785312.765625 , average reconstruction error: 1259179844775.709717\n",
            "Epoch [2 / 300] average total error: 131737313879993.000000 , average MSE loss: 11698574.761719 , average reconstruction error: 131737313879993.000000\n",
            "Epoch [3 / 300] average total error: 47761369236.271896 , average MSE loss: 11512268.617188 , average reconstruction error: 47761369236.271896\n",
            "Epoch [4 / 300] average total error: 47763878377.715660 , average MSE loss: 11488964.988281 , average reconstruction error: 47763878377.715660\n",
            "Epoch [5 / 300] average total error: 310768039043.136475 , average MSE loss: 11458496.560547 , average reconstruction error: 310768039043.136475\n",
            "Epoch [6 / 300] average total error: 2824966193.653162 , average MSE loss: 11439332.617188 , average reconstruction error: 2824966193.653162\n",
            "Epoch [7 / 300] average total error: 477139407353.955139 , average MSE loss: 11343059.986328 , average reconstruction error: 477139407353.955139\n",
            "Epoch [8 / 300] average total error: 1151287198051.791504 , average MSE loss: 11264429.582031 , average reconstruction error: 1151287198051.791504\n",
            "Epoch [9 / 300] average total error: 67650907934.254814 , average MSE loss: 11163892.878906 , average reconstruction error: 67650907934.254814\n",
            "Epoch [10 / 300] average total error: 402948953134.695068 , average MSE loss: 11109638.357422 , average reconstruction error: 402948953134.695068\n",
            "Epoch [11 / 300] average total error: 23262251643.682232 , average MSE loss: 11072837.119141 , average reconstruction error: 23262251643.682232\n",
            "Epoch [12 / 300] average total error: 711466448653.714111 , average MSE loss: 11037290.904297 , average reconstruction error: 711466448653.714111\n",
            "Epoch [13 / 300] average total error: 1693183828392.864014 , average MSE loss: 10933434.375000 , average reconstruction error: 1693183828392.864014\n",
            "Epoch [14 / 300] average total error: 49226259389.327942 , average MSE loss: 10902697.257812 , average reconstruction error: 49226259389.327942\n",
            "Epoch [15 / 300] average total error: 220900762917.007172 , average MSE loss: 10878672.033203 , average reconstruction error: 220900762917.007172\n",
            "Epoch [16 / 300] average total error: 103703240345.467300 , average MSE loss: 10773608.154297 , average reconstruction error: 103703240345.467300\n",
            "Epoch [17 / 300] average total error: 2184833451554.978271 , average MSE loss: 10670909.892578 , average reconstruction error: 2184833451554.978271\n",
            "Epoch [18 / 300] average total error: 23600830381.011448 , average MSE loss: 10636635.345703 , average reconstruction error: 23600830381.011448\n",
            "Epoch [19 / 300] average total error: 6595575362656.869141 , average MSE loss: 10350131.818359 , average reconstruction error: 6595575362656.869141\n",
            "Epoch [20 / 300] average total error: 16792607751.928869 , average MSE loss: 10166123.599609 , average reconstruction error: 16792607751.928869\n",
            "Epoch [21 / 300] average total error: 28532442523.258720 , average MSE loss: 10152394.810547 , average reconstruction error: 28532442523.258720\n",
            "Epoch [22 / 300] average total error: 30554471118.855286 , average MSE loss: 10106339.248047 , average reconstruction error: 30554471118.855286\n",
            "Epoch [23 / 300] average total error: 4812041139.135447 , average MSE loss: 10088062.429688 , average reconstruction error: 4812041139.135447\n",
            "Epoch [24 / 300] average total error: 3551962380853.104492 , average MSE loss: 10071844.205078 , average reconstruction error: 3551962380853.104492\n",
            "Epoch [25 / 300] average total error: 741374031930.157715 , average MSE loss: 9783859.566406 , average reconstruction error: 741374031930.157715\n",
            "Epoch [26 / 300] average total error: 477158984.935229 , average MSE loss: 9776872.218750 , average reconstruction error: 477158984.935229\n",
            "Epoch [27 / 300] average total error: 6778803233.744640 , average MSE loss: 9776679.435547 , average reconstruction error: 6778803233.744640\n",
            "Epoch [28 / 300] average total error: 31380348773.447403 , average MSE loss: 9770287.230469 , average reconstruction error: 31380348773.447403\n",
            "Epoch [29 / 300] average total error: 53474967217.058685 , average MSE loss: 9760969.068359 , average reconstruction error: 53474967217.058685\n",
            "Epoch [30 / 300] average total error: 11489314669.419695 , average MSE loss: 9750588.109375 , average reconstruction error: 11489314669.419695\n",
            "Epoch [31 / 300] average total error: 2694528564.235556 , average MSE loss: 9748601.250000 , average reconstruction error: 2694528564.235556\n",
            "Epoch [32 / 300] average total error: 15111659140.949583 , average MSE loss: 9745952.251953 , average reconstruction error: 15111659140.949583\n",
            "Epoch [33 / 300] average total error: 24550822282.118824 , average MSE loss: 9724330.314453 , average reconstruction error: 24550822282.118824\n",
            "Epoch [34 / 300] average total error: 98640479290.508987 , average MSE loss: 9704708.375000 , average reconstruction error: 98640479290.508987\n",
            "Epoch [35 / 300] average total error: 1739847774.886810 , average MSE loss: 9642045.642578 , average reconstruction error: 1739847774.886810\n",
            "Epoch [36 / 300] average total error: 818519072.347838 , average MSE loss: 9640077.335938 , average reconstruction error: 818519072.347838\n",
            "Epoch [37 / 300] average total error: 570250885.638081 , average MSE loss: 9638563.558594 , average reconstruction error: 570250885.638081\n",
            "Epoch [38 / 300] average total error: 5608994552.925327 , average MSE loss: 9632725.490234 , average reconstruction error: 5608994552.925327\n",
            "Epoch [39 / 300] average total error: 2634118222.375363 , average MSE loss: 9627638.898438 , average reconstruction error: 2634118222.375363\n",
            "Epoch [40 / 300] average total error: 7922652087.155160 , average MSE loss: 9613219.015625 , average reconstruction error: 7922652087.155160\n",
            "Epoch [41 / 300] average total error: 30126201211.614826 , average MSE loss: 9498967.818359 , average reconstruction error: 30126201211.614826\n",
            "Epoch [42 / 300] average total error: 2831766794.585074 , average MSE loss: 9403737.976562 , average reconstruction error: 2831766794.585074\n",
            "Epoch [43 / 300] average total error: 9532987577.397120 , average MSE loss: 9386216.593750 , average reconstruction error: 9532987577.397120\n",
            "Epoch [44 / 300] average total error: 8465092841896.728516 , average MSE loss: 9082696.513672 , average reconstruction error: 8465092841896.728516\n",
            "Epoch [45 / 300] average total error: 4105145237.462618 , average MSE loss: 8719404.097656 , average reconstruction error: 4105145237.462618\n",
            "Epoch [46 / 300] average total error: 163034900625.282562 , average MSE loss: 8685657.554688 , average reconstruction error: 163034900625.282562\n",
            "Epoch [47 / 300] average total error: 381133248.759811 , average MSE loss: 8607295.341797 , average reconstruction error: 381133248.759811\n",
            "Epoch [48 / 300] average total error: 1642882406.857240 , average MSE loss: 8548241.367188 , average reconstruction error: 1642882406.857240\n",
            "Epoch [49 / 300] average total error: 524885128.364962 , average MSE loss: 8471826.076172 , average reconstruction error: 524885128.364962\n",
            "Epoch [50 / 300] average total error: 2304483612.054233 , average MSE loss: 8417449.611328 , average reconstruction error: 2304483612.054233\n",
            "Epoch [51 / 300] average total error: 484156967.920422 , average MSE loss: 8387362.912109 , average reconstruction error: 484156967.920422\n",
            "Epoch [52 / 300] average total error: 1562552949.029615 , average MSE loss: 8370520.460938 , average reconstruction error: 1562552949.029615\n",
            "Epoch [53 / 300] average total error: 315463569.281931 , average MSE loss: 8349790.990234 , average reconstruction error: 315463569.281931\n",
            "Epoch [54 / 300] average total error: 341641396.536519 , average MSE loss: 8337890.849609 , average reconstruction error: 341641396.536519\n",
            "Epoch [55 / 300] average total error: 84365907.234511 , average MSE loss: 8335764.947266 , average reconstruction error: 84365907.234511\n",
            "Epoch [56 / 300] average total error: 42321306.016443 , average MSE loss: 8333626.416016 , average reconstruction error: 42321306.016443\n",
            "Epoch [57 / 300] average total error: 33777819.570812 , average MSE loss: 8329367.396484 , average reconstruction error: 33777819.570812\n",
            "Epoch [58 / 300] average total error: 517959651.248774 , average MSE loss: 8320136.894531 , average reconstruction error: 517959651.248774\n",
            "Epoch [59 / 300] average total error: 275245522.328625 , average MSE loss: 8300720.947266 , average reconstruction error: 275245522.328625\n",
            "Epoch [60 / 300] average total error: 61343087.376544 , average MSE loss: 8276044.189453 , average reconstruction error: 61343087.376544\n",
            "Epoch [61 / 300] average total error: 173039832.596975 , average MSE loss: 8258066.107422 , average reconstruction error: 173039832.596975\n",
            "Epoch [62 / 300] average total error: 11735715.303052 , average MSE loss: 8253088.591797 , average reconstruction error: 11735715.303052\n",
            "Epoch [63 / 300] average total error: 145922688.297965 , average MSE loss: 8249003.769531 , average reconstruction error: 145922688.297965\n",
            "Epoch [64 / 300] average total error: 40395365.718341 , average MSE loss: 8244605.292969 , average reconstruction error: 40395365.718341\n",
            "Epoch [65 / 300] average total error: 18217408.990053 , average MSE loss: 8241059.644531 , average reconstruction error: 18217408.990053\n",
            "Epoch [66 / 300] average total error: 89224413.458121 , average MSE loss: 8231312.447266 , average reconstruction error: 89224413.458121\n",
            "Epoch [67 / 300] average total error: 103285890.906704 , average MSE loss: 8210388.761719 , average reconstruction error: 103285890.906704\n",
            "Epoch [68 / 300] average total error: 1414788720.888036 , average MSE loss: 8071743.917969 , average reconstruction error: 1414788720.888036\n",
            "Epoch [69 / 300] average total error: 148016447.609738 , average MSE loss: 7984896.478516 , average reconstruction error: 148016447.609738\n",
            "Epoch [70 / 300] average total error: 38170661.058639 , average MSE loss: 7955045.259766 , average reconstruction error: 38170661.058639\n",
            "Epoch [71 / 300] average total error: 106796492.231105 , average MSE loss: 7943951.476562 , average reconstruction error: 106796492.231105\n",
            "Epoch [72 / 300] average total error: 790857083.885084 , average MSE loss: 7833900.726562 , average reconstruction error: 790857083.885084\n",
            "Epoch [73 / 300] average total error: 90913967.146121 , average MSE loss: 7790282.919922 , average reconstruction error: 90913967.146121\n",
            "Epoch [74 / 300] average total error: 38202629.121366 , average MSE loss: 7775678.189453 , average reconstruction error: 38202629.121366\n",
            "Epoch [75 / 300] average total error: 872606270.213345 , average MSE loss: 7761545.199219 , average reconstruction error: 872606270.213345\n",
            "Epoch [76 / 300] average total error: 38851349.383358 , average MSE loss: 7566392.707031 , average reconstruction error: 38851349.383358\n",
            "Epoch [77 / 300] average total error: 78279219.037155 , average MSE loss: 7527356.623047 , average reconstruction error: 78279219.037155\n",
            "Epoch [78 / 300] average total error: 8478053.685910 , average MSE loss: 7494232.644531 , average reconstruction error: 8478053.685910\n",
            "Epoch [79 / 300] average total error: 8542626.296103 , average MSE loss: 7485967.783203 , average reconstruction error: 8542626.296103\n",
            "Epoch [80 / 300] average total error: 6186886.478834 , average MSE loss: 7483931.429688 , average reconstruction error: 6186886.478834\n",
            "Epoch [81 / 300] average total error: 9377778.016125 , average MSE loss: 7478619.302734 , average reconstruction error: 9377778.016125\n",
            "Epoch [82 / 300] average total error: 1725496867.984648 , average MSE loss: 7083135.753906 , average reconstruction error: 1725496867.984648\n",
            "Epoch [83 / 300] average total error: 132034374.060274 , average MSE loss: 6902492.890625 , average reconstruction error: 132034374.060274\n",
            "Epoch [84 / 300] average total error: 3407728.626590 , average MSE loss: 6880273.965820 , average reconstruction error: 3407728.626590\n",
            "Epoch [85 / 300] average total error: 70413264.005814 , average MSE loss: 6860825.763672 , average reconstruction error: 70413264.005814\n",
            "Epoch [86 / 300] average total error: 3452276.020258 , average MSE loss: 6860353.049805 , average reconstruction error: 3452276.020258\n",
            "Epoch [87 / 300] average total error: 2651788.718750 , average MSE loss: 6855744.300781 , average reconstruction error: 2651788.718750\n",
            "Epoch [88 / 300] average total error: 16959776.389807 , average MSE loss: 6853380.412109 , average reconstruction error: 16959776.389807\n",
            "Epoch [89 / 300] average total error: 4761629.456077 , average MSE loss: 6844990.978516 , average reconstruction error: 4761629.456077\n",
            "Epoch [90 / 300] average total error: 3220198.435774 , average MSE loss: 6841124.887695 , average reconstruction error: 3220198.435774\n",
            "Epoch [91 / 300] average total error: 8503702.428007 , average MSE loss: 6834572.706055 , average reconstruction error: 8503702.428007\n",
            "Epoch [92 / 300] average total error: 1348099.184866 , average MSE loss: 6832216.424805 , average reconstruction error: 1348099.184866\n",
            "Epoch [93 / 300] average total error: 5371954.401344 , average MSE loss: 6829606.842773 , average reconstruction error: 5371954.401344\n",
            "Epoch [94 / 300] average total error: 7104864.163654 , average MSE loss: 6817451.566406 , average reconstruction error: 7104864.163654\n",
            "Epoch [95 / 300] average total error: 3467856.312364 , average MSE loss: 6802472.734375 , average reconstruction error: 3467856.312364\n",
            "Epoch [96 / 300] average total error: 5782138.413608 , average MSE loss: 6793522.096680 , average reconstruction error: 5782138.413608\n",
            "Epoch [97 / 300] average total error: 1518725.131677 , average MSE loss: 6781464.097656 , average reconstruction error: 1518725.131677\n",
            "Epoch [98 / 300] average total error: 19857811.013036 , average MSE loss: 6745110.895508 , average reconstruction error: 19857811.013036\n",
            "Epoch [99 / 300] average total error: 5874630.924464 , average MSE loss: 6680354.500977 , average reconstruction error: 5874630.924464\n",
            "Epoch [100 / 300] average total error: 1140320.004815 , average MSE loss: 6535640.478516 , average reconstruction error: 1140320.004815\n",
            "Epoch [101 / 300] average total error: 9456868.340298 , average MSE loss: 6404497.159180 , average reconstruction error: 9456868.340298\n",
            "Epoch [102 / 300] average total error: 3636676.651208 , average MSE loss: 6267844.266602 , average reconstruction error: 3636676.651208\n",
            "Epoch [103 / 300] average total error: 1263297.541561 , average MSE loss: 6214730.975586 , average reconstruction error: 1263297.541561\n",
            "Epoch [104 / 300] average total error: 4537746.142396 , average MSE loss: 6110807.146484 , average reconstruction error: 4537746.142396\n",
            "Epoch [105 / 300] average total error: 2091295.071017 , average MSE loss: 5940706.166016 , average reconstruction error: 2091295.071017\n",
            "Epoch [106 / 300] average total error: 8667134.392328 , average MSE loss: 5338865.072266 , average reconstruction error: 8667134.392328\n",
            "Epoch [107 / 300] average total error: 27405734.170104 , average MSE loss: 4858046.887207 , average reconstruction error: 27405734.170104\n",
            "Epoch [108 / 300] average total error: 2374504.153502 , average MSE loss: 4474575.324219 , average reconstruction error: 2374504.153502\n",
            "Epoch [109 / 300] average total error: 712650.307095 , average MSE loss: 4446667.235840 , average reconstruction error: 712650.307095\n",
            "Epoch [110 / 300] average total error: 1598969.371082 , average MSE loss: 4396193.250977 , average reconstruction error: 1598969.371082\n",
            "Epoch [111 / 300] average total error: 826586.056096 , average MSE loss: 4351447.153320 , average reconstruction error: 826586.056096\n",
            "Epoch [112 / 300] average total error: 587923.771689 , average MSE loss: 4318809.562988 , average reconstruction error: 587923.771689\n",
            "Epoch [113 / 300] average total error: 462432.241472 , average MSE loss: 4281300.440918 , average reconstruction error: 462432.241472\n",
            "Epoch [114 / 300] average total error: 1190044.326717 , average MSE loss: 4226306.825684 , average reconstruction error: 1190044.326717\n",
            "Epoch [115 / 300] average total error: 803518.117199 , average MSE loss: 4117962.171387 , average reconstruction error: 803518.117199\n",
            "Epoch [116 / 300] average total error: 176087.502180 , average MSE loss: 4039313.934082 , average reconstruction error: 176087.502180\n",
            "Epoch [117 / 300] average total error: 360988.898858 , average MSE loss: 4007496.642090 , average reconstruction error: 360988.898858\n",
            "Epoch [118 / 300] average total error: 457582.791652 , average MSE loss: 3925898.765137 , average reconstruction error: 457582.791652\n",
            "Epoch [119 / 300] average total error: 329259.587635 , average MSE loss: 3869739.071289 , average reconstruction error: 329259.587635\n",
            "Epoch [120 / 300] average total error: 2003679.382761 , average MSE loss: 3541560.105469 , average reconstruction error: 2003679.382761\n",
            "Epoch [121 / 300] average total error: 279860.301746 , average MSE loss: 3432191.118652 , average reconstruction error: 279860.301746\n",
            "Epoch [122 / 300] average total error: 139094.985227 , average MSE loss: 3347482.027466 , average reconstruction error: 139094.985227\n",
            "Epoch [123 / 300] average total error: 209270.053084 , average MSE loss: 3202568.079102 , average reconstruction error: 209270.053084\n",
            "Epoch [124 / 300] average total error: 102387.062395 , average MSE loss: 2222101.657981 , average reconstruction error: 102387.062395\n",
            "Epoch [125 / 300] average total error: 155825.785150 , average MSE loss: 1543410.164124 , average reconstruction error: 155825.785150\n",
            "Epoch [126 / 300] average total error: 65721.338859 , average MSE loss: 1296824.202117 , average reconstruction error: 65721.338859\n",
            "Epoch [127 / 300] average total error: 51989.126462 , average MSE loss: 1180542.126381 , average reconstruction error: 51989.126462\n",
            "Epoch [128 / 300] average total error: 46083.742084 , average MSE loss: 1170873.182312 , average reconstruction error: 46083.742084\n",
            "Epoch [129 / 300] average total error: 40808.089972 , average MSE loss: 1036379.300224 , average reconstruction error: 40808.089972\n",
            "Epoch [130 / 300] average total error: 39675.360328 , average MSE loss: 1060630.552162 , average reconstruction error: 39675.360328\n",
            "Epoch [131 / 300] average total error: 35554.823461 , average MSE loss: 1065537.752438 , average reconstruction error: 35554.823461\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-224-1410cec50ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Backpropagate the loss. Before doing that, make sure all previously stored gradients are zero (e.g. from previous iterations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Store all losses here\n",
        "losses = []\n",
        "mse_losses = []\n",
        "reconstruction_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # Put a zero into losses. This is where we will cumulatively sum all the losses\n",
        "  # from each batch. After all batches are done, we will divide by the number of batches\n",
        "  # to obtain the average loss per batch\n",
        "  losses.append(0)\n",
        "  mse_losses.append(0)\n",
        "  reconstruction_losses.append(0)\n",
        "\n",
        "  # To compute the number of batches (since it varies depending on dataset size)\n",
        "  # update a counter variable\n",
        "  number_of_batches = 0\n",
        "\n",
        "  # Grab the batch, we are only interested in images not on their labels\n",
        "  for images, area, current in trainloader:\n",
        "    # Store image batch to device\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Set previous gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Feed images through the VAE to obtain their reconstruction\n",
        "    beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi, a, b = vae(images, area, current)\n",
        "\n",
        "    # Compare reconstruction and images via the loss function\n",
        "    # loss, kl, reconstruction_loss = vae_loss(images, beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi)\n",
        "    \n",
        "    loss, mse = vae_loss(images, beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi)\n",
        "    \n",
        "    # Backpropagate the loss. Before doing that, make sure all previously stored gradients are zero (e.g. from previous iterations)\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    # Use the accumulated gradients to do a step in the right direction\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add loss to the cumulative sum\n",
        "    losses[-1] += loss.item()        #.item() grabs the value in a tensor with only 1 value\n",
        "    mse_losses[-1] += mse.item()\n",
        "    # reconstruction_losses[-1] += reconstruction_loss.item()\n",
        "    number_of_batches += 1\n",
        "  \n",
        "  # At the end of all batches divide the total loss for this epoch by the number of \n",
        "  # batches to obtain an average loss per batch\n",
        "  losses[-1] /= number_of_batches\n",
        "  # kl_losses[-1] /= number_of_batches\n",
        "  # reconstruction_losses[-1] /= number_of_batches\n",
        "  print('Epoch [%d / %d] average total error: %f , average MSE loss: %f , average reconstruction error: %f' % (epoch+1, epochs, losses[-1], mse_losses[-1], losses[-1]))    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq4EV0x-v49K"
      },
      "source": [
        "## Visualizing Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "DxDMXVLzvuVc",
        "outputId": "99a9b6dc-c955-4e52-e2c1-2cbbe9c7f0ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(16, 10))\n",
        "plt.subplot(221)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total loss during training')\n",
        "plt.subplot(222)\n",
        "plt.plot(mse_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total MSE loss during training')\n",
        "plt.subplot(223)\n",
        "plt.plot(reconstruction_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total Reconstruction loss during training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd1P6Wsf4ECY"
      },
      "source": [
        "## Evaluate VAE on Test Set\n",
        "\n",
        "To understand why we need to use `.eval()` have a look [here](https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tug8vKUKrEKD",
        "outputId": "d3deaf65-66eb-40ce-9627-ec2d751a770d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta:  tensor([[2.3687, 2.3668, 2.2168, 2.3749, 2.1713, 2.4104, 2.2363],\n",
            "        [1.9778, 1.9371, 1.9105, 1.9536, 1.7616, 1.8533, 1.8504]])\n",
            "T63:  tensor([[481.5851, 470.5987,  73.7068, 517.9283,  40.5443, 782.0988,  94.7594],\n",
            "        [580.6821, 320.8796, 215.7728, 408.9507,  20.0551,  89.5329,  85.4310]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.9153, 2.9983, 2.9682, 3.0464, 2.9403, 2.9996, 2.6797],\n",
            "        [1.7750, 1.8079, 1.7587, 1.7905, 1.7661, 1.8066, 1.8245]])\n",
            "T63:  tensor([[ 856.4897, 1843.3221, 1401.1749, 2836.1562, 1082.6638, 1865.4122,\n",
            "           81.3234],\n",
            "        [  24.4731,   42.1223,   18.5893,   31.6824,   21.0637,   41.2312,\n",
            "           55.1070]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.0240, 1.9013, 1.8286, 1.8833, 1.9331, 1.9826, 1.8224],\n",
            "        [2.6021, 2.8126, 2.6011, 2.8281, 3.0667, 2.9922, 2.8160]])\n",
            "T63:  tensor([[1126.0551,  189.4552,   61.0359,  144.0956,  305.1115,  628.0026,\n",
            "           55.2361],\n",
            "        [  39.8077,  359.6970,   39.3643,  418.7869, 3826.6548, 1969.7200,\n",
            "          371.8564]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.7259, 2.8750, 2.7594, 2.6460, 2.8307, 2.5680, 2.9330],\n",
            "        [2.2755, 2.4714, 2.3013, 2.2851, 2.3359, 2.4169, 2.1520]])\n",
            "T63:  tensor([[ 440.3924, 1890.4725,  616.9104,  192.5076, 1239.4453,   82.8298,\n",
            "         3238.6292],\n",
            "        [ 120.3186, 1219.5406,  165.9138,  135.6627,  253.1018,  657.5005,\n",
            "           24.0477]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.3772, 2.2763, 2.2507, 2.2975, 2.2808, 2.3324, 2.3195],\n",
            "        [2.2547, 2.2377, 2.2935, 2.4152, 2.3317, 2.2151, 2.3286]])\n",
            "T63:  tensor([[452.8242, 133.1244,  96.4752, 173.2344, 140.9042, 265.5686, 226.9418],\n",
            "        [101.1197,  81.4249, 164.4385, 702.7839, 262.5388,  60.8474, 252.7619]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3443, 2.2332, 2.4257, 2.4753, 2.5280, 2.3268, 2.2447],\n",
            "        [2.7567, 2.5088, 2.5364, 2.7371, 2.7184, 2.9852, 2.6661]])\n",
            "T63:  tensor([[ 298.7377,   75.0708,  774.8884, 1356.5688, 2419.3433,  241.8315,\n",
            "           87.0323],\n",
            "        [ 353.7824,   24.1962,   33.2525,  289.6979,  239.2807, 3144.4192,\n",
            "          138.3425]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[3.0331, 2.6232, 2.8855, 2.8455, 2.7292, 2.9964, 3.0225],\n",
            "        [2.3320, 2.2388, 2.3780, 2.2431, 2.3057, 2.4750, 2.3742]])\n",
            "T63:  tensor([[3612.3706,   65.1328,  932.9812,  635.8627,  199.4676, 2601.4446,\n",
            "         3286.7634],\n",
            "        [ 283.2130,   88.9335,  489.2946,   93.9194,  205.4582, 1483.3274,\n",
            "          468.1718]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.8378, 2.9975, 2.7532, 2.7178, 2.8337, 2.8893, 2.9884],\n",
            "        [1.7899, 1.9662, 1.8291, 1.8791, 1.8212, 1.8693, 1.8126]])\n",
            "T63:  tensor([[ 766.1755, 3385.8447,  332.2765,  231.7533,  736.4535, 1252.5250,\n",
            "         3121.2349],\n",
            "        [  33.9450,  517.0068,   64.2437,  140.7153,   56.5898,  121.0533,\n",
            "           49.2129]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7650, 1.7593, 1.7972, 1.7481, 1.8310, 1.8082, 1.8235],\n",
            "        [2.5377, 2.7270, 2.8470, 2.7216, 2.6588, 2.7918, 2.7994]])\n",
            "T63:  tensor([[ 20.9194,  18.9899,  35.7572,  15.6811,  61.8100,  42.7769,  54.8654],\n",
            "        [ 37.1034, 285.3413, 939.1589, 269.9435, 140.2296, 548.0500, 590.4129]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3116, 2.2945, 2.3620, 2.1988, 2.2859, 2.2125, 2.2067],\n",
            "        [1.8026, 1.7973, 1.8098, 1.7969, 1.7259, 1.8265, 1.7956]])\n",
            "T63:  tensor([[202.3466, 163.8040, 371.5062,  48.3215, 147.3127,  57.8124,  53.6309],\n",
            "        [ 40.3770,  37.0234,  45.4053,  36.7879,  11.0656,  59.5153,  35.9990]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.5112, 2.7893, 2.7095, 2.9050, 2.6892, 2.7468, 2.7521],\n",
            "        [1.7740, 1.7650, 1.8564, 1.7960, 1.7901, 1.8079, 1.7177]])\n",
            "T63:  tensor([[ 129.4164, 2325.1294, 1059.1146, 6893.6426,  862.0840, 1535.5135,\n",
            "         1617.1102],\n",
            "        [  25.5031,   21.9220,   96.8055,   36.7301,   33.3406,   44.5999,\n",
            "            9.7135]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.8363, 3.0735, 2.9708, 2.8408, 2.9889, 2.6947, 3.1890],\n",
            "        [1.8939, 1.8596, 1.9274, 1.8103, 1.8934, 1.9232, 1.8358]])\n",
            "T63:  tensor([[  640.8814,  5667.6816,  2271.9771,   669.7928,  2676.4282,   153.9796,\n",
            "         15094.1885],\n",
            "        [  175.0036,   103.1536,   289.7559,    46.9558,   173.7167,   272.3799,\n",
            "            70.8275]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7550, 1.7991, 1.8245, 1.8371, 1.7449, 1.7310, 1.8638],\n",
            "        [2.9916, 2.8295, 2.7597, 2.6297, 2.6363, 2.8778, 2.4930]])\n",
            "T63:  tensor([[  20.2113,   42.1793,   63.6644,   77.8883,   17.0366,   13.4048,\n",
            "          118.4995],\n",
            "        [3094.7747,  680.0020,  341.1806,   88.0922,   94.5879, 1081.2690,\n",
            "           19.0380]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.8921, 1.7938, 1.9408, 1.8488, 1.9511, 1.8011, 1.8208],\n",
            "        [2.1980, 2.3650, 2.3965, 2.2290, 2.5085, 2.3379, 2.2126]])\n",
            "T63:  tensor([[ 171.8529,   36.1580,  356.5053,   87.7864,  414.8495,   40.7537,\n",
            "           56.1826],\n",
            "        [  69.9623,  554.8812,  801.1871,  104.3703, 2813.5000,  402.2249,\n",
            "           84.5342]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.9665, 1.9512, 1.9656, 1.8750, 1.8006, 1.7621, 1.9088],\n",
            "        [1.8392, 1.9218, 1.8282, 2.0002, 1.8969, 1.8895, 1.8521]])\n",
            "T63:  tensor([[512.8281, 410.0909, 505.8144, 130.5981,  39.9209,  21.0522, 218.8035],\n",
            "        [ 76.2440, 271.9687,  64.0001, 848.8100, 186.8168, 167.0604,  93.4892]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.4801, 2.9928, 2.7171, 2.8235, 2.7094, 2.7760, 2.8674],\n",
            "        [2.8022, 2.9351, 2.7769, 2.7380, 2.8818, 2.6504, 2.5370]])\n",
            "T63:  tensor([[  14.3081, 2775.3638,  194.6945,  566.7422,  179.7866,  354.3207,\n",
            "          865.8289],\n",
            "        [ 943.0193, 3308.8330,  735.4326,  499.0340, 2019.1633,  202.2209,\n",
            "           58.8401]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.7709, 1.8769, 1.8396, 1.9122, 1.9192, 1.8978, 1.8716],\n",
            "        [1.8239, 1.8102, 1.8428, 1.7437, 1.8294, 1.8833, 1.7034]])\n",
            "T63:  tensor([[ 23.7343, 130.7439,  72.8284, 223.8732, 248.4410, 179.8970, 120.3166],\n",
            "        [ 59.7286,  47.8269,  80.7534,  15.7788,  65.2088, 151.9322,   7.8127]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.4388, 2.2484, 2.2643, 2.1335, 2.2215, 2.2515, 2.2814],\n",
            "        [2.3054, 2.2981, 2.0571, 2.3102, 2.4491, 2.1709, 2.3704]])\n",
            "T63:  tensor([[1153.6870,  118.0957,  144.4481,   26.0391,   83.7971,  122.9144,\n",
            "          178.8199],\n",
            "        [ 178.4066,  162.9844,    6.4898,  189.1363,  970.1402,   31.7267,\n",
            "          390.7690]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.7033, 2.8195, 2.5854, 2.7299, 2.5571, 2.5834, 2.7662],\n",
            "        [1.7963, 1.7997, 1.7630, 1.7561, 1.8454, 1.8434, 1.8417]])\n",
            "T63:  tensor([[ 352.7635, 1124.6411,  100.9398,  462.7006,   73.8347,   98.8233,\n",
            "          666.5745],\n",
            "        [  37.6695,   39.8077,   21.6345,   19.2452,   83.0714,   80.5341,\n",
            "           78.3609]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2771, 2.4503, 2.3956, 2.3206, 2.2619, 2.3185, 2.5479],\n",
            "        [2.1526, 2.3822, 2.2680, 2.2775, 2.0509, 2.3106, 2.3557]])\n",
            "T63:  tensor([[ 204.6043, 1577.0178,  846.3181,  348.6978,  169.4776,  339.9557,\n",
            "         4577.7930],\n",
            "        [  24.2411,  439.4337,  109.6377,  123.4580,    5.7941,  186.1419,\n",
            "          321.0417]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.9356, 2.6643, 2.7824, 2.8534, 2.9327, 2.6143, 2.7381],\n",
            "        [1.7300, 1.8506, 1.9635, 1.9314, 1.8426, 1.8666, 1.8714]])\n",
            "T63:  tensor([[1465.4862,   99.3794,  335.6538,  673.4556, 1426.1631,   57.9414,\n",
            "          214.3571],\n",
            "        [  13.4062,   98.1124,  539.3483,  336.9150,   86.5455,  125.9885,\n",
            "          135.8403]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7500, 1.7359, 1.7749, 1.7809, 1.7641, 1.7624, 1.7875],\n",
            "        [2.7902, 2.8804, 2.5392, 3.1443, 2.6132, 2.9916, 2.6269]])\n",
            "T63:  tensor([[  16.6714,   13.0865,   25.3582,   28.0246,   21.1775,   20.5514,\n",
            "           31.2707],\n",
            "        [ 328.5888,  791.6042,   22.5798, 8452.2197,   51.6636, 2224.4109,\n",
            "           60.0257]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.4611, 2.3232, 2.2543, 2.4504, 2.0997, 2.3013, 2.2578],\n",
            "        [1.6708, 1.7469, 1.8611, 1.8135, 1.8652, 1.8194, 1.7688]])\n",
            "T63:  tensor([[1272.5232,  255.0020,  108.5099, 1128.5630,   13.8182,  195.2264,\n",
            "          113.4558],\n",
            "        [   4.1968,   16.1060,  104.2247,   48.8812,  111.1109,   53.8035,\n",
            "           23.3479]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.8684, 1.9947, 1.6892, 1.8817, 1.8108, 1.8962, 1.9357],\n",
            "        [1.8192, 1.8327, 1.8568, 1.8220, 1.7841, 1.8175, 1.8310]])\n",
            "T63:  tensor([[111.0309, 724.4203,   5.5531, 136.3702,  44.4120, 170.2169, 307.9429],\n",
            "        [ 55.1067,  68.4735, 100.2253,  57.6621,  31.0691,  53.6797,  66.6559]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.3889, 2.4808, 2.3545, 2.1806, 2.4611, 2.5048, 2.4294],\n",
            "        [2.7900, 2.9141, 2.6809, 3.0692, 2.9521, 2.7240, 2.8511]])\n",
            "T63:  tensor([[ 733.7425, 2077.3154,  490.4171,   55.6446, 1668.8591, 2701.6040,\n",
            "         1168.3536],\n",
            "        [ 330.5380, 1096.8057,  107.8368, 4463.2939, 1561.3390,  169.0945,\n",
            "          601.7114]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7570, 1.8517, 1.7594, 1.7862, 1.8129, 1.8598, 1.7965],\n",
            "        [2.8169, 2.5219, 2.9773, 2.8792, 2.8366, 2.7230, 2.7447]])\n",
            "T63:  tensor([[  19.7279,   92.5393,   20.5190,   32.1546,   49.8468,  105.0677,\n",
            "           38.1233],\n",
            "        [ 834.3955,   37.4841, 3746.6174, 1515.6898, 1009.2262,  327.4110,\n",
            "          407.7562]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.7876, 3.0093, 2.7320, 2.9476, 2.9680, 2.7242, 2.6941],\n",
            "        [1.7933, 1.8099, 1.7548, 1.7878, 1.8756, 1.7782, 1.8313]])\n",
            "T63:  tensor([[ 515.2810, 4135.3022,  294.6931, 2366.0308, 2850.2192,  272.1575,\n",
            "          199.4247],\n",
            "        [  35.1570,   46.1208,   18.4605,   32.1312,  130.7542,   27.3974,\n",
            "           65.2010]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2813, 2.1907, 2.2807, 2.3742, 2.2897, 2.2358, 2.3127],\n",
            "        [2.1735, 2.2833, 2.2831, 2.1884, 2.1743, 2.3577, 2.1800]])\n",
            "T63:  tensor([[144.9092,  45.3441, 143.8513, 446.7331, 160.8185,  81.5126, 213.5799],\n",
            "        [ 41.8448, 171.7488, 171.2499,  50.9907,  42.2783, 423.8873,  45.6434]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.9588, 2.9366, 2.9396, 2.8504, 2.8091, 2.6519, 2.9317],\n",
            "        [2.3653, 2.2969, 2.2368, 2.3421, 2.2661, 2.2196, 2.2439]])\n",
            "T63:  tensor([[1249.0409, 1016.3495, 1044.3529,  446.7370,  297.9486,   58.5516,\n",
            "          970.6206],\n",
            "        [ 402.6090,  176.2570,   82.7050,  305.4090,  119.9764,   66.2563,\n",
            "           90.5548]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.7570, 1.8167, 1.8053, 1.8290, 1.7670, 1.7362, 1.8460],\n",
            "        [2.3045, 2.2655, 2.3394, 2.2461, 2.1545, 2.2042, 2.3088]])\n",
            "T63:  tensor([[ 19.0663,  51.2649,  42.5749,  62.4085,  22.5725,  13.3423,  81.8204],\n",
            "        [197.9768, 121.8452, 302.6394,  95.3731,  28.6382,  55.4807, 208.6034]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.8480, 2.7060, 2.8934, 2.6990, 2.7883, 2.7232, 2.7064],\n",
            "        [2.4266, 2.8478, 2.8031, 2.7964, 2.5938, 2.8952, 2.9027]])\n",
            "T63:  tensor([[ 676.0444,  163.1482, 1043.4969,  151.7260,  376.8404,  194.9497,\n",
            "          163.9461],\n",
            "        [   8.4261,  793.2945,  512.9478,  480.5312,   58.1859, 1245.1644,\n",
            "         1336.7161]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.8547, 1.8147, 1.8913, 1.8119, 1.8516, 1.7643, 1.8316],\n",
            "        [2.0796, 2.2766, 2.3528, 2.3965, 2.2460, 2.2287, 2.1616]])\n",
            "T63:  tensor([[ 96.5047,  51.0654, 170.2253,  48.7447,  91.9377,  22.1914,  67.0092],\n",
            "        [ 11.3450, 156.0353, 394.7945, 660.0204, 106.1828,  85.0323,  35.2335]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3147, 2.1815, 2.2710, 2.2207, 2.3067, 2.3058, 2.3902],\n",
            "        [1.8688, 1.8828, 1.7315, 1.8285, 1.8414, 1.8039, 1.8459]])\n",
            "T63:  tensor([[219.9194,  40.3706, 128.0565,  67.4483, 199.3847, 197.2198, 541.6544],\n",
            "        [123.7709, 153.6710,  13.0259,  65.4910,  80.5529,  44.0293,  86.4425]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.8747, 2.6753, 2.8279, 2.8710, 2.7964, 2.8641, 3.0417],\n",
            "        [2.9105, 2.9712, 2.6218, 2.7684, 2.8145, 2.8899, 2.9642]])\n",
            "T63:  tensor([[ 747.0911,  100.4232,  474.8943,  721.0715,  347.8528,  674.4767,\n",
            "         3470.8516],\n",
            "        [1589.9883, 2779.2930,   87.6737,  403.0638,  635.9645, 1310.6251,\n",
            "         2607.7861]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.7687, 2.7322, 2.8494, 2.7884, 2.9840, 2.9415, 2.4228],\n",
            "        [2.5823, 2.9586, 2.7294, 2.6761, 2.8887, 2.8198, 2.8129]])\n",
            "T63:  tensor([[ 377.3812,  260.6494,  834.3558,  459.3918, 2928.7463, 1986.9014,\n",
            "            8.2862],\n",
            "        [  53.9558, 2364.5916,  258.2636,  148.5331, 1235.4652,  637.3118,\n",
            "          595.9451]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[3.0756, 2.9771, 2.9041, 2.7490, 2.9337, 2.8148, 2.8362],\n",
            "        [2.8720, 2.6928, 3.0016, 2.6153, 2.8427, 2.7064, 2.8625]])\n",
            "T63:  tensor([[4521.2388, 1880.3472,  955.9292,  209.0067, 1260.3257,  403.8121,\n",
            "          498.2847],\n",
            "        [ 839.8472,  140.0461, 2792.6365,   61.1556,  633.8560,  161.4909,\n",
            "          767.0689]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.6995, 2.6293, 2.7973, 2.8013, 2.7174, 2.8508, 2.8114],\n",
            "        [2.1861, 2.3724, 2.2986, 2.2551, 2.3008, 2.4022, 2.1972]])\n",
            "T63:  tensor([[150.8292,  71.5224, 407.3025, 423.7018, 181.5858, 687.0692, 468.1931],\n",
            "        [ 54.0652, 549.5222, 226.3810, 131.6339, 232.4692, 777.0809,  62.5293]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.3522, 2.2240, 2.1960, 2.2778, 2.3451, 2.4759, 2.2300],\n",
            "        [1.7705, 1.7836, 1.7703, 1.7740, 1.7782, 1.7602, 1.7789]])\n",
            "T63:  tensor([[ 387.9184,   79.1181,   54.9471,  156.6549,  356.2783, 1607.9189,\n",
            "           85.5559],\n",
            "        [  24.2897,   30.2131,   24.2148,   25.7735,   27.6179,   20.4299,\n",
            "           27.9661]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.3637, 2.2265, 2.1933, 2.0830, 2.1696, 2.2623, 2.2373],\n",
            "        [1.7377, 1.7254, 1.7778, 1.7635, 1.7805, 1.8202, 1.7892]])\n",
            "T63:  tensor([[373.3869,  68.3094,  44.2469,   9.7539,  32.2641, 107.8265,  78.4507],\n",
            "        [ 14.6221,  11.8129,  28.8697,  22.7108,  30.2024,  57.8474,  34.8692]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7404, 2.9341, 2.5516, 2.9720, 2.7913, 2.7457, 2.8688],\n",
            "        [2.7218, 2.7835, 2.6282, 2.8116, 2.8489, 2.8362, 2.8322]])\n",
            "T63:  tensor([[ 214.0594, 1412.0933,   28.0762, 2002.6576,  357.8258,  226.0117,\n",
            "          762.8972],\n",
            "        [ 171.9788,  321.8597,   63.7998,  425.5742,  612.7249,  541.4428,\n",
            "          521.0446]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.9221, 1.8838, 1.9100, 1.7723, 1.9365, 1.9355, 1.9209],\n",
            "        [2.5271, 2.3643, 2.3904, 2.4180, 2.3457, 2.4598, 2.2595]])\n",
            "T63:  tensor([[ 267.8202,  149.9144,  223.3796,   25.1066,  331.7775,  326.9643,\n",
            "          263.2361],\n",
            "        [2804.5554,  445.7069,  605.8466,  833.3538,  357.5965, 1339.2252,\n",
            "          123.9337]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.9113, 1.7509, 1.8992, 1.8216, 1.9544, 1.9978, 1.8361],\n",
            "        [1.7974, 1.7966, 1.6858, 1.8102, 1.8008, 1.7506, 1.8309]])\n",
            "T63:  tensor([[220.5900,  16.8908, 183.6941,  54.5424, 418.3469, 779.9877,  68.8465],\n",
            "        [ 36.8700,  36.4001,   5.4003,  45.4370,  38.9910,  16.8380,  63.4714]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.8265, 2.7326, 2.7707, 3.0403, 2.6805, 2.7125, 2.6969],\n",
            "        [2.2142, 2.2869, 2.2692, 2.3653, 2.2374, 2.3051, 2.2573]])\n",
            "T63:  tensor([[ 447.6997,  174.2381,  256.8311, 3282.5586,  101.2064,  141.5190,\n",
            "          120.3007],\n",
            "        [  71.8041,  180.3428,  144.6474,  465.9052,   96.7524,  225.8307,\n",
            "          124.5549]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8047, 1.8690, 1.7933, 1.8687, 1.7618, 1.8060, 1.8284],\n",
            "        [1.9803, 1.8692, 1.9264, 1.9434, 1.9563, 1.7773, 1.7907]])\n",
            "T63:  tensor([[ 41.5329, 115.6719,  34.4221, 115.2103,  20.3667,  42.4303,  60.9745],\n",
            "        [654.5165, 124.9901, 298.2148, 383.4674, 463.2184,  28.5246,  35.6295]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.4489, 2.3542, 2.3658, 2.3670, 2.2672, 2.2699, 2.2076],\n",
            "        [2.3081, 2.4874, 2.3984, 2.4178, 2.1273, 2.4758, 2.2723]])\n",
            "T63:  tensor([[1106.4006,  369.2785,  424.0133,  430.2656,  127.4920,  131.7957,\n",
            "           59.4122],\n",
            "        [ 232.5322, 1862.5341,  680.8389,  851.7844,   22.3154, 1638.7378,\n",
            "          149.3564]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3890, 2.4917, 2.2461, 2.4899, 2.2658, 2.4284, 2.4438],\n",
            "        [1.7900, 1.8145, 1.8922, 1.8212, 1.9016, 1.8846, 1.7350]])\n",
            "T63:  tensor([[ 478.5403, 1536.9500,   83.5385, 1505.6252,  107.2762,  755.0859,\n",
            "          899.1097],\n",
            "        [  33.3952,   49.8226,  169.1455,   55.5270,  195.0363,  150.4979,\n",
            "           13.1826]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7727, 1.7865, 1.7610, 1.8396, 1.7586, 1.8014, 1.8020],\n",
            "        [1.7833, 1.8670, 1.8058, 1.8771, 1.7314, 1.7450, 1.8640]])\n",
            "T63:  tensor([[ 25.5717,  32.2237,  21.0158,  76.1392,  20.1847,  41.1713,  41.5713],\n",
            "        [ 29.6674, 113.8581,  42.9837, 133.2937,  12.2924,  15.5436, 108.7205]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2663, 2.1673, 2.2710, 2.2290, 2.4644, 2.1591, 2.2739],\n",
            "        [1.7986, 1.8301, 1.8488, 1.8487, 1.8512, 1.8581, 1.7752]])\n",
            "T63:  tensor([[ 116.3953,   32.1731,  123.5508,   72.4003, 1220.6294,   28.7963,\n",
            "          128.1554],\n",
            "        [  38.0538,   63.4258,   85.3367,   85.1820,   88.6355,   98.7478,\n",
            "           25.8082]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.8441, 1.8748, 1.8492, 1.9455, 1.7506, 1.7735, 1.9317],\n",
            "        [2.7552, 2.7428, 2.7792, 2.7823, 2.8161, 2.7358, 2.6286]])\n",
            "T63:  tensor([[ 79.6622, 128.8523,  86.4549, 373.9959,  17.1505,  25.2404, 304.8086],\n",
            "        [271.9615, 239.5999, 346.1249, 357.0547, 499.1397, 223.1175,  72.1268]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.7475, 1.7794, 1.7853, 1.9063, 1.8489, 1.7249, 1.8046],\n",
            "        [2.2370, 2.3601, 2.2920, 2.3325, 2.2067, 2.2816, 2.1913]])\n",
            "T63:  tensor([[ 16.6341,  28.5032,  31.3911, 213.2090,  87.8856,  11.2497,  43.1317],\n",
            "        [ 92.7133, 422.0302, 185.1071, 303.6587,  62.6255, 162.7724,  51.1730]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.7666, 2.9104, 2.7122, 2.8281, 2.6851, 2.9186, 2.7253],\n",
            "        [2.3596, 2.4277, 2.2989, 2.4131, 2.5896, 2.3419, 2.4076]])\n",
            "T63:  tensor([[ 293.5075, 1187.8533,  168.3256,  540.1443,  126.8935, 1281.7915,\n",
            "          192.8397],\n",
            "        [ 363.2540,  803.2744,  174.1848,  679.3763, 4716.5981,  293.8857,\n",
            "          638.1479]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7881, 2.6130, 2.9100, 2.7865, 2.7329, 3.1142, 2.9433],\n",
            "        [2.7292, 2.7351, 2.6555, 2.8537, 2.6169, 3.0572, 2.5508]])\n",
            "T63:  tensor([[  540.3333,    88.1938,  1742.4265,   531.9223,   310.8141, 10724.2793,\n",
            "          2369.8777],\n",
            "        [  218.2041,   231.9626,   101.0065,   752.1188,    66.6463,  4867.0288,\n",
            "            31.9886]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.4100, 2.2447, 2.2145, 2.2174, 2.4196, 2.3951, 2.4073],\n",
            "        [2.7424, 2.7061, 2.8295, 2.7755, 2.8725, 2.9134, 2.8523]])\n",
            "T63:  tensor([[ 809.6521,  109.9039,   74.5542,   77.3631,  904.4977,  681.9880,\n",
            "          784.9702],\n",
            "        [ 321.7556,  221.7741,  763.5270,  448.5920, 1153.9363, 1695.3094,\n",
            "          951.3459]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.5030, 2.4899, 2.2259, 2.3317, 2.3928, 2.4168, 2.3569],\n",
            "        [1.8106, 1.9242, 1.9887, 1.7400, 1.8680, 1.7757, 1.8514]])\n",
            "T63:  tensor([[2031.1528, 1760.0477,   75.8530,  284.1232,  586.1246,  773.6342,\n",
            "          383.9081],\n",
            "        [  47.4148,  277.6714,  710.3719,   14.5555,  118.0081,   26.6848,\n",
            "           90.9821]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2902, 2.1913, 2.2629, 2.2448, 2.2393, 2.2606, 2.2987],\n",
            "        [1.8058, 1.7996, 1.7402, 1.8004, 1.7839, 1.7798, 1.8783]])\n",
            "T63:  tensor([[161.3383,  45.5277, 114.7372,  91.1482,  85.0599, 111.4105, 179.2763],\n",
            "        [ 41.5186,  37.5278,  13.8362,  37.9999,  28.9272,  27.0325, 131.2904]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7850, 2.7334, 2.9117, 2.8038, 2.9370, 2.5183, 2.7570],\n",
            "        [1.7772, 1.8661, 1.6798, 1.8655, 1.7569, 1.7666, 1.8243]])\n",
            "T63:  tensor([[ 310.2971,  183.9340, 1059.1090,  374.1684, 1341.0453,   17.6823,\n",
            "          234.1780],\n",
            "        [  26.9520,  112.7910,    4.9539,  111.7780,   19.1503,   22.5580,\n",
            "           58.2908]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.7623, 2.6133, 2.6191, 3.0777, 2.5915, 2.6733, 2.5792],\n",
            "        [1.8376, 1.8040, 1.7608, 1.7498, 1.8271, 1.8126, 1.8165]])\n",
            "T63:  tensor([[ 535.2786,  113.8574,  121.1835, 9938.1172,   89.7866,  215.3955,\n",
            "           78.4814],\n",
            "        [  68.3757,   39.7331,   19.3726,   16.0652,   57.7979,   45.7066,\n",
            "           48.6971]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3423, 2.2423, 2.2328, 2.3629, 2.1338, 2.1922, 2.3058],\n",
            "        [2.4183, 2.3889, 2.3626, 2.2731, 2.2149, 2.2482, 2.2010]])\n",
            "T63:  tensor([[316.0153,  91.6299,  81.1194, 404.2511,  21.7975,  47.8054, 202.8356],\n",
            "        [878.0678, 625.1823, 459.1006, 154.7372,  73.7606, 113.0710,  61.5594]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8028, 1.7734, 1.8765, 1.7777, 1.8832, 1.8999, 1.8680],\n",
            "        [1.8618, 1.7199, 1.7807, 1.8854, 1.8200, 1.8180, 1.8190]])\n",
            "T63:  tensor([[ 42.8597,  26.3446, 138.2133,  28.3254, 153.2316, 197.4790, 121.0673],\n",
            "        [103.5383,   9.9184,  27.9679, 149.1075,  53.2908,  51.5986,  52.4240]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.7267, 1.8141, 1.7920, 1.7616, 1.7981, 1.7704, 1.8695],\n",
            "        [1.7984, 1.9375, 1.8947, 1.8963, 1.8864, 1.9181, 1.8482]])\n",
            "T63:  tensor([[ 11.3560,  49.2400,  34.2970,  20.6616,  37.9680,  23.9668, 118.5518],\n",
            "        [ 38.2183, 332.9826, 175.3373, 179.7148, 154.4579, 249.4456,  85.2313]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.1899, 2.2756, 2.2330, 2.2472, 2.2711, 2.3261, 2.2176],\n",
            "        [2.3572, 2.3412, 2.3419, 2.3760, 2.4326, 2.3256, 2.3904]])\n",
            "T63:  tensor([[ 53.5167, 160.5624,  93.7290, 112.3186, 151.7849, 298.5604,  76.7935],\n",
            "        [402.7133, 332.8074, 335.5043, 503.1437, 967.9685, 275.4638, 595.3167]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8896, 2.0008, 1.8574, 1.8476, 1.9093, 1.7935, 1.8094],\n",
            "        [2.9740, 2.7200, 2.8403, 2.8797, 2.8494, 2.7376, 2.7457]])\n",
            "T63:  tensor([[ 171.5603,  878.1398,  104.2142,   89.3813,  231.2101,   37.3681,\n",
            "           48.5130],\n",
            "        [1492.5381,  125.4859,  421.7973,  617.2296,  460.6172,  150.5469,\n",
            "          163.6970]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.7966, 2.6828, 2.8936, 2.9654, 3.1950, 2.6772, 2.8150],\n",
            "        [1.8301, 1.7637, 1.8288, 1.8070, 1.7619, 1.8210, 1.7878]])\n",
            "T63:  tensor([[  397.1781,   124.1989,  1016.0718,  1981.6031, 14557.6074,   117.1051,\n",
            "           476.5875],\n",
            "        [   63.7807,    21.4179,    62.4932,    43.9038,    20.7922,    55.0950,\n",
            "            32.0736]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4421, 2.3162, 2.2198, 2.3849, 2.4194, 2.2981, 2.3993],\n",
            "        [2.9900, 3.0106, 2.7133, 3.0140, 2.8515, 2.9900, 2.8359]])\n",
            "T63:  tensor([[1240.6902,  284.1372,   85.0026,  644.0946,  958.9371,  227.9150,\n",
            "          760.6175],\n",
            "        [1878.4236, 2260.9346,  127.8365, 2331.0378,  512.3475, 1878.2612,\n",
            "          440.0383]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.2181, 2.2262, 2.2009, 2.2064, 2.3967, 2.2801, 2.3769],\n",
            "        [1.9205, 1.7728, 1.6816, 1.8092, 1.9124, 1.8145, 1.9846]])\n",
            "T63:  tensor([[ 71.5483,  79.5157,  57.2485,  61.4705, 639.1202, 157.3400, 506.9634],\n",
            "        [253.1463,  24.4634,   4.9949,  44.6538, 224.3906,  48.6391, 645.8218]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.6612, 2.8510, 2.7260, 2.6485, 2.6730, 2.8406, 2.6208],\n",
            "        [1.7665, 1.8161, 1.7764, 1.8087, 1.7862, 1.8134, 1.8359]])\n",
            "T63:  tensor([[134.7612, 913.6251, 264.5533, 117.8240, 152.6284, 827.0122,  87.4780],\n",
            "        [ 22.0666,  49.9890,  26.0438,  44.3832,  30.6234,  47.9123,  68.7755]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.1895, 2.2565, 2.2271, 2.2463, 2.1896, 2.1439, 2.2175],\n",
            "        [1.8296, 1.7573, 1.7860, 1.8095, 1.7120, 1.7757, 1.8149]])\n",
            "T63:  tensor([[ 69.9082, 165.3441, 113.9081, 145.3698,  70.0556,  38.1036, 100.6142],\n",
            "        [ 62.9210,  19.1312,  30.9224,  45.5174,   8.7416,  26.0519,  49.6707]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.1833, 2.3073, 2.3202, 2.2520, 2.4323, 2.3974, 2.3356],\n",
            "        [1.8037, 1.8479, 1.8049, 1.7479, 1.8161, 1.7692, 1.7359]])\n",
            "T63:  tensor([[  59.0369,  284.5352,  332.8999,  143.3304, 1237.9036,  830.2367,\n",
            "          400.9037],\n",
            "        [  44.1804,   89.7727,   45.0314,   17.3997,   53.9982,   24.9354,\n",
            "           14.1672]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7797, 1.8259, 1.7782, 1.8718, 1.7767, 1.8666, 1.7801],\n",
            "        [1.7592, 1.7646, 1.7479, 1.8343, 1.8305, 1.8025, 1.7348]])\n",
            "T63:  tensor([[ 27.5421,  58.6282,  26.8901, 121.0720,  26.2215, 111.7368,  27.7268],\n",
            "        [ 19.6901,  21.5827,  16.2407,  67.6396,  63.6642,  40.5028,  12.9709]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.5681, 2.7718, 2.8188, 2.8807, 2.7154, 2.6781, 2.7594],\n",
            "        [1.8020, 1.8135, 1.7691, 1.8107, 1.7559, 1.8186, 1.7172]])\n",
            "T63:  tensor([[  53.8855,  459.9141,  731.1927, 1324.4625,  260.0572,  176.6332,\n",
            "          406.4478],\n",
            "        [  41.9436,   50.5949,   24.3159,   48.3981,   19.4681,   54.9383,\n",
            "            9.9774]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.9383, 2.6944, 2.8552, 2.7570, 2.9697, 2.7040, 3.0879],\n",
            "        [2.1238, 2.1980, 2.2745, 2.2290, 2.2054, 2.3053, 2.1463]])\n",
            "T63:  tensor([[1962.0293,  179.9402,  897.4949,  341.8493, 2615.2852,  198.6729,\n",
            "         7451.4927],\n",
            "        [  28.9919,   78.1439,  206.9690,  116.6969,   86.1249,  302.4117,\n",
            "           39.3470]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8709, 1.7959, 1.8003, 1.7852, 1.7374, 1.8061, 1.8344],\n",
            "        [2.3152, 2.2967, 2.1803, 2.2758, 2.2590, 2.3901, 2.2033]])\n",
            "T63:  tensor([[125.5530,  37.9340,  40.7761,  31.7546,  14.1602,  44.8233,  70.7659],\n",
            "        [255.2585, 203.5688,  45.9704, 156.9598, 127.1516, 622.2525,  62.2000]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3487, 2.2932, 2.2319, 2.1974, 2.2441, 2.4133, 2.4142],\n",
            "        [2.3511, 2.3588, 2.2354, 2.3465, 2.3121, 2.3637, 2.2254]])\n",
            "T63:  tensor([[297.0655, 151.0628,  69.5628,  44.3838,  81.3814, 634.8749, 641.1777],\n",
            "        [335.8442, 368.2630,  80.2108, 317.7508, 209.5372, 390.2184,  70.4729]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[3.0390, 2.9040, 2.7302, 2.7737, 3.1121, 2.7918, 2.8039],\n",
            "        [1.8085, 1.8249, 1.8464, 1.8279, 1.8332, 1.7565, 1.8584]])\n",
            "T63:  tensor([[4517.4883, 1324.1337,  241.5937,  374.8023, 8513.2930,  448.9544,\n",
            "          505.9770],\n",
            "        [  45.4505,   59.3191,   83.4666,   62.1672,   67.7233,   19.1725,\n",
            "          100.7995]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7887, 1.8664, 1.9506, 1.9085, 1.8227, 1.7491, 1.8844],\n",
            "        [1.8571, 1.9141, 1.8914, 1.8585, 1.8349, 1.7887, 1.8303]])\n",
            "T63:  tensor([[ 31.6034, 110.0327, 392.3223, 209.6764,  55.0405,  16.2453, 145.3136],\n",
            "        [100.1197, 239.8286, 169.9472, 102.3274,  70.4103,  33.2510,  65.4310]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.7602, 1.8696, 1.8334, 1.8088, 2.0384, 1.8725, 1.8464],\n",
            "        [2.9372, 2.9239, 2.7908, 2.5581, 2.6499, 2.6552, 2.9113]])\n",
            "T63:  tensor([[  20.1847,  118.7734,   67.2170,   45.2234, 1398.9919,  124.1982,\n",
            "           82.5704],\n",
            "        [1550.1809, 1371.1995,  380.6252,   32.4689,   89.0350,   94.2504,\n",
            "         1218.2568]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.1591, 2.2491, 2.2873, 2.2184, 2.2162, 2.3515, 2.2246],\n",
            "        [2.7399, 2.9424, 2.6851, 2.8761, 2.8463, 2.8141, 2.7090]])\n",
            "T63:  tensor([[  30.9475,  100.4223,  162.4000,   67.7691,   65.8693,  354.6016,\n",
            "           73.4573],\n",
            "        [ 305.0579, 2157.8010,  173.4020, 1162.1389,  873.2142,  639.0564,\n",
            "          222.1849]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.8659, 2.7511, 2.6417, 2.7445, 2.8704, 2.7911, 2.6798],\n",
            "        [1.9887, 1.8897, 1.8862, 1.8452, 1.8963, 1.7159, 1.8206]])\n",
            "T63:  tensor([[1002.8193,  324.8475,  103.9396,  303.6437, 1047.4188,  484.4370,\n",
            "          155.7555],\n",
            "        [ 697.7903,  161.9036,  153.5822,   81.0503,  179.1868,    9.3999,\n",
            "           54.7564]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8688, 1.7743, 1.8055, 1.7937, 1.8446, 1.8679, 1.8427],\n",
            "        [2.7865, 2.9468, 2.7590, 2.8530, 2.7310, 2.8466, 2.7746]])\n",
            "T63:  tensor([[ 120.4086,   26.2524,   43.9730,   36.2479,   82.3750,  118.6219,\n",
            "           79.8886],\n",
            "        [ 344.0063, 1600.2323,  260.8991,  660.6525,  196.0814,  620.7672,\n",
            "          305.4297]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.4826, 2.3098, 2.4034, 2.4412, 2.5748, 2.4494, 2.3372],\n",
            "        [1.8253, 1.8075, 1.7550, 1.8014, 1.8119, 1.7795, 1.7769]])\n",
            "T63:  tensor([[1401.9482,  187.0568,  570.9043,  881.0947, 3800.7251,  966.2652,\n",
            "          260.9876],\n",
            "        [  57.9041,   43.4425,   18.1336,   39.2996,   46.6585,   27.3534,\n",
            "           26.2077]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7381, 1.7939, 1.7509, 1.7452, 1.7035, 1.7583, 1.8411],\n",
            "        [2.7655, 2.7232, 2.9553, 2.8800, 2.8768, 2.8822, 2.8486]])\n",
            "T63:  tensor([[  13.8400,   35.4042,   17.2279,   15.6277,    7.5600,   19.5500,\n",
            "           76.0246],\n",
            "        [ 295.5139,  192.0790, 1832.1389,  906.2473,  879.4760,  926.0731,\n",
            "          671.2306]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.7380, 1.7082, 1.8284, 1.7847, 1.7373, 1.7731, 1.7654],\n",
            "        [1.7078, 1.8140, 1.7828, 1.7970, 1.8305, 1.8061, 1.7968]])\n",
            "T63:  tensor([[13.7082,  8.1538, 61.6320, 30.1902, 13.5465, 24.9134, 21.8809],\n",
            "        [ 8.1701, 49.3069, 29.5566, 37.3678, 64.3606, 43.4122, 37.2336]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2188, 2.2769, 2.2413, 2.2204, 2.0641, 2.1401, 2.3305],\n",
            "        [2.3627, 2.3475, 2.4972, 2.2034, 2.2047, 2.4339, 2.1121]])\n",
            "T63:  tensor([[  74.8936,  156.6222,   99.9657,   76.4118,    9.0775,   26.3215,\n",
            "          302.1622],\n",
            "        [ 382.6406,  319.2653, 1777.8156,   52.4931,   53.4610,  875.7126,\n",
            "           15.2967]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.1094, 2.3545, 2.4109, 2.3658, 2.4108, 2.3596, 2.4008],\n",
            "        [2.7767, 2.9233, 2.8393, 2.7742, 2.9333, 2.7561, 2.8775]])\n",
            "T63:  tensor([[  16.6127,  389.2980,  752.7737,  445.0646,  752.3134,  413.4185,\n",
            "          670.3612],\n",
            "        [ 318.5578, 1313.6479,  590.6040,  310.6475, 1441.5568,  258.9310,\n",
            "          852.7822]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.7969, 2.6257, 2.8653, 2.7183, 2.8327, 2.4814, 3.0142],\n",
            "        [1.8104, 1.8304, 1.8058, 1.7852, 1.8277, 1.8052, 1.8145]])\n",
            "T63:  tensor([[ 374.9636,   63.3675,  730.6261,  169.0223,  533.1870,   12.3813,\n",
            "         2899.3831],\n",
            "        [  46.9425,   64.8161,   43.5234,   31.0071,   62.0055,   43.1316,\n",
            "           50.1244]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8905, 1.8248, 1.9106, 1.7783, 1.8447, 1.8297, 1.8846],\n",
            "        [2.2094, 2.3056, 2.2745, 2.1997, 2.3583, 2.3064, 2.3248]])\n",
            "T63:  tensor([[161.0738,  57.5471, 218.6154,  26.8660,  78.9774,  62.2752, 147.2729],\n",
            "        [ 60.5153, 204.3647, 139.0634,  53.3510, 386.0639, 206.5166, 258.2126]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.7949, 2.9515, 2.9293, 2.9718, 3.1354, 3.0843, 2.7339],\n",
            "        [2.7548, 2.5703, 2.8624, 2.8322, 2.7249, 2.8172, 2.9051]])\n",
            "T63:  tensor([[ 318.9276, 1432.5938, 1165.8937, 1725.6746, 7307.2842, 4707.0156,\n",
            "          171.9692],\n",
            "        [ 289.2053,   40.5262,  835.7590,  624.1909,  213.0836,  539.1329,\n",
            "         1254.1252]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2957, 2.2532, 2.4211, 2.3136, 2.1457, 2.1913, 2.3932],\n",
            "        [2.7752, 2.8456, 2.8613, 2.8278, 2.8888, 2.8418, 2.5157]])\n",
            "T63:  tensor([[ 197.9593,  116.4659,  878.0893,  246.7382,   28.4569,   52.3222,\n",
            "          636.2793],\n",
            "        [ 364.0662,  727.5457,  845.8162,  612.0720, 1099.1885,  700.9490,\n",
            "           22.4614]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7606, 1.7317, 1.7855, 1.7481, 1.8031, 1.7572, 1.7448],\n",
            "        [2.8556, 2.8076, 2.8799, 2.8381, 2.8458, 2.8073, 3.0383]])\n",
            "T63:  tensor([[  19.9663,   12.1730,   30.2972,   16.1358,   40.4762,   18.8483,\n",
            "           15.2454],\n",
            "        [ 416.7503,  259.8867,  526.9755,  351.3604,  378.7622,  259.1366,\n",
            "         2281.6646]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3480, 2.2450, 2.1345, 2.2724, 2.2471, 2.3339, 2.2431],\n",
            "        [2.6412, 2.7856, 2.8194, 2.7202, 2.5454, 2.4140, 2.4369]])\n",
            "T63:  tensor([[ 329.5944,   92.3222,   21.4270,  130.5769,   94.8620,  278.3052,\n",
            "           90.1148],\n",
            "        [ 197.8595,  863.7137, 1200.1575,  449.1936,   69.7559,   15.1617,\n",
            "           19.9379]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.8526, 2.6596, 2.8303, 2.9116, 2.8976, 2.7340, 2.7355],\n",
            "        [1.8684, 1.9258, 1.8885, 1.9064, 2.0162, 1.8759, 1.8681]])\n",
            "T63:  tensor([[ 739.5405,  104.8340,  595.9382, 1295.7218, 1135.7860,  227.9230,\n",
            "          231.3836],\n",
            "        [ 116.7014,  279.0594,  159.1160,  208.6543, 1029.2847,  131.0152,\n",
            "          116.0630]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7639, 1.7100, 1.7814, 1.7787, 1.8299, 1.8746, 1.7916],\n",
            "        [2.4122, 2.4599, 2.3487, 2.2706, 2.3920, 2.2631, 2.2628]])\n",
            "T63:  tensor([[  20.8535,    8.2179,   27.9555,   26.7215,   61.6742,  124.7248,\n",
            "           33.1056],\n",
            "        [ 797.9723, 1371.8810,  379.3219,  146.0145,  632.1205,  132.9675,\n",
            "          132.5195]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.4919, 2.3603, 2.4703, 2.1562, 2.3958, 2.2475, 2.3785],\n",
            "        [1.7038, 1.8531, 1.9151, 1.9343, 1.9887, 1.8202, 1.9457]])\n",
            "T63:  tensor([[1531.6357,  339.0028, 1205.0814,   25.4621,  515.0560,   84.4818,\n",
            "          420.4796],\n",
            "        [   7.8269,   94.5994,  245.2309,  326.2130,  717.9717,   56.0592,\n",
            "          385.7903]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.8848, 1.7597, 1.7544, 1.7622, 1.8312, 1.7471, 1.7531],\n",
            "        [2.8452, 2.6321, 2.7573, 2.9834, 2.3521, 2.7013, 3.0108]])\n",
            "T63:  tensor([[1.5197e+02, 2.0226e+01, 1.8503e+01, 2.1111e+01, 6.5619e+01, 1.6325e+01,\n",
            "         1.8102e+01],\n",
            "        [7.7419e+02, 8.8320e+01, 3.2518e+02, 2.8106e+03, 3.3380e+00, 1.8348e+02,\n",
            "         3.5944e+03]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.2916, 2.3544, 2.4323, 2.3148, 2.4002, 2.1864, 2.2148],\n",
            "        [1.8052, 1.8642, 1.8620, 1.8296, 1.8900, 1.9291, 1.9534]])\n",
            "T63:  tensor([[ 199.2203,  426.1340, 1054.1039,  264.7962,  729.2685,   52.0492,\n",
            "           75.4036],\n",
            "        [  41.2193,  105.6219,  102.1201,   61.1417,  157.4313,  283.7560,\n",
            "          405.7296]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.4255, 2.2647, 2.2170, 2.2005, 2.3253, 2.2139, 2.1482],\n",
            "        [3.0726, 2.7820, 3.0305, 2.7678, 2.9388, 2.8769, 2.8978]])\n",
            "T63:  tensor([[1167.6880,  171.5083,   93.5565,   75.4780,  360.9458,   89.8977,\n",
            "           37.7301],\n",
            "        [3782.8379,  248.4282, 2607.3157,  215.3660, 1131.5206,  630.5443,\n",
            "          769.3257]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.6761, 2.7146, 3.2029, 2.5778, 2.9303, 2.7250, 2.6578],\n",
            "        [1.7588, 1.9003, 1.8394, 1.8993, 1.8768, 1.9368, 1.8510]])\n",
            "T63:  tensor([[  106.6730,   159.6529, 14436.7715,    36.6281,  1327.3677,   177.7689,\n",
            "            87.7273],\n",
            "        [   19.4428,   187.9726,    73.0509,   185.2245,   131.1188,   324.9225,\n",
            "            87.6843]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.6559, 2.8271, 2.8850, 2.7940, 2.8651, 2.7779, 2.7906],\n",
            "        [1.8539, 1.7967, 1.7529, 1.7579, 1.7853, 1.7511, 1.8152]])\n",
            "T63:  tensor([[ 213.6762, 1197.7732, 2076.2817,  868.0610, 1721.0095,  741.1855,\n",
            "          839.2562],\n",
            "        [  92.2738,   36.7987,   17.7033,   19.2507,   30.4965,   17.1658,\n",
            "           49.7776]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[3.0581, 2.6939, 2.7331, 2.8301, 2.9232, 2.8099, 2.8192],\n",
            "        [2.7643, 2.8457, 2.9030, 3.0330, 2.9697, 3.1225, 2.6690]])\n",
            "T63:  tensor([[ 3177.8438,    95.5080,   143.7650,   381.6687,   932.4781,   312.6820,\n",
            "           342.6744],\n",
            "        [  386.7555,   860.7283,  1483.2500,  4832.0503,  2742.1851, 10472.5527,\n",
            "           145.2291]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.4994, 2.2154, 2.2748, 2.2749, 2.3842, 2.2914, 2.3131],\n",
            "        [2.3139, 2.2563, 2.2583, 2.2465, 2.2139, 2.3100, 2.1516]])\n",
            "T63:  tensor([[2290.6919,   78.2473,  166.4414,  166.6491,  623.6149,  204.5174,\n",
            "          267.0656],\n",
            "        [ 241.2198,  118.0480,  121.0412,  104.2167,   68.6007,  230.1450,\n",
            "           30.0222]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3061, 2.5140, 2.3845, 2.3875, 2.1193, 2.4479, 2.4319],\n",
            "        [2.8435, 2.8182, 3.0361, 2.9305, 3.2756, 2.7209, 2.9232]])\n",
            "T63:  tensor([[2.0469e+02, 2.2560e+03, 5.2396e+02, 5.4229e+02, 1.7957e+01, 1.0848e+03,\n",
            "         9.0491e+02],\n",
            "        [6.2468e+02, 4.8784e+02, 3.7224e+03, 1.4274e+03, 2.7898e+04, 1.8325e+02,\n",
            "         1.3329e+03]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.7423, 2.7873, 2.6484, 3.1012, 2.9189, 2.9478, 2.7581],\n",
            "        [2.4464, 2.2104, 2.1699, 2.2548, 2.2745, 2.2238, 2.2580]])\n",
            "T63:  tensor([[ 262.6923,  413.0540,   98.5989, 7475.7710, 1464.3951, 1914.3650,\n",
            "          308.0569],\n",
            "        [ 932.4448,   52.9941,   30.9862,   93.9012,  120.3075,   63.0836,\n",
            "           97.7321]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8674, 1.8903, 1.8355, 1.6902, 1.7546, 1.8167, 1.7736],\n",
            "        [1.8542, 1.7708, 1.7566, 1.8205, 1.7792, 1.7344, 1.7381]])\n",
            "T63:  tensor([[112.5203, 160.2680,  68.1248,   5.8256,  17.9615,  50.3061,  24.7522],\n",
            "        [ 96.6971,  24.9597,  19.6663,  56.5378,  28.7554,  13.4311,  14.3290]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2967, 2.2348, 2.4067, 2.4413, 2.2574, 2.2864, 2.4254],\n",
            "        [2.4150, 2.1956, 2.3095, 2.3821, 2.2015, 2.4406, 2.2794]])\n",
            "T63:  tensor([[ 197.9680,   90.9271,  734.5857, 1091.0959,  121.2248,  174.2312,\n",
            "          910.3507],\n",
            "        [ 787.4062,   53.1020,  225.3224,  537.2231,   57.3595, 1054.3802,\n",
            "          155.3859]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.4989, 2.3117, 2.2852, 2.1338, 2.2601, 2.4061, 2.4321],\n",
            "        [1.8137, 1.7416, 1.7802, 1.8423, 1.7998, 1.7421, 1.8006]])\n",
            "T63:  tensor([[1664.5457,  189.8154,  136.8441,   18.8752,   99.8140,  584.3878,\n",
            "          788.1704],\n",
            "        [  48.3154,   14.4916,   27.8610,   76.4065,   38.5540,   14.6165,\n",
            "           39.0332]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2108, 2.2010, 2.1280, 2.2251, 2.2129, 2.0705, 2.3673],\n",
            "        [2.7852, 2.8063, 2.5942, 2.7271, 2.6699, 2.7160, 2.8170]])\n",
            "T63:  tensor([[ 72.9204,  64.2299,  24.1569,  87.7595,  75.0011,  10.7890, 505.4306],\n",
            "        [308.0778, 380.1153,  41.3663, 170.8380,  93.9558, 152.4573, 422.4066]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.8630, 2.0268, 1.9057, 1.8595, 1.9469, 1.9086, 1.8571],\n",
            "        [1.8106, 1.8972, 1.8820, 1.9176, 1.9929, 1.9204, 1.8943]])\n",
            "T63:  tensor([[ 103.3531, 1152.0378,  199.2111,   97.8284,  368.2267,  208.3563,\n",
            "           94.2484],\n",
            "        [  46.3679,  180.7246,  143.4159,  245.9948,  737.3842,  256.4813,\n",
            "          173.1364]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.9574, 1.8246, 1.9262, 1.8123, 1.8660, 1.7857, 1.8279],\n",
            "        [2.9126, 2.8850, 2.8156, 2.8896, 2.8061, 3.0608, 2.8213]])\n",
            "T63:  tensor([[ 460.4081,   60.4007,  290.8534,   49.5797,  116.3044,   32.0574,\n",
            "           63.7348],\n",
            "        [ 926.8598,  713.9297,  363.8563,  745.4803,  331.2115, 3569.7319,\n",
            "          384.9052]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.5872, 2.8639, 2.7523, 2.7693, 2.9409, 2.6580, 2.8564],\n",
            "        [2.6578, 2.9523, 2.7432, 2.8512, 2.8240, 2.8767, 2.7603]])\n",
            "T63:  tensor([[  64.5827, 1097.8976,  367.6672,  436.1723, 2255.4695,  138.7319,\n",
            "         1021.4907],\n",
            "        [ 123.9615, 2248.1338,  300.4449,  871.5366,  669.9183, 1111.7528,\n",
            "          357.0543]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.7522, 2.6848, 2.8862, 2.9485, 3.0204, 2.7951, 2.6574],\n",
            "        [2.4077, 2.3664, 2.0451, 2.2524, 2.4682, 2.4041, 2.2980]])\n",
            "T63:  tensor([[ 194.4339,   96.6516,  728.2620, 1306.3213, 2513.1541,  299.5472,\n",
            "           72.2442],\n",
            "        [ 952.2921,  589.3228,    8.9093,  146.8216, 1885.6565,  913.3090,\n",
            "          259.0334]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.5645, 2.4993, 2.8946, 2.7115, 2.3620, 2.9033, 2.5371],\n",
            "        [1.8697, 2.0004, 1.9479, 1.8915, 1.8479, 1.8920, 1.8584]])\n",
            "T63:  tensor([[ 311.6637,  151.0931, 8475.9639, 1453.3784,   29.9673, 9183.6299,\n",
            "          230.4506],\n",
            "        [ 116.8930,  810.3765,  380.8385,  163.7145,   83.1468,  165.0206,\n",
            "           98.1252]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3593, 2.2294, 2.2659, 2.2777, 2.1666, 2.3800, 2.2044],\n",
            "        [2.8923, 2.8159, 2.7544, 2.8162, 2.8611, 2.9190, 2.6290]])\n",
            "T63:  tensor([[ 385.2874,   77.3016,  123.0155,  142.6682,   33.8785,  491.9921,\n",
            "           55.8918],\n",
            "        [ 954.3277,  456.5233,  247.0396,  457.7926,  708.8918, 1227.2944,\n",
            "           66.2823]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.5592, 2.8333, 2.8394, 2.6938, 2.6904, 2.9283, 2.9635],\n",
            "        [2.7901, 2.6800, 2.8110, 2.6635, 2.7452, 2.8411, 2.9437]])\n",
            "T63:  tensor([[  54.4550,  933.5458,  990.0868,  231.5058,  223.4558, 2286.7971,\n",
            "         3156.7893],\n",
            "        [ 468.5196,  152.1864,  575.7822,  127.8702,  298.4250,  772.0809,\n",
            "         2033.1577]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.3329, 2.2902, 2.2868, 2.2827, 2.2940, 2.3438, 2.2995],\n",
            "        [2.2926, 2.2733, 2.2517, 2.1970, 2.2953, 2.2557, 2.3267]])\n",
            "T63:  tensor([[295.5506, 175.4418, 168.0835, 159.7976, 183.8772, 336.9545, 196.8214],\n",
            "        [173.0167, 135.9763, 103.6226,  51.0838, 178.8599, 108.9299, 262.6275]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8180, 1.8304, 1.8280, 1.8174, 1.9157, 1.9545, 1.8860],\n",
            "        [1.9253, 1.8029, 1.8385, 1.7524, 1.7780, 1.8023, 1.7123]])\n",
            "T63:  tensor([[ 51.4177,  62.8076,  60.4190,  50.9778, 235.4776, 418.4729, 150.2151],\n",
            "        [294.5110,  43.6578,  77.4827,  18.8273,  28.9496,  43.2711,   9.3972]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.3814, 2.1561, 2.2902, 2.2146, 2.3452, 2.2463, 2.3063],\n",
            "        [2.3215, 2.4260, 2.2767, 2.1849, 2.1904, 2.2305, 2.2431]])\n",
            "T63:  tensor([[ 500.0404,   29.4035,  166.6972,   63.8279,  325.5396,   95.9699,\n",
            "          203.2800],\n",
            "        [ 368.7773, 1255.7141,  213.0389,   65.8233,   70.8213,  119.0118,\n",
            "          139.6552]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.9646, 2.7278, 2.7530, 3.1060, 2.9005, 3.1084, 2.6597],\n",
            "        [2.3472, 2.5107, 2.2816, 2.3527, 2.3966, 2.4632, 2.4120]])\n",
            "T63:  tensor([[2162.2563,  219.0996,  283.0267, 7548.6357, 1193.6365, 7704.2300,\n",
            "          107.5726],\n",
            "        [ 382.4890, 2463.7849,  171.9748,  408.1141,  683.8900, 1459.8175,\n",
            "          817.2824]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8111, 1.7974, 1.8416, 1.9567, 1.8946, 1.9092, 1.9676],\n",
            "        [2.8811, 2.8518, 2.9489, 3.0361, 2.8213, 2.6608, 2.9538]])\n",
            "T63:  tensor([[  47.0849,   37.6791,   76.7910,  442.0010,  175.1495,  218.4785,\n",
            "          517.7402],\n",
            "        [ 958.9806,  724.8549, 1807.1631, 3971.1123,  538.6931,  104.8154,\n",
            "         1891.2483]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.7982, 1.8159, 1.7135, 1.8068, 1.7681, 1.7848, 1.7958],\n",
            "        [3.0616, 3.0384, 2.9199, 2.6332, 2.9011, 2.8702, 3.0836]])\n",
            "T63:  tensor([[  37.3975,   49.9186,    8.8522,   43.0573,   22.6631,   29.9857,\n",
            "           35.9332],\n",
            "        [4089.7202, 3333.4946, 1133.2430,   63.4000,  950.2933,  707.8666,\n",
            "         4952.5527]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[3.0951, 2.7642, 2.8751, 2.5699, 2.6271, 2.6380, 2.6979],\n",
            "        [1.8092, 1.8562, 1.9386, 1.8492, 1.8361, 1.8432, 1.7929]])\n",
            "T63:  tensor([[8753.0566,  408.2264, 1203.1309,   52.4817,   98.1849,  110.3609,\n",
            "          207.5055],\n",
            "        [  45.1523,   95.7246,  335.4442,   85.7171,   69.6224,   77.9915,\n",
            "           34.5875]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4386, 2.9614, 2.7330, 2.7020, 2.6734, 2.8390, 2.7192],\n",
            "        [1.8670, 2.0148, 1.8068, 1.7613, 1.7791, 2.0494, 1.9338]])\n",
            "T63:  tensor([[  10.8675, 2545.1663,  282.1740,  205.0793,  152.1530,  807.1823,\n",
            "          244.8389],\n",
            "        [ 113.3074, 1001.2236,   43.4050,   20.4105,   27.4825, 1615.7863,\n",
            "          312.3474]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8171, 1.7614, 1.8188, 1.8826, 1.9371, 1.9749, 1.9372],\n",
            "        [2.3175, 2.2017, 2.4520, 2.4748, 2.2636, 2.2467, 2.3133]])\n",
            "T63:  tensor([[  53.9910,   21.4880,   55.4899,  151.3128,  344.1008,  596.6848,\n",
            "          344.6901],\n",
            "        [ 238.9542,   55.3884, 1154.3805, 1489.1721,  122.6222,   98.9915,\n",
            "          226.9964]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.7488, 1.8844, 1.9392, 1.8549, 1.9576, 1.8312, 1.8188],\n",
            "        [2.6335, 2.8586, 2.7522, 2.7499, 3.0075, 2.8093, 2.7391]])\n",
            "T63:  tensor([[  16.1600,  145.3546,  331.4970,   91.9259,  434.0201,   63.1050,\n",
            "           51.6991],\n",
            "        [ 102.0523,  998.8899,  351.1208,  342.9562, 3952.2183,  619.6190,\n",
            "          307.5287]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.9011, 1.8658, 1.8917, 2.0305, 1.8679, 1.8822, 1.9710],\n",
            "        [1.7998, 1.8643, 1.9283, 1.8076, 1.9164, 1.9575, 1.9769]])\n",
            "T63:  tensor([[ 192.1900,  111.6644,  166.5797, 1251.7058,  115.4252,  143.8225,\n",
            "          540.0009],\n",
            "        [  43.4433,  121.6325,  321.8133,   49.3626,  269.6054,  494.1855,\n",
            "          654.2362]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.8279, 1.7921, 1.8002, 1.7518, 1.8653, 1.8059, 1.7583],\n",
            "        [1.7465, 1.8117, 1.7644, 1.7690, 1.7677, 1.8253, 1.8145]])\n",
            "T63:  tensor([[ 61.8289,  34.5559,  39.4798,  17.5675, 111.6997,  43.3393,  19.6296],\n",
            "        [ 16.0220,  47.5266,  21.7311,  23.4782,  22.9495,  59.1711,  49.7181]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.7046, 2.9140, 2.7891, 2.6807, 2.6807, 2.8297, 2.9872],\n",
            "        [2.3130, 2.2127, 2.2426, 2.3185, 2.2582, 2.1955, 2.3830]])\n",
            "T63:  tensor([[ 114.4373,  913.0922,  271.6824,   88.9656,   88.9431,  406.4025,\n",
            "         1797.4558],\n",
            "        [ 202.3194,   56.9452,   83.7857,  216.2480,  102.2137,   45.5027,\n",
            "          467.7541]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7130, 2.7803, 2.5934, 2.6079, 2.7691, 2.5388, 2.8250],\n",
            "        [1.9359, 1.8360, 1.8054, 1.8982, 1.9160, 1.9004, 2.0091]])\n",
            "T63:  tensor([[229.8431, 453.9727,  64.3860,  75.4642, 406.1111,  35.0293, 704.8737],\n",
            "        [322.1267,  69.4709,  42.4274, 182.8595, 239.2345, 189.0977, 924.6078]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.7600, 3.0340, 2.7730, 2.7845, 2.5688, 2.8015, 2.6426],\n",
            "        [2.3883, 2.3268, 2.3549, 2.3697, 2.2684, 2.4486, 2.2777]])\n",
            "T63:  tensor([[ 247.8798, 3320.4021,  282.7405,  317.1772,   32.3218,  375.6849,\n",
            "           72.7602],\n",
            "        [ 566.1713,  272.2807,  381.7601,  455.1191,  132.4913, 1130.3906,\n",
            "          148.9705]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8876, 1.7265, 1.8221, 2.0003, 1.8197, 1.9090, 1.8944],\n",
            "        [2.2726, 2.1677, 2.1795, 2.3890, 2.2861, 2.1065, 2.3122]])\n",
            "T63:  tensor([[155.6600,  11.2307,  55.6772, 817.3000,  53.5489, 215.6792, 172.8441],\n",
            "        [130.9868,  33.6113,  39.3598, 535.3973, 154.8671,  14.5286, 213.7479]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3577, 2.4407, 2.3858, 2.3861, 2.4396, 2.3968, 2.3511],\n",
            "        [2.9431, 2.6725, 2.9091, 2.5396, 2.8688, 2.8170, 2.7169]])\n",
            "T63:  tensor([[ 384.9538, 1008.6679,  535.9747,  538.0806,  996.2784,  609.1842,\n",
            "          355.6689],\n",
            "        [1637.9636,  113.2383, 1193.6539,   26.3325,  815.0618,  493.6624,\n",
            "          180.1623]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.2860, 2.3439, 2.3261, 2.2015, 2.2768, 2.3290, 2.5054],\n",
            "        [2.2957, 2.1552, 2.2607, 2.2426, 2.2473, 2.2645, 2.3230]])\n",
            "T63:  tensor([[ 149.9602,  304.0725,  245.2985,   50.9650,  133.5733,  254.1338,\n",
            "         1932.2057],\n",
            "        [ 181.7191,   29.6042,  117.3001,   93.2561,   99.0215,  123.1195,\n",
            "          253.7715]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.5444, 2.2520, 2.3731, 2.3362, 2.2324, 2.5049, 2.6034],\n",
            "        [2.9802, 2.9646, 2.6687, 2.7276, 2.9958, 2.8749, 2.6691]])\n",
            "T63:  tensor([[3211.3823,  106.9981,  470.0604,  302.9830,   83.3183, 2097.0010,\n",
            "         5982.9194],\n",
            "        [3226.6611, 2802.7476,  155.3907,  286.0144, 3712.9326, 1218.2836,\n",
            "          156.0348]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.1393, 2.3237, 2.3574, 2.3713, 2.5485, 2.2876, 2.1697],\n",
            "        [2.9214, 2.9249, 2.9572, 2.9392, 2.9013, 2.7402, 2.6359]])\n",
            "T63:  tensor([[  23.8298,  255.8380,  383.7791,  452.2938, 3303.7795,  164.2284,\n",
            "           35.9529],\n",
            "        [ 988.2121, 1021.5941, 1378.3418, 1167.2284,  818.1782,  167.0198,\n",
            "           55.5365]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.7536, 2.7622, 2.8901, 3.0249, 2.9568, 2.7905, 2.7280],\n",
            "        [1.8542, 1.8647, 1.8998, 1.8843, 1.9461, 1.6994, 1.8523]])\n",
            "T63:  tensor([[ 191.7101,  209.0477,  735.2626, 2549.3462, 1374.1230,  278.1599,\n",
            "          147.3348],\n",
            "        [ 101.4449,  119.5350,  205.0045,  161.6763,  408.2604,    7.6452,\n",
            "           98.4166]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4262, 2.2986, 2.5507, 2.4619, 2.1573, 2.1498, 2.5175],\n",
            "        [1.8026, 1.9708, 1.8532, 1.9559, 1.8078, 1.8205, 1.8723]])\n",
            "T63:  tensor([[1118.7964,  247.7778, 4392.1167, 1671.3113,   40.4313,   36.5642,\n",
            "         3077.8809],\n",
            "        [  43.3833,  572.5640,   97.6601,  461.4384,   47.2136,   58.0041,\n",
            "          131.5013]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[3.0315, 3.0192, 2.8770, 2.6307, 3.1117, 3.0787, 2.9486],\n",
            "        [2.4161, 2.2979, 2.3801, 2.3245, 2.2129, 2.2747, 2.5058]])\n",
            "T63:  tensor([[2784.0833, 2494.5605,  668.7149,   54.2333, 5611.7588, 4219.0601,\n",
            "         1311.5521],\n",
            "        [ 829.0677,  203.4559,  546.0409,  281.5300,   69.4277,  152.6690,\n",
            "         2263.1804]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.3471, 2.2696, 2.3024, 2.1994, 2.1973, 2.4111, 2.2291],\n",
            "        [2.1059, 2.3106, 2.0911, 2.1277, 2.3798, 2.1696, 2.3223]])\n",
            "T63:  tensor([[304.5719, 117.7428, 176.8650,  47.7444,  46.4319, 646.5686,  70.2502],\n",
            "        [ 15.2157, 220.9255,  12.3686,  20.6104, 506.3197,  36.4020, 254.6518]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8158, 1.8434, 1.8884, 1.9125, 1.8430, 1.9199, 1.8136],\n",
            "        [2.1929, 2.2649, 2.3064, 2.1841, 2.4067, 2.1868, 2.2612]])\n",
            "T63:  tensor([[ 51.2266,  79.7348, 160.7840, 231.5950,  79.1768, 258.5974,  49.4222],\n",
            "        [ 49.8868, 126.0003, 211.0243,  44.4099, 696.2404,  45.9902, 120.2257]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.6543, 2.7305, 2.7488, 2.7752, 2.8988, 2.8039, 2.9347],\n",
            "        [1.9255, 1.7771, 1.8870, 1.9202, 1.7871, 1.9813, 2.0001]])\n",
            "T63:  tensor([[  97.4927,  216.4127,  260.9277,  340.2929, 1130.4387,  452.9240,\n",
            "         1580.4634],\n",
            "        [ 292.8408,   28.2829,  163.6774,  270.6473,   33.4169,  661.0146,\n",
            "          863.1138]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.1060, 2.2922, 2.5237, 2.4528, 2.3300, 2.4249, 2.2769],\n",
            "        [2.8524, 2.9128, 2.9230, 2.7935, 2.7024, 2.8124, 2.9123]])\n",
            "T63:  tensor([[  21.3063,  243.6685, 3499.6958, 1605.4271,  385.8216, 1171.9695,\n",
            "          201.5847],\n",
            "        [ 605.2743, 1076.0983, 1183.7710,  339.4108,  134.1795,  409.2689,\n",
            "         1070.6877]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7771, 1.8839, 1.7954, 1.8143, 1.8009, 1.7095, 1.7929],\n",
            "        [2.6493, 2.8935, 2.5817, 2.8134, 2.7410, 2.8734, 2.9353]])\n",
            "T63:  tensor([[  25.5515,  141.5631,   34.6194,   47.1390,   37.9184,    8.0014,\n",
            "           33.2475],\n",
            "        [  83.2831,  972.3676,   39.7847,  448.7798,  217.2513,  803.0112,\n",
            "         1438.1447]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.8168, 2.7088, 2.8274, 2.9181, 2.7037, 2.9007, 2.7888],\n",
            "        [3.0666, 2.9384, 2.6230, 3.1259, 2.8888, 2.7274, 2.7180]])\n",
            "T63:  tensor([[  666.7332,   225.5432,   739.0659,  1750.4081,   213.9554,  1487.9196,\n",
            "           506.4222],\n",
            "        [ 6726.4644,  2136.0085,    92.1746, 11187.4385,  1346.3951,   276.1920,\n",
            "           250.8158]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.4384, 2.9831, 2.8358, 2.8859, 2.7756, 2.6658, 2.8121],\n",
            "        [2.8656, 2.8807, 2.6217, 2.6106, 2.9389, 2.7268, 2.6925]])\n",
            "T63:  tensor([[   5.5668, 1701.3959,  423.5225,  687.3794,  232.9410,   74.5483,\n",
            "          335.4505],\n",
            "        [1907.7383, 2198.2690,  163.9688,  145.6266, 3770.9268,  490.4518,\n",
            "          345.1529]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2838, 2.1813, 2.2868, 2.2887, 2.2005, 2.2971, 2.1950],\n",
            "        [2.2589, 2.1445, 2.1848, 2.2755, 2.2943, 2.3067, 2.2720]])\n",
            "T63:  tensor([[213.2901,  57.4885, 221.3559, 226.6855,  74.0208, 251.2686,  68.8936],\n",
            "        [170.6526,  38.3779,  65.7920, 209.8430, 264.9250, 308.2605, 201.0244]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8907, 1.8982, 1.9247, 1.9814, 1.8937, 1.9383, 1.8915],\n",
            "        [2.5783, 2.7704, 2.7653, 2.6301, 2.6740, 3.0169, 2.7417]])\n",
            "T63:  tensor([[ 163.4735,  183.2595,  273.0656,  625.3655,  171.1102,  333.9083,\n",
            "          165.4685],\n",
            "        [  70.2058,  525.5101,  499.5655,  123.3764,  196.3040, 5311.0078,\n",
            "          394.0397]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3502, 2.2877, 2.2686, 2.2076, 2.2366, 2.1394, 2.3532],\n",
            "        [2.2054, 2.4055, 2.1528, 2.2962, 2.3302, 2.3043, 2.1714]])\n",
            "T63:  tensor([[360.9643, 168.6643, 132.8557,  60.8291,  88.5723,  24.4758, 374.2734],\n",
            "        [ 55.2184, 646.7248,  27.4357, 175.3044, 265.5247, 193.6873,  35.2325]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8429, 1.7700, 1.7932, 1.7938, 1.7195, 1.7563, 1.7818],\n",
            "        [2.8317, 2.9524, 3.0081, 2.7910, 2.7226, 2.8447, 3.0314]])\n",
            "T63:  tensor([[  76.8338,   23.3782,   34.4017,   34.7529,    9.8296,   18.5549,\n",
            "           28.4814],\n",
            "        [ 559.4195, 1754.8671, 2909.4175,  374.6262,  187.5712,  634.8248,\n",
            "         3581.2256]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.1795, 2.2090, 2.5266, 2.4266, 2.2526, 2.4243, 2.2604],\n",
            "        [1.8036, 1.8993, 1.8897, 1.7796, 1.8976, 1.8552, 1.8600]])\n",
            "T63:  tensor([[  40.0946,   59.1277, 2555.3992,  841.7961,  103.6109,  819.7769,\n",
            "          114.3143],\n",
            "        [  42.0766,  190.0139,  163.9610,   28.3407,  185.2089,   96.2196,\n",
            "          103.6892]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7835, 2.5645, 2.9264, 2.7789, 2.5743, 2.7573, 3.0257],\n",
            "        [1.7353, 1.7733, 1.8190, 1.7400, 1.7757, 1.7555, 1.8174]])\n",
            "T63:  tensor([[ 432.5992,   43.0531, 1706.2886,  413.2971,   48.0157,  333.0488,\n",
            "         4195.5581],\n",
            "        [  12.7802,   24.3911,   51.7340,   13.8683,   25.3840,   18.0585,\n",
            "           50.4333]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.9352, 1.8167, 1.7549, 1.9456, 1.9765, 1.8508, 1.8151],\n",
            "        [1.8412, 1.7895, 1.8064, 1.8399, 1.8067, 1.8555, 1.7981]])\n",
            "T63:  tensor([[339.4982,  54.4619,  19.5608, 395.9861, 619.6212,  93.7715,  53.0884],\n",
            "        [ 77.7173,  33.6476,  44.4632,  76.1688,  44.6420,  97.4360,  38.8069]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2849, 2.1891, 2.3238, 2.3532, 2.4712, 2.4199, 2.1517],\n",
            "        [1.7506, 1.8498, 1.8144, 1.8183, 1.8040, 1.8496, 1.8003]])\n",
            "T63:  tensor([[ 147.5826,   43.1630,  237.8539,  339.0874, 1321.9340,  739.9630,\n",
            "           26.1574],\n",
            "        [  16.6583,   84.7579,   48.1834,   51.3082,   40.6427,   84.6106,\n",
            "           38.2957]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.6756, 2.8231, 2.3866, 2.9033, 2.6889, 2.9799, 2.4890],\n",
            "        [1.7963, 1.7756, 1.7438, 1.7536, 1.8596, 1.7837, 1.7461]])\n",
            "T63:  tensor([[ 264.8579, 1165.0170,   10.0296, 2492.9517,  304.1212, 5020.5776,\n",
            "           34.0945],\n",
            "        [  35.6598,   25.2633,   14.7476,   17.4456,   98.4721,   28.9620,\n",
            "           15.3551]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.5393, 2.2538, 2.2187, 2.4136, 2.3975, 2.4347, 2.4529],\n",
            "        [2.7094, 2.7379, 3.0105, 2.8914, 2.3424, 2.8432, 2.6623]])\n",
            "T63:  tensor([[3.0022e+03, 1.0790e+02, 6.8779e+01, 7.4245e+02, 6.1667e+02, 9.4509e+02,\n",
            "         1.1609e+03],\n",
            "        [1.6397e+02, 2.2005e+02, 2.9806e+03, 9.9520e+02, 2.3780e+00, 6.2709e+02,\n",
            "         9.9997e+01]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.9732, 2.8226, 2.8168, 2.7731, 2.8665, 2.6511, 2.9304],\n",
            "        [2.5737, 2.3523, 2.1864, 2.3238, 2.2686, 2.3120, 2.4982]])\n",
            "T63:  tensor([[2356.6155,  568.4084,  536.9661,  347.9622,  869.1072,   98.2027,\n",
            "         1588.9436],\n",
            "        [4326.2622,  362.0801,   45.0299,  256.8429,  130.0052,  222.6861,\n",
            "         1921.4805]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.2654, 2.1508, 2.1552, 2.1853, 2.3219, 2.1113, 2.2151],\n",
            "        [1.8540, 2.0005, 1.8826, 1.8319, 1.8326, 1.9870, 1.8458]])\n",
            "T63:  tensor([[128.7537,  28.8396,  30.6092,  45.7963, 258.7263,  16.7688,  67.6631],\n",
            "        [ 91.6553, 812.1897, 143.0355,  64.5251,  65.3158, 670.4771,  80.5197]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.3096, 2.3712, 2.3640, 2.3468, 2.4214, 2.2328, 2.3027],\n",
            "        [2.8609, 2.7859, 2.8266, 2.8836, 2.6539, 2.6764, 2.7473]])\n",
            "T63:  tensor([[ 234.7583,  491.9043,  451.5117,  367.9126,  881.2024,   89.6883,\n",
            "          215.7874],\n",
            "        [ 996.2686,  480.0016,  715.4653, 1236.0576,  123.7716,  156.8720,\n",
            "          326.1212]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.8753, 1.7782, 1.8306, 1.7885, 2.0283, 1.7776, 1.8870],\n",
            "        [1.7345, 1.6550, 1.6841, 1.7017, 1.6849, 1.6468, 1.6478]])\n",
            "T63:  tensor([[ 131.0776,   27.5749,   64.9441,   32.7133, 1230.0861,   27.3088,\n",
            "          156.8974],\n",
            "        [ 102.6707,   25.9960,   43.4518,   58.8943,   44.0016,   22.4556,\n",
            "           22.8448]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.3263, 2.3243, 2.4450, 2.1778, 2.4030, 2.2612, 2.3305],\n",
            "        [2.1801, 2.1250, 2.3296, 2.2614, 2.1891, 2.1995, 2.2825]])\n",
            "T63:  tensor([[ 257.9064,  251.7823, 1034.2864,   39.1167,  639.6618,  115.3483,\n",
            "          271.4706],\n",
            "        [  53.3502,   25.3807,  351.7435,  152.2258,   60.0562,   68.9021,\n",
            "          197.9977]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.9490, 2.7898, 2.9977, 2.7680, 2.6120, 2.9141, 2.7580],\n",
            "        [2.6484, 2.8761, 2.8633, 2.8296, 2.5145, 2.5987, 2.8287]])\n",
            "T63:  tensor([[1728.3491,  376.5892, 2692.6824,  302.9862,   58.9571, 1250.8665,\n",
            "          273.7789],\n",
            "        [ 143.7404, 1405.2697, 1244.8877,  900.8904,   32.8754,   84.1867,\n",
            "          892.9423]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2310, 2.3536, 2.2543, 2.3280, 2.4183, 2.1423, 2.3359],\n",
            "        [2.9113, 2.7644, 2.6404, 2.9970, 3.0916, 2.8548, 2.8723]])\n",
            "T63:  tensor([[  86.2786,  392.6014,  116.0563,  288.9333,  836.4503,   26.6803,\n",
            "          317.7738],\n",
            "        [1310.6371,  315.0306,   86.7745, 2875.1616, 6614.6567,  766.7137,\n",
            "          906.6061]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.6592, 2.4853, 2.7525, 2.8222, 2.7857, 2.6670, 2.8124],\n",
            "        [1.8387, 1.8034, 1.8221, 1.8341, 1.8993, 1.8473, 2.0196]])\n",
            "T63:  tensor([[ 122.1748,   17.5483,  321.3381,  642.6821,  448.2560,  132.7734,\n",
            "          583.8260],\n",
            "        [  72.2119,   40.9080,   55.3822,   67.1391,  185.2635,   82.8295,\n",
            "         1066.5068]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7792, 1.7843, 1.8002, 1.8235, 1.7385, 1.8291, 1.8154],\n",
            "        [2.7581, 2.7919, 2.7722, 3.0556, 2.5090, 2.6852, 2.8944]])\n",
            "T63:  tensor([[  27.8986,   30.3941,   39.4689,   57.6395,   14.0097,   63.0073,\n",
            "           50.5234],\n",
            "        [ 295.5990,  414.7260,  340.8599, 4835.7021,   20.0489,  139.6112,\n",
            "         1118.2565]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.7145, 2.9719, 2.8525, 2.8740, 2.9901, 2.5822, 2.7662],\n",
            "        [1.8808, 1.7512, 2.0443, 1.8058, 1.7966, 1.8009, 1.7628]])\n",
            "T63:  tensor([[ 143.6058, 1762.5626,  573.0813,  704.9844, 2080.1230,   34.5480,\n",
            "          243.9335],\n",
            "        [ 146.2477,   17.9462, 1569.7815,   44.5601,   38.3470,   41.1335,\n",
            "           21.8552]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2485, 2.2020, 2.2360, 2.3336, 2.3451, 2.4463, 2.2685],\n",
            "        [1.8830, 1.8029, 1.9158, 1.8202, 1.8833, 1.8613, 1.8410]])\n",
            "T63:  tensor([[ 91.3250,  50.0566,  77.8328, 262.3152, 301.2840, 979.9173, 117.6401],\n",
            "        [142.3723,  40.0057, 234.4127,  52.9226, 143.1312, 101.6649,  73.8159]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.3403, 2.4240, 2.2715, 2.1574, 2.4550, 2.3611, 2.4129],\n",
            "        [2.1802, 2.2943, 2.0960, 2.2517, 2.4263, 2.2962, 2.3978]])\n",
            "T63:  tensor([[ 316.3786,  844.3785,  136.0806,   30.8679, 1198.7445,  405.6268,\n",
            "          743.1252],\n",
            "        [  46.6879,  200.8496,   14.7894,  117.9404,  959.9161,  205.6059,\n",
            "          691.8578]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.7157, 2.8234, 2.7288, 2.9001, 3.0610, 2.7682, 2.7667],\n",
            "        [2.8375, 3.0218, 2.6894, 2.4712, 3.1497, 3.1332, 2.9437]])\n",
            "T63:  tensor([[  128.4776,   381.9686,   147.1229,   800.8522,  3477.1079,   220.1479,\n",
            "           216.7664],\n",
            "        [  643.0094,  3562.7114,   144.4988,    12.7345, 10761.3438,  9365.8145,\n",
            "          1755.2830]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[3.0168, 2.7786, 2.9998, 2.7572, 2.6143, 2.6584, 2.8743],\n",
            "        [2.8030, 2.9105, 2.6424, 2.7861, 2.8415, 2.8060, 2.7859]])\n",
            "T63:  tensor([[3183.7483,  335.4389, 2733.0291,  270.6546,   60.2705,   96.9737,\n",
            "          854.8057],\n",
            "        [ 626.7108, 1751.2708,  120.9858,  530.1985,  910.9262,  644.9385,\n",
            "          529.1476]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.8542, 1.9856, 1.7570, 1.8336, 1.9635, 1.7983, 1.8747],\n",
            "        [2.7635, 2.9431, 2.9815, 2.9061, 2.8097, 2.8043, 2.6696]])\n",
            "T63:  tensor([[  91.8412,  656.3066,   18.7747,   66.2718,  477.8168,   37.4272,\n",
            "          126.4365],\n",
            "        [ 347.3935, 1955.2316, 2776.0178, 1387.2261,  549.9250,  521.5830,\n",
            "          131.9538]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3685, 2.3207, 2.2419, 2.2835, 2.1773, 2.1289, 2.3232],\n",
            "        [2.2625, 2.2973, 2.1613, 2.2265, 2.0942, 2.2269, 2.2775]])\n",
            "T63:  tensor([[408.8076, 230.3486,  86.2099, 145.6921,  37.0777,  19.2471, 237.3813],\n",
            "        [107.8673, 166.4760,  28.8021,  68.0508,  11.4139,  68.4446, 130.1604]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3796, 2.0876, 2.1661, 2.3602, 2.4322, 2.3125, 2.3986],\n",
            "        [2.2032, 2.2129, 2.3006, 2.3096, 2.3117, 2.2270, 2.2144]])\n",
            "T63:  tensor([[ 597.0600,   14.0639,   41.3871,  474.8601, 1094.5348,  267.9492,\n",
            "          744.7930],\n",
            "        [  50.1891,   56.9568,  173.1552,  193.3196,  198.3489,   68.3520,\n",
            "           58.0926]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.0670, 2.2965, 2.2076, 2.3970, 2.4033, 2.4196, 2.6300],\n",
            "        [1.8054, 1.8404, 2.0184, 1.7760, 1.7688, 1.9203, 1.6873]])\n",
            "T63:  tensor([[1.1061e+01, 2.3119e+02, 7.5166e+01, 7.6657e+02, 8.2476e+02, 9.9448e+02,\n",
            "         9.6268e+03],\n",
            "        [4.2862e+01, 7.5214e+01, 1.0626e+03, 2.6352e+01, 2.3372e+01, 2.5762e+02,\n",
            "         5.6600e+00]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2686, 2.3828, 2.4468, 2.3145, 2.2408, 2.2816, 2.3627],\n",
            "        [2.9532, 2.8168, 2.7675, 2.7462, 2.6312, 2.5934, 2.6761]])\n",
            "T63:  tensor([[ 117.0202,  468.6113,  978.8604,  206.7342,   82.2926,  137.7459,\n",
            "          369.6584],\n",
            "        [1867.3105,  511.9676,  313.5861,  252.8902,   75.8095,   50.1651,\n",
            "          122.3712]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.1710, 2.4058, 2.5157, 2.4930, 2.3197, 2.4977, 2.3260],\n",
            "        [1.8203, 1.9422, 1.8340, 1.9458, 1.9848, 1.8559, 1.7607]])\n",
            "T63:  tensor([[  38.3252,  708.0289, 2424.7688, 1891.8762,  255.2488, 1993.6646,\n",
            "          275.5157],\n",
            "        [  57.8080,  377.3055,   72.0809,  397.8527,  699.9819,  101.8281,\n",
            "           21.6394]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.4260, 2.3265, 2.3725, 2.3358, 2.4091, 2.4911, 2.3172],\n",
            "        [1.7518, 1.7979, 1.7806, 1.7754, 1.7364, 1.7630, 1.8132]])\n",
            "T63:  tensor([[ 973.1040,  302.8467,  523.5469,  338.7563,  802.1163, 2016.1235,\n",
            "          270.5778],\n",
            "        [  16.8782,   36.5338,   27.4331,   25.1543,   12.9715,   20.4104,\n",
            "           46.8624]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.9029, 1.9664, 1.8439, 1.8541, 1.8110, 1.7772, 1.8266],\n",
            "        [2.6880, 2.8952, 2.9963, 2.9422, 3.0367, 2.8539, 2.7551]])\n",
            "T63:  tensor([[ 195.5102,  500.4762,   78.4470,   92.0801,   46.2652,   26.4971,\n",
            "           59.4846],\n",
            "        [ 112.2916,  887.6081, 2262.3174, 1379.3047, 3245.7942,  598.0976,\n",
            "          224.7795]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.9906, 2.7947, 2.6636, 2.6438, 2.8642, 2.9520, 2.9854],\n",
            "        [2.6186, 2.8840, 3.0110, 2.8523, 2.8971, 2.8013, 2.8220]])\n",
            "T63:  tensor([[2049.1277,  318.4010,   82.3437,   66.5588,  628.8948, 1438.4172,\n",
            "         1954.2445],\n",
            "        [  66.8204,  988.1556, 3183.3945,  730.1948, 1118.9149,  443.8164,\n",
            "          544.1674]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2972, 2.3355, 2.2612, 2.3470, 2.2607, 2.3147, 2.2615],\n",
            "        [2.0135, 1.7706, 1.8322, 1.8115, 1.9529, 1.7906, 1.8271]])\n",
            "T63:  tensor([[170.4316, 272.1530, 108.7964, 312.4609, 108.0528, 211.3642, 109.2044],\n",
            "        [972.1589,  23.5510,  64.6375,  46.2850, 408.3935,  32.8827,  59.5549]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2698, 2.2469, 2.0760, 2.4287, 2.2303, 2.3202, 2.5774],\n",
            "        [2.2814, 2.2018, 2.2729, 2.2574, 2.2226, 2.3399, 2.1948]])\n",
            "T63:  tensor([[ 126.9853,   95.0581,    9.4898,  850.5657,   76.8851,  236.7005,\n",
            "         4333.4614],\n",
            "        [ 143.5538,   51.8722,  128.9693,  106.1556,   68.0055,  293.8126,\n",
            "           47.3087]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.8291, 1.7031, 1.7588, 1.7895, 1.8303, 1.8095, 1.8165],\n",
            "        [1.7697, 1.7831, 1.7853, 1.7607, 1.8420, 1.8625, 1.7954]])\n",
            "T63:  tensor([[ 62.7567,   7.4905,  19.7076,  32.9051,  63.9542,  45.6768,  51.2477],\n",
            "        [ 22.9815,  28.7704,  29.8471,  19.7430,  74.8889, 103.4722,  35.2710]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.6619, 2.8759, 2.7538, 2.8233, 2.7547, 2.5849, 2.7858],\n",
            "        [2.2962, 2.1327, 2.3476, 2.2210, 2.4258, 2.3934, 2.4031]])\n",
            "T63:  tensor([[102.9299, 885.2392, 266.8069, 533.1996, 269.2849,  44.5890, 368.0948],\n",
            "        [214.6686,  25.7182, 400.0278,  83.1199, 996.2431, 685.5052, 767.4526]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7585, 2.5685, 2.6859, 2.7704, 2.6764, 2.8389, 2.7797],\n",
            "        [2.9550, 2.9168, 2.8516, 2.7589, 2.7441, 2.9585, 2.8209]])\n",
            "T63:  tensor([[ 468.8468,   63.2557,  223.2874,  527.9072,  202.2272, 1032.4028,\n",
            "          578.8258],\n",
            "        [1966.7880, 1380.4592,  743.2395,  297.9316,  256.2563, 2029.2810,\n",
            "          551.7166]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.4714, 2.5360, 2.3852, 2.3274, 2.4821, 2.2835, 2.3335],\n",
            "        [2.7716, 2.7730, 2.7945, 2.6059, 2.7450, 2.6802, 2.9402]])\n",
            "T63:  tensor([[1810.7943, 3665.0027,  680.2642,  342.5272, 2038.7139,  200.3572,\n",
            "          368.7028],\n",
            "        [ 307.2546,  311.5917,  386.2313,   53.8406,  234.7277,  119.8412,\n",
            "         1562.1517]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7714, 1.8594, 1.7712, 1.7930, 1.7429, 1.8077, 1.7782],\n",
            "        [2.6917, 2.6976, 2.7431, 2.6457, 2.7691, 2.8790, 2.8067]])\n",
            "T63:  tensor([[  24.6838,  102.6497,   24.6082,   35.3321,   15.2207,   44.9529,\n",
            "           27.6428],\n",
            "        [ 171.5405,  182.5821,  291.3196,  105.4988,  378.9358, 1107.9738,\n",
            "          550.4852]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.6999, 2.9481, 2.8278, 2.8868, 2.9289, 2.7092, 2.9895],\n",
            "        [2.4031, 2.2591, 2.2811, 2.2634, 2.2423, 2.3570, 2.3004]])\n",
            "T63:  tensor([[ 100.5894, 1163.8818,  368.8304,  653.7250,  973.3184,  110.9456,\n",
            "         1702.7667],\n",
            "        [ 512.5954,   89.2801,  117.8301,   94.2587,   72.1231,  297.8082,\n",
            "          149.8024]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.7123, 1.7424, 1.8132, 1.8402, 1.8627, 1.7323, 1.7570],\n",
            "        [2.7884, 2.8890, 2.6785, 2.7665, 2.8432, 2.8965, 2.7193]])\n",
            "T63:  tensor([[   8.9899,   15.1639,   49.4610,   76.3016,  108.7162,   12.7371,\n",
            "           19.4822],\n",
            "        [ 354.9980,  943.9385,  115.1130,  285.2462,  608.7602, 1013.3058,\n",
            "          176.1998]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.7773, 1.7766, 1.7040, 1.7714, 1.8214, 1.7416, 1.7900],\n",
            "        [2.2775, 2.3392, 2.3977, 2.3538, 2.4090, 2.4260, 2.3901]])\n",
            "T63:  tensor([[ 27.2116,  26.9231,   7.7158,  24.6495,  56.1054,  14.8621,  33.6164],\n",
            "        [138.0408, 294.4196, 587.8303, 350.3039, 669.5233, 814.6423, 537.5562]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.7177, 1.7943, 1.7595, 1.7492, 1.8452, 1.7293, 1.7821],\n",
            "        [1.9283, 1.8715, 1.8374, 1.9110, 1.7544, 1.9455, 1.8791]])\n",
            "T63:  tensor([[  9.7302,  35.7326,  19.9917,  16.7709,  81.2680,  11.9036,  29.2314],\n",
            "        [284.8022, 120.2789,  70.3674, 219.8117,  17.9405, 367.0083, 135.2920]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.8053, 2.8133, 2.8074, 2.8321, 3.0895, 2.7726, 2.7173],\n",
            "        [2.3024, 2.3508, 2.1435, 2.1794, 2.1375, 2.3614, 2.2767]])\n",
            "T63:  tensor([[ 371.5995,  402.4338,  379.6768,  483.6483, 5158.7017,  268.2364,\n",
            "          152.4115],\n",
            "        [ 210.3738,  377.9076,   26.9708,   43.7184,   24.8733,  428.7534,\n",
            "          152.9934]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8275, 1.8083, 1.8032, 1.7478, 1.7972, 1.8011, 1.7176],\n",
            "        [1.7952, 1.8291, 1.8428, 1.7877, 1.8078, 1.7883, 1.8147]])\n",
            "T63:  tensor([[60.0071, 43.9578, 40.4865, 16.0269, 36.6887, 39.1205,  9.4905],\n",
            "        [40.2671, 69.7666, 86.7063, 35.5768, 49.4421, 35.9499, 55.3462]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.3700, 2.2629, 2.3344, 2.1690, 2.0931, 2.2192, 2.2994],\n",
            "        [2.7278, 2.6963, 2.6048, 2.6690, 2.7573, 2.6658, 2.7913]])\n",
            "T63:  tensor([[439.2972, 119.0056, 287.4099,  35.1678,  12.3922,  68.1357, 187.6476],\n",
            "        [485.1801, 351.5654, 133.9374, 265.1608, 652.7050, 256.2263, 913.4227]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.7427, 2.8335, 2.5958, 2.9428, 3.0255, 2.7670, 3.0113],\n",
            "        [2.2693, 2.3069, 2.3301, 2.4191, 2.3200, 2.3826, 2.3285]])\n",
            "T63:  tensor([[ 154.5543,  384.1649,   32.1212, 1091.2670, 2321.3340,  198.0276,\n",
            "         2044.0298],\n",
            "        [ 112.1559,  178.9114,  237.4433,  679.1193,  210.1574,  444.4580,\n",
            "          232.8927]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.9376, 2.6873, 2.4994, 2.7566, 2.9075, 2.8175, 2.8806],\n",
            "        [2.5079, 2.3209, 2.4624, 2.3639, 2.3247, 2.3594, 2.4127]])\n",
            "T63:  tensor([[1577.5602,  134.2951,   16.8304,  274.0450, 1192.1708,  502.7064,\n",
            "          924.0190],\n",
            "        [2111.2544,  245.3770, 1276.0966,  410.9878,  256.9340,  389.3503,\n",
            "          726.4247]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7395, 2.8989, 2.6981, 2.9148, 2.6744, 2.8976, 2.7465],\n",
            "        [2.6767, 2.7933, 2.8553, 2.9400, 2.7721, 2.7610, 2.8433]])\n",
            "T63:  tensor([[ 179.6587,  862.9253,  116.9723, 1003.1625,   91.1471,  852.4598,\n",
            "          193.0657],\n",
            "        [ 186.2743,  610.5053, 1115.0043, 2470.6165,  494.5760,  442.2564,\n",
            "          994.2128]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.8206, 2.9274, 2.7110, 2.9910, 2.8657, 2.9772, 2.9539],\n",
            "        [1.8784, 1.7942, 1.7542, 1.8868, 1.8817, 1.8718, 1.8871]])\n",
            "T63:  tensor([[ 645.1550, 1780.6415,  214.8655, 3181.7554,  996.7081, 2809.1099,\n",
            "         2272.7563],\n",
            "        [ 136.8674,   35.7396,   18.2962,  155.7252,  143.9168,  123.4321,\n",
            "          156.3140]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4164, 2.7235, 2.6611, 2.7475, 2.7342, 2.6645, 2.4491],\n",
            "        [2.0967, 2.1892, 2.2641, 2.2814, 2.2425, 2.1689, 2.2443]])\n",
            "T63:  tensor([[ 11.9689, 361.8401, 189.6029, 461.3352, 403.2763, 196.5658,  17.7206],\n",
            "        [ 13.7934,  48.6172, 127.5132, 158.2942,  96.9760,  37.0806,  99.3364]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.4182, 2.3433, 2.2920, 2.3645, 2.4020, 2.3155, 2.3190],\n",
            "        [2.2776, 2.2529, 2.2693, 2.3717, 2.2897, 2.4481, 2.2943]])\n",
            "T63:  tensor([[ 794.8159,  329.9308,  176.6468,  425.1292,  659.2855,  235.9195,\n",
            "          246.1071],\n",
            "        [ 139.4123,  102.1936,  125.6567,  436.7251,  162.1146, 1054.6892,\n",
            "          171.4878]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3787, 2.3740, 2.3220, 2.3134, 2.2699, 2.3497, 2.2272],\n",
            "        [1.7692, 1.7916, 1.9775, 1.7046, 1.9318, 1.7573, 1.8768]])\n",
            "T63:  tensor([[556.2093, 526.5436, 283.2630, 255.1752, 148.9663, 395.0951,  86.7359],\n",
            "        [ 22.7010,  32.9432, 575.7447,   7.4300, 295.3921,  18.5517, 128.4671]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2792, 2.2430, 2.1708, 2.2377, 2.2306, 2.2389, 2.3618],\n",
            "        [2.4248, 2.1462, 2.2826, 2.1695, 2.3647, 2.1174, 2.1988]])\n",
            "T63:  tensor([[150.1158,  95.1757,  37.0765,  88.9022,  81.2052,  90.3433, 409.8040],\n",
            "        [756.6283,  23.4187, 138.3844,  32.0791, 375.6922,  15.7496,  47.3548]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[3.0871, 3.0203, 2.5698, 2.9119, 2.9146, 2.7435, 2.7818],\n",
            "        [2.7232, 2.7929, 2.6788, 2.9471, 2.8675, 2.8893, 2.9055]])\n",
            "T63:  tensor([[3508.1270, 1940.9408,   20.8679,  714.8718,  733.5584,  135.8439,\n",
            "          200.6525],\n",
            "        [ 160.2843,  325.0755,  100.8743, 1426.9023,  674.6258,  830.6772,\n",
            "          968.0298]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2625, 2.1638, 2.1751, 2.1651, 2.1984, 2.1995, 2.2459],\n",
            "        [1.8754, 1.7750, 1.9157, 1.8497, 1.8278, 1.8528, 1.7895]])\n",
            "T63:  tensor([[170.6312,  47.6506,  55.3640,  48.4770,  75.1952,  76.2928, 138.4445],\n",
            "        [131.8781,  26.2274, 243.5239,  88.2438,  62.3193,  92.6639,  33.4182]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.8172, 1.9476, 1.9578, 1.8173, 1.8859, 1.8540, 1.9518],\n",
            "        [2.2154, 2.3433, 2.3609, 2.1918, 2.5932, 2.4158, 2.5065]])\n",
            "T63:  tensor([[  51.1758,  380.6661,  441.9560,   51.2202,  150.8161,   91.9571,\n",
            "          404.5958],\n",
            "        [  62.6408,  309.1326,  381.3878,   46.0059, 5052.2109,  724.8188,\n",
            "         2001.7891]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.8654, 1.8783, 1.9479, 1.7374, 1.9159, 1.8919, 1.8563],\n",
            "        [2.1860, 2.3305, 2.2007, 2.2451, 2.2928, 2.2947, 2.2876]])\n",
            "T63:  tensor([[112.3242, 137.3013, 390.4512,  13.7967, 243.0212, 169.1495,  97.4963],\n",
            "        [ 59.2257, 365.2367,  71.8333, 127.2234, 230.6352, 236.1443, 216.4559]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.7541, 2.5203, 2.7708, 2.4747, 2.5511, 3.0017, 2.6890],\n",
            "        [1.7847, 1.7743, 1.7488, 1.7361, 1.8274, 1.7265, 1.8139]])\n",
            "T63:  tensor([[ 175.4282,   13.7746,  207.8341,    8.0580,   19.6562, 1890.8890,\n",
            "           89.2736],\n",
            "        [  32.3186,   27.2026,   17.6646,   14.2170,   64.8519,   12.0303,\n",
            "           52.1658]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4719, 2.2347, 2.2960, 2.4295, 2.1543, 2.4137, 2.2368],\n",
            "        [1.8286, 1.7264, 1.7578, 1.8096, 1.7389, 1.8275, 1.7884]])\n",
            "T63:  tensor([[1452.0693,   85.5402,  184.9179,  900.8944,   29.6799,  751.7809,\n",
            "           87.8625],\n",
            "        [  62.0411,   11.2592,   19.2990,   45.6687,   13.9624,   60.9137,\n",
            "           32.2539]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[3.1542, 2.8206, 2.7728, 2.4679, 2.8089, 2.9700, 3.1265],\n",
            "        [2.6252, 2.6607, 3.0581, 2.8269, 2.4919, 3.1142, 2.7865]])\n",
            "T63:  tensor([[11670.0771,   570.3706,   355.3675,    12.8500,   508.3794,  2334.1895,\n",
            "          9236.8037],\n",
            "        [   72.4955,   106.0789,  4876.2710,   576.5421,    16.1544,  7919.9790,\n",
            "           387.2636]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.8702, 2.5601, 2.9731, 2.5164, 2.7879, 2.8935, 2.8129],\n",
            "        [1.8344, 1.7330, 1.7902, 1.8332, 1.8311, 1.8349, 1.7595]])\n",
            "T63:  tensor([[ 826.1702,   33.2224, 2156.4180,   20.1785,  370.0709, 1030.9990,\n",
            "          473.7838],\n",
            "        [  69.1287,   12.8218,   33.7746,   67.8837,   65.5901,   69.7378,\n",
            "           20.1951]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.7755, 1.8224, 1.8263, 1.7306, 1.7409, 1.8156, 1.7677],\n",
            "        [2.8212, 2.6710, 2.7680, 2.7039, 2.7804, 2.6440, 2.9253]])\n",
            "T63:  tensor([[  26.2632,   56.6901,   60.3077,   12.2179,   14.6090,   50.7577,\n",
            "           23.0543],\n",
            "        [ 638.8508,  139.1910,  377.5517,  196.4341,  427.3885,  104.4707,\n",
            "         1720.0558]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.2389, 2.3107, 2.3066, 2.2976, 2.2278, 2.3130, 2.4143],\n",
            "        [2.9267, 2.9405, 2.8402, 2.6583, 2.7105, 2.8662, 2.9530]])\n",
            "T63:  tensor([[  77.3956,  190.3477,  181.1738,  162.0593,   67.1171,  195.9384,\n",
            "          652.4175],\n",
            "        [1480.5841, 1682.1528,  651.3609,  102.6161,  177.4578,  836.4579,\n",
            "         1888.5833]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.8328, 3.0289, 2.9789, 3.0262, 2.5050, 2.8550, 2.8234],\n",
            "        [1.8567, 1.8088, 1.8207, 1.8053, 1.8784, 1.8478, 1.8279]])\n",
            "T63:  tensor([[ 516.7590, 3208.1167, 2046.3120, 3129.6433,   15.7424,  640.6432,\n",
            "          471.2354],\n",
            "        [  96.5524,   44.9490,   54.4289,   42.4021,  135.3398,   83.9688,\n",
            "           61.1753]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.4598, 1.9698, 2.5143, 2.5715, 2.5714, 2.3868, 2.4342],\n",
            "        [2.2738, 2.3484, 2.3002, 2.2138, 2.2317, 2.2345, 2.2129]])\n",
            "T63:  tensor([[1.2509e+03, 2.0375e+00, 2.2832e+03, 4.2169e+03, 4.2119e+03, 5.4261e+02,\n",
            "         9.3690e+02],\n",
            "        [1.7064e+02, 4.2361e+02, 2.3641e+02, 7.9547e+01, 1.0023e+02, 1.0381e+02,\n",
            "         7.8678e+01]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.4324, 2.3709, 2.4600, 2.2875, 2.1532, 2.4062, 2.3927],\n",
            "        [2.3205, 2.4333, 2.2879, 2.1740, 2.3227, 2.2707, 2.3861]])\n",
            "T63:  tensor([[ 785.9729,  384.7219, 1074.2817,  139.9081,   24.4411,  581.3905,\n",
            "          496.8837],\n",
            "        [ 235.3243,  887.2946,  157.6000,   36.3841,  241.7678,  127.1584,\n",
            "          514.6033]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.9170, 2.7135, 2.6850, 2.9284, 2.6560, 2.7872, 3.0065],\n",
            "        [1.7840, 1.8211, 1.7031, 1.7427, 1.7541, 1.7610, 1.7877]])\n",
            "T63:  tensor([[1212.9114,  163.6564,  121.4724, 1348.0920,   89.3504,  345.9627,\n",
            "         2753.1755],\n",
            "        [  29.7030,   54.5690,    7.4124,   14.8093,   17.9967,   20.2289,\n",
            "           31.5874]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2911, 2.2371, 2.2895, 2.1919, 2.3565, 2.2294, 2.3147],\n",
            "        [2.8067, 3.0170, 2.8602, 2.9179, 2.7461, 2.7642, 2.9208]])\n",
            "T63:  tensor([[ 187.3605,   95.0571,  183.6206,   52.8871,  413.7938,   86.1274,\n",
            "          250.2832],\n",
            "        [ 419.7748, 3025.7446,  707.4948, 1223.0055,  228.7123,  274.8005,\n",
            "         1255.9075]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.8095, 2.7963, 2.4829, 2.8006, 2.7330, 2.6576, 2.7683],\n",
            "        [1.7091, 1.7722, 1.7614, 1.6963, 1.8159, 1.7966, 1.7379]])\n",
            "T63:  tensor([[733.4415, 644.6291,  22.5782, 672.5534, 342.4926, 156.8312, 488.4577],\n",
            "        [  8.5225,  25.2555,  21.0688,   6.7904,  51.8586,  37.8543,  14.1069]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[3.0094, 2.8648, 3.0411, 2.6525, 2.5563, 3.0222, 2.4762],\n",
            "        [2.6241, 2.4468, 2.9491, 2.8877, 2.7395, 2.6559, 2.8255]])\n",
            "T63:  tensor([[4968.3599, 1315.9714, 6576.0874,  156.6924,   54.9954, 5566.1685,\n",
            "           22.0128],\n",
            "        [  59.5486,    7.8042, 1539.9297,  866.6619,  201.2390,   83.9484,\n",
            "          475.9604]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2334, 2.2957, 2.2162, 2.3969, 2.2417, 2.3025, 2.5989],\n",
            "        [1.7919, 1.7899, 1.7643, 1.8011, 1.8064, 1.7622, 1.7965]])\n",
            "T63:  tensor([[  76.3246,  167.3271,   61.0835,  563.2961,   84.8942,  181.9823,\n",
            "         5194.9067],\n",
            "        [  34.3297,   33.2061,   21.6507,   39.9143,   43.5548,   20.9088,\n",
            "           37.0083]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.6699, 2.7382, 2.6965, 2.7619, 2.6546, 2.8346, 2.7456],\n",
            "        [2.3490, 2.3984, 2.2841, 2.2340, 2.2946, 2.1500, 2.3166]])\n",
            "T63:  tensor([[173.2974, 350.8721, 228.7432, 445.7534, 147.4368, 912.2099, 378.3543],\n",
            "        [389.9277, 696.8828, 176.9634,  94.1418, 201.5560,  31.1708, 263.7835]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7965, 2.8660, 2.6715, 2.8578, 2.7612, 2.7107, 2.7344],\n",
            "        [1.7240, 1.7879, 1.7555, 1.7271, 1.7855, 1.7449, 1.8607]])\n",
            "T63:  tensor([[491.8069, 965.5406, 137.3967, 892.5562, 345.7246, 206.7226, 263.6302],\n",
            "        [ 11.3030,  33.4540,  19.4566,  11.9445,  32.1453,  16.2372, 107.8686]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8483, 1.7859, 1.7859, 1.8427, 1.9398, 1.7618, 1.8442],\n",
            "        [1.8869, 1.8886, 1.7849, 1.8670, 1.9496, 1.9324, 1.8602]])\n",
            "T63:  tensor([[ 85.5045,  31.1588,  31.1570,  78.1665, 344.8787,  20.8135,  80.0859],\n",
            "        [156.1715, 160.1363,  30.7304, 114.8636, 399.0758, 310.0214, 103.2388]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.8478, 1.7323, 1.7803, 1.7756, 1.7425, 1.7651, 1.7939],\n",
            "        [2.9959, 2.4673, 2.6444, 2.8281, 2.6533, 2.9472, 2.8589]])\n",
            "T63:  tensor([[  82.7316,   12.2463,   27.7012,   25.6163,   14.6071,   21.4591,\n",
            "           34.6850],\n",
            "        [3574.3760,   15.8110,  115.4253,  747.8870,  126.8755, 2299.3069,\n",
            "         1006.1169]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.8276, 1.7618, 1.7759, 1.7849, 1.8558, 1.7733, 1.7454],\n",
            "        [2.7177, 2.6665, 3.0110, 2.6995, 2.7037, 2.7977, 2.6719]])\n",
            "T63:  tensor([[  63.7522,   21.5837,   27.3370,   31.7563,   99.6337,   26.1977,\n",
            "           16.3362],\n",
            "        [ 190.3485,  111.4290, 3174.1416,  157.5343,  164.6576,  427.1078,\n",
            "          118.0135]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.2890, 2.2163, 2.2555, 2.3288, 2.2708, 2.2859, 2.3332],\n",
            "        [1.8551, 1.8450, 1.7882, 1.8233, 1.8881, 1.9298, 1.7654]])\n",
            "T63:  tensor([[164.8841,  65.5982, 108.3564, 268.4991, 131.4109, 158.6696, 283.2010],\n",
            "        [ 94.5163,  80.5567,  32.1456,  57.0091, 157.4605, 295.2704,  21.9743]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2108, 2.4100, 2.3479, 2.3280, 2.3021, 2.4208, 2.4433],\n",
            "        [2.2344, 2.3344, 2.2653, 2.3206, 2.4626, 2.2880, 2.3054]])\n",
            "T63:  tensor([[  56.9738,  655.4481,  315.8778,  248.6154,  181.2025,  741.9183,\n",
            "          959.7504],\n",
            "        [  76.0094,  263.9543,  112.5983,  223.1559, 1172.7659,  149.5786,\n",
            "          185.4081]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.2416, 2.2752, 2.3116, 2.2707, 2.1774, 2.1634, 2.2640],\n",
            "        [1.7695, 1.7530, 1.8302, 1.8213, 1.8113, 1.7915, 1.7859]])\n",
            "T63:  tensor([[108.3051, 165.3487, 259.0959, 156.4180,  46.9948,  38.9994, 143.7267],\n",
            "        [ 23.6703,  17.9113,  64.0580,  55.5330,  47.1983,  34.1115,  31.1037]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.1099, 2.2582, 2.2512, 2.3507, 2.2361, 2.2674, 2.3871],\n",
            "        [2.3194, 2.2048, 2.2693, 2.2938, 2.2779, 2.2125, 2.2651]])\n",
            "T63:  tensor([[ 16.5433, 118.1834, 108.0691, 367.7383,  89.1421, 132.6241, 564.5908],\n",
            "        [252.7374,  59.6205, 136.1233, 184.6870, 151.6444,  65.9428, 129.2436]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.2614, 2.1894, 2.5292, 2.3268, 2.3742, 2.2847, 2.0998],\n",
            "        [1.7209, 1.7874, 1.7518, 1.7882, 1.8402, 1.8275, 1.7955]])\n",
            "T63:  tensor([[ 121.0726,   47.7926, 2741.4226,  271.6159,  478.3758,  162.0962,\n",
            "           14.1174],\n",
            "        [   9.7931,   30.4087,   16.7103,   30.7872,   71.6085,   58.4264,\n",
            "           34.7724]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.6829, 2.6790, 2.9086, 2.6357, 2.8362, 2.8344, 2.7477],\n",
            "        [1.8053, 1.7432, 1.8121, 1.7415, 1.7394, 1.7522, 1.8364]])\n",
            "T63:  tensor([[ 167.9601,  161.3655, 1564.5015,  101.8544,  785.6057,  772.3742,\n",
            "          327.3477],\n",
            "        [  42.2020,   14.9116,   47.1060,   14.4937,   13.9733,   17.4013,\n",
            "           69.6910]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2228, 2.3246, 2.3515, 2.2196, 2.1358, 2.3057, 2.1348],\n",
            "        [1.7003, 1.7148, 1.7381, 1.7841, 1.8556, 1.7262, 1.7359]])\n",
            "T63:  tensor([[ 64.4188, 231.0374, 319.3690,  61.7975,  20.2045, 183.3279,  19.9430],\n",
            "        [  6.9886,   9.0363,  13.5663,  29.5564,  93.6358,  11.0338,  13.0464]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7054, 2.5510, 3.1615, 2.5106, 2.6978, 2.7458, 2.6003],\n",
            "        [2.2708, 2.3549, 2.3706, 2.3405, 2.5798, 2.3423, 2.4374]])\n",
            "T63:  tensor([[  189.3192,    35.6825, 13067.9805,    22.4984,   174.8152,   286.3359,\n",
            "            61.7135],\n",
            "        [  150.7831,   420.0346,   505.7784,   353.8665,  5162.7607,   361.6134,\n",
            "          1094.6840]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.8421, 2.8090, 2.7639, 2.8416, 2.7147, 2.7090, 2.7189],\n",
            "        [2.8537, 2.7358, 2.8455, 2.4301, 2.4792, 2.7082, 2.7253]])\n",
            "T63:  tensor([[ 518.0076,  374.2712,  238.2544,  515.5632,  143.9163,  135.6423,\n",
            "          150.2965],\n",
            "        [1703.6123,  537.1268, 1575.1326,   18.7747,   33.4690,  405.6550,\n",
            "          483.0255]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.7902, 1.8187, 1.7673, 1.8140, 1.8631, 1.7701, 1.7972],\n",
            "        [1.7947, 1.7620, 1.7374, 1.7704, 1.8034, 1.7475, 1.7842]])\n",
            "T63:  tensor([[ 32.9889,  52.5760,  22.5415,  48.7032, 106.4573,  23.6286,  37.0121],\n",
            "        [ 35.9607,  20.8245,  13.6923,  23.9972,  41.4426,  16.2748,  30.2445]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.1801, 2.3423, 2.1838, 2.3973, 2.4422, 2.3712, 2.2651],\n",
            "        [2.9033, 2.8210, 2.7245, 2.7011, 2.6870, 2.4777, 2.8423]])\n",
            "T63:  tensor([[  43.5061,  337.1282,   45.7138,  645.2547, 1080.0347,  475.3524,\n",
            "          130.5345],\n",
            "        [1147.5200,  521.3293,  197.8722,  155.3754,  134.1400,   13.0695,\n",
            "          641.2204]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.9039, 2.8734, 2.9443, 2.6136, 2.8003, 2.9502, 2.7982],\n",
            "        [2.7468, 2.6540, 2.9199, 2.8134, 2.7159, 2.5057, 2.7219]])\n",
            "T63:  tensor([[1806.3364, 1356.3188, 2622.6765,   97.9721,  670.1094, 2768.6724,\n",
            "          656.4088],\n",
            "        [ 357.5563,  136.9398, 1909.6318,  693.0339,  261.1396,   26.5649,\n",
            "          277.7238]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.3079, 2.3645, 2.5440, 2.1568, 2.4602, 2.2081, 2.3372],\n",
            "        [1.8270, 1.8258, 1.8331, 1.7495, 1.9525, 1.9057, 1.7600]])\n",
            "T63:  tensor([[ 229.9323,  454.6603, 3417.3337,   33.0565, 1367.5387,   65.2564,\n",
            "          328.3062],\n",
            "        [  60.4401,   59.3042,   66.6869,   16.7798,  412.6452,  205.8830,\n",
            "           20.0406]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7594, 1.7812, 1.7913, 1.6834, 1.8444, 1.8625, 1.7668],\n",
            "        [1.9224, 1.8216, 1.8891, 1.8907, 1.8899, 1.8575, 1.8219]])\n",
            "T63:  tensor([[ 21.1557,  30.4922,  36.0163,   5.6044,  84.9079, 112.9737,  23.9827],\n",
            "        [281.4906,  59.0708, 170.2329, 174.4710, 172.1594, 104.3821,  59.3275]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.7627, 1.7729, 1.8311, 1.7683, 1.8067, 1.7663, 1.7921],\n",
            "        [1.7996, 1.7993, 1.6893, 1.8339, 1.7512, 1.7419, 1.7445]])\n",
            "T63:  tensor([[21.9598, 26.0757, 67.5425, 24.1473, 45.5881, 23.3428, 35.8916],\n",
            "        [38.7739, 38.5554,  5.8344, 67.5161, 17.2502, 14.7268, 15.3850]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.8427, 1.7845, 1.7913, 1.7839, 1.8069, 1.7557, 1.8624],\n",
            "        [2.9955, 2.9605, 2.8719, 2.9487, 2.8407, 2.8841, 2.7154]])\n",
            "T63:  tensor([[  75.2900,   29.2853,   32.7655,   28.9986,   42.3532,   18.0523,\n",
            "          102.6922],\n",
            "        [2009.3718, 1459.0398,  634.8962, 1309.4071,  469.7040,  713.5585,\n",
            "          133.1497]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3043, 2.1447, 2.3382, 2.2552, 2.2226, 2.1842, 2.2541],\n",
            "        [2.4701, 2.4259, 2.4042, 2.4466, 2.4346, 2.3759, 2.3286]])\n",
            "T63:  tensor([[ 202.1310,   25.7165,  305.2194,  109.6399,   72.2562,   43.7127,\n",
            "          108.1132],\n",
            "        [1100.1053,  667.3795,  519.2838,  844.7586,  736.5673,  372.9254,\n",
            "          211.6742]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.4919, 2.5484, 2.4863, 2.2972, 2.4288, 2.3743, 2.0892],\n",
            "        [2.8343, 2.7881, 3.0282, 2.8141, 2.9149, 2.6828, 2.8187]])\n",
            "T63:  tensor([[1788.8597, 3299.4653, 1681.4111,  185.2673,  882.1921,  469.0791,\n",
            "           11.9066],\n",
            "        [ 633.7510,  402.5278, 3832.3743,  520.5515, 1366.4468,  137.3329,\n",
            "          544.2001]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.4515, 2.2852, 2.3731, 2.0917, 2.2956, 2.3529, 2.3144],\n",
            "        [1.8276, 1.8047, 1.8031, 1.8296, 1.7020, 1.7683, 1.8046]])\n",
            "T63:  tensor([[1142.1265,  160.0194,  463.1768,   12.3447,  181.8029,  364.3452,\n",
            "          229.1214],\n",
            "        [  60.4392,   41.7014,   40.6571,   62.4190,    7.2590,   22.8165,\n",
            "           41.6478]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.2506, 2.2250, 2.2178, 2.0907, 2.2013, 2.3752, 2.2020],\n",
            "        [2.6919, 2.6136, 2.5498, 2.8104, 2.5960, 2.8484, 2.7629]])\n",
            "T63:  tensor([[ 99.7088,  71.8349,  65.4471,  11.7068,  52.7660, 457.2769,  53.2610],\n",
            "        [203.2410,  88.5174,  43.7573, 671.2563,  73.0737, 969.1727, 419.3604]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7682, 1.8993, 1.9184, 2.0174, 1.8549, 1.8519, 1.8722],\n",
            "        [2.5930, 2.6811, 2.9460, 2.5990, 2.4072, 2.6895, 2.6563]])\n",
            "T63:  tensor([[  23.1663,  187.9955,  250.6612, 1049.3655,   94.7657,   90.3057,\n",
            "          124.1538],\n",
            "        [  65.8667,  169.4224, 2268.2466,   70.3479,    7.6306,  184.8513,\n",
            "          130.3992]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.3094, 2.3633, 1.9887, 2.3804, 2.3994, 2.2074, 2.2413],\n",
            "        [1.8280, 1.8558, 1.7727, 1.8341, 1.8845, 1.8811, 1.8775]])\n",
            "T63:  tensor([[204.4737, 391.4399,   2.5754, 478.8979, 598.4229,  56.3019,  87.2731],\n",
            "        [ 58.7806,  91.3657,  23.7333,  64.7961, 142.8032, 135.4096, 128.1825]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7521, 1.8031, 1.8415, 1.6903, 1.7857, 1.8045, 1.8185],\n",
            "        [2.1576, 2.2030, 2.3482, 2.2222, 2.0908, 2.2358, 2.1899]])\n",
            "T63:  tensor([[ 18.3553,  42.9425,  79.7238,   6.2337,  32.2832,  43.9604,  55.1632],\n",
            "        [ 36.9948,  67.4712, 412.7701,  86.5272,  14.7527, 103.0283,  56.8578]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3258, 2.3551, 2.3813, 2.2309, 2.3876, 2.4431, 2.3875],\n",
            "        [2.0357, 2.3018, 2.3237, 2.2604, 2.2077, 2.3108, 2.2288]])\n",
            "T63:  tensor([[ 350.2314,  497.0724,  676.6658,  107.6466,  727.8102, 1373.6029,\n",
            "          727.1473],\n",
            "        [   5.3600,  191.5107,  250.1180,  114.3243,   58.1163,  213.9114,\n",
            "           76.3697]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.4269, 2.2436, 2.3628, 2.4730, 2.3724, 2.4391, 2.3269],\n",
            "        [1.8461, 1.9590, 1.7681, 1.8580, 1.8054, 1.8396, 1.8189]])\n",
            "T63:  tensor([[ 785.8025,   85.8002,  372.4610, 1322.4438,  417.2260,  902.4960,\n",
            "          241.9152],\n",
            "        [  78.4130,  434.8465,   21.9802,   94.6989,   40.7696,   70.7521,\n",
            "           50.7787]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.6715, 2.7860, 2.6818, 2.8209, 2.6615, 2.8333, 2.7983],\n",
            "        [1.7494, 1.8682, 1.9183, 1.8007, 1.7497, 1.7626, 1.9263]])\n",
            "T63:  tensor([[153.7413, 495.0681, 171.4028, 697.2502, 138.4422, 786.6597, 559.0090],\n",
            "        [ 16.5021, 114.2655, 245.2495,  38.9028,  16.5751,  20.6559, 276.7276]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.9381, 1.9096, 1.7170, 1.9524, 1.8386, 1.7513, 1.8837],\n",
            "        [1.7011, 1.7717, 1.8244, 1.7932, 1.7756, 1.8340, 1.7007]])\n",
            "T63:  tensor([[323.5011, 211.5355,   9.2314, 399.2630,  70.4015,  16.7010, 142.5106],\n",
            "        [  6.9697,  23.6301,  56.1319,  33.7695,  25.2248,  65.4087,   6.9240]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.7928, 1.7832, 1.7570, 1.7820, 1.7735, 1.8398, 1.7813],\n",
            "        [2.1918, 2.3029, 2.2942, 2.4500, 2.2112, 2.3778, 2.2871]])\n",
            "T63:  tensor([[  33.6282,   28.6826,   18.4786,   28.1437,   24.4187,   71.9634,\n",
            "           27.8059],\n",
            "        [  48.1889,  198.3748,  178.2145, 1120.9929,   62.1675,  488.0707,\n",
            "          163.2149]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.4941, 2.5178, 2.4614, 2.5934, 2.6259, 2.8802, 2.5364],\n",
            "        [1.8420, 1.8011, 1.8498, 1.8137, 1.8184, 1.8776, 1.8730]])\n",
            "T63:  tensor([[  39.6676,   52.0249,   27.1042,  120.8375,  171.5597, 2196.4624,\n",
            "           64.2243],\n",
            "        [  77.5659,   40.1639,   87.8268,   49.2836,   53.2215,  135.4509,\n",
            "          126.0103]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3260, 2.3471, 2.1733, 2.2915, 2.3026, 2.1292, 2.2530],\n",
            "        [1.7913, 1.8159, 1.7018, 1.7882, 1.7382, 1.7825, 1.7164]])\n",
            "T63:  tensor([[244.3018, 315.0636,  35.0185, 160.0667, 183.5707,  19.2494,  98.8356],\n",
            "        [ 33.9353,  50.7862,   7.3325,  32.2485,  13.8774,  29.3752,   9.4963]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.3624, 2.1913, 2.2986, 2.3711, 2.2869, 2.1628, 2.3905],\n",
            "        [2.7563, 2.7847, 2.8110, 2.7807, 2.7264, 2.7169, 2.7617]])\n",
            "T63:  tensor([[415.9349,  49.0341, 192.3274, 460.8709, 166.5336,  33.5241, 579.0489],\n",
            "        [329.0333, 437.5357, 566.7902, 420.4602, 242.9661, 220.4576, 347.4384]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7867, 1.7715, 1.8022, 1.8097, 1.7787, 1.6886, 1.7463],\n",
            "        [2.1861, 2.4246, 2.2958, 2.4835, 2.3109, 2.3166, 2.4341]])\n",
            "T63:  tensor([[  33.0243,   25.6453,   42.6215,   48.1578,   28.9278,    6.0930,\n",
            "           16.7580],\n",
            "        [  42.4286,  798.0775,  172.5215, 1548.3503,  207.8870,  222.7980,\n",
            "          889.1595]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3300, 2.2976, 2.2978, 2.5429, 2.4531, 2.3860, 2.5313],\n",
            "        [1.8662, 1.8722, 1.8828, 1.7122, 1.7539, 1.7951, 1.7817]])\n",
            "T63:  tensor([[ 255.6632,  172.0052,  172.4695, 2884.4436, 1074.8943,  497.3704,\n",
            "         2545.4111],\n",
            "        [ 117.8001,  129.2456,  152.1026,    9.2211,   18.9674,   37.7899,\n",
            "           30.2899]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7093, 2.4989, 2.8815, 2.9095, 2.7427, 2.7671, 2.6709],\n",
            "        [2.5580, 2.4600, 2.6482, 2.6212, 2.5633, 2.7767, 2.4976]])\n",
            "T63:  tensor([[ 237.7987,   23.7615, 1305.3552, 1699.9391,  334.9439,  428.3512,\n",
            "          159.2515],\n",
            "        [ 216.9656,   72.3496,  566.0180,  426.7669,  229.7627, 2057.0415,\n",
            "          111.0037]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.8738, 2.8507, 2.7033, 2.8596, 2.9097, 2.7298, 2.4977],\n",
            "        [1.7534, 1.7852, 1.7667, 1.7700, 1.8252, 1.7724, 1.7877]])\n",
            "T63:  tensor([[2068.1458, 1661.7515,  386.3475, 1808.3783, 2894.6069,  506.2981,\n",
            "           41.3541],\n",
            "        [  17.4930,   29.8216,   21.8901,   23.1401,   57.3649,   24.0975,\n",
            "           31.1036]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8409, 1.8995, 1.8957, 1.8747, 1.7408, 1.8171, 1.8704],\n",
            "        [1.9133, 1.8907, 1.7169, 1.8241, 1.8095, 1.7202, 1.9416]])\n",
            "T63:  tensor([[ 76.6076, 190.3903, 179.5011, 130.1608,  14.6668,  52.3447, 121.6898],\n",
            "        [221.3475, 157.1049,   9.1227,  55.2859,  43.6250,   9.6559, 337.2480]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.9060, 1.8760, 1.8546, 1.8499, 1.8847, 1.9100, 1.8511],\n",
            "        [2.4102, 2.1371, 2.2013, 2.3087, 2.3426, 2.1226, 2.3846]])\n",
            "T63:  tensor([[204.2898, 129.1110,  92.4979,  85.9343, 147.7501, 216.8061,  87.5461],\n",
            "        [720.9613,  23.4333,  55.3774, 215.9202, 325.3571,  19.1995, 535.4424]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[3.0755, 2.7718, 2.8586, 2.4870, 2.8418, 2.7806, 2.8717],\n",
            "        [1.8336, 1.8543, 1.7557, 1.7822, 1.7621, 1.7372, 1.7630]])\n",
            "T63:  tensor([[6282.6289,  372.0084,  870.4903,   17.0935,  740.4900,  406.3954,\n",
            "          986.8334],\n",
            "        [  70.8239,   98.2678,   19.6672,   30.6566,   21.9005,   14.3251,\n",
            "           22.2483]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.2019, 2.3104, 2.2509, 2.3003, 2.2377, 2.2903, 2.2087],\n",
            "        [2.5181, 2.4497, 2.4149, 2.4174, 2.4493, 2.4565, 2.3962]])\n",
            "T63:  tensor([[  76.7442,  300.8307,  143.8815,  265.9236,  121.7185,  235.1611,\n",
            "           83.8317],\n",
            "        [2146.8052, 1006.5521,  677.0180,  696.5709, 1001.6228, 1086.3904,\n",
            "          545.0385]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.6673, 2.7089, 2.9821, 2.7246, 2.7675, 2.6655, 2.9729],\n",
            "        [2.7492, 2.9606, 3.0679, 2.7122, 3.1104, 2.9135, 3.0812]])\n",
            "T63:  tensor([[ 106.6260,  165.0533, 2329.4519,  194.0044,  300.1002,  104.6115,\n",
            "         2142.1646],\n",
            "        [ 189.0870, 1464.8224, 3838.9485,  129.0795, 5552.2056,  945.2401,\n",
            "         4313.3218]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.6251, 2.8008, 2.7704, 2.7472, 2.9431, 2.8576, 2.5938],\n",
            "        [1.9205, 1.7707, 1.8316, 1.9138, 1.9160, 1.8419, 1.8534]])\n",
            "T63:  tensor([[  69.4548,  429.2503,  317.1262,  250.5672, 1674.2737,  747.7877,\n",
            "           49.2960],\n",
            "        [ 260.7369,   24.3674,   65.9651,  235.7396,  243.9077,   77.7870,\n",
            "           93.2233]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3675, 2.2728, 2.3063, 2.4335, 2.3592, 2.2125, 2.1924],\n",
            "        [2.6907, 2.9395, 2.6889, 3.0752, 2.7919, 2.6765, 2.6256]])\n",
            "T63:  tensor([[ 391.3131,  123.4341,  186.9670,  841.8017,  354.4333,   57.0465,\n",
            "           43.8330],\n",
            "        [ 110.6595, 1290.9351,  108.5238, 4378.3496,  312.1104,   95.1870,\n",
            "           55.0898]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.6089, 2.3271, 2.3448, 2.3267, 2.4487, 2.3032, 2.4432],\n",
            "        [1.8599, 1.8079, 1.7589, 1.9015, 1.7365, 1.7926, 1.7901]])\n",
            "T63:  tensor([[5663.9653,  241.5810,  299.2098,  240.5736, 1002.3730,  180.4379,\n",
            "          942.0129],\n",
            "        [ 110.9226,   48.4065,   21.4801,  210.0955,   14.6388,   37.6868,\n",
            "           36.1406]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.9150, 1.7948, 1.8546, 1.8851, 1.8115, 1.8574, 1.7859],\n",
            "        [2.5102, 2.2149, 2.3489, 2.3134, 2.2392, 2.5230, 2.4062]])\n",
            "T63:  tensor([[ 236.4233,   35.7561,   93.4293,  150.0208,   46.9809,   97.6911,\n",
            "           30.8554],\n",
            "        [2096.0413,   62.5926,  332.7274,  216.5986,   85.6105, 2408.7454,\n",
            "          652.4072]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.2585, 2.4096, 2.2900, 2.2986, 2.1597, 2.2268, 2.2993],\n",
            "        [1.7489, 1.7724, 1.7540, 1.8447, 1.7826, 1.7777, 1.7515]])\n",
            "T63:  tensor([[140.9138, 866.8201, 208.4721, 231.9153,  38.9895,  94.1476, 233.8212],\n",
            "        [ 17.6905,  26.3367,  19.3152,  85.3982,  31.2320,  28.7533,  18.5164]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7775, 2.6538, 2.7000, 2.5233, 2.6253, 2.5475, 2.6551],\n",
            "        [2.4501, 2.3515, 2.4092, 2.4909, 2.4037, 2.5096, 2.4813]])\n",
            "T63:  tensor([[ 778.0857,  220.2559,  356.1105,   52.9346,  162.6677,   69.5084,\n",
            "          223.3463],\n",
            "        [1065.9003,  339.3492,  668.6261, 1679.5159,  627.3333, 2062.8142,\n",
            "         1510.1414]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.7825, 1.7420, 1.7844, 1.7688, 1.7723, 1.7581, 1.7770],\n",
            "        [2.4234, 2.4664, 2.0725, 2.2576, 2.5344, 2.4646, 2.1853]])\n",
            "T63:  tensor([[  29.7220,   14.9949,   30.7020,   23.6500,   25.0975,   19.7421,\n",
            "           27.1509],\n",
            "        [ 807.8593, 1314.5134,    9.1156,  109.9400, 2772.0879, 1288.1853,\n",
            "           43.1186]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.2680, 2.3758, 2.4780, 2.3075, 2.3591, 2.3871, 2.1476],\n",
            "        [2.8758, 2.9879, 2.9320, 2.9260, 2.9541, 2.8528, 3.1915]])\n",
            "T63:  tensor([[  125.4769,   465.4404,  1495.2039,   204.8956,   381.7557,   530.8766,\n",
            "            25.9925],\n",
            "        [  628.6474,  1789.7346,  1069.3235,  1011.7883,  1312.8666,   503.4025,\n",
            "         10494.7207]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.6773, 2.4251, 2.3838, 2.4682, 2.3928, 2.2389, 2.2626],\n",
            "        [2.6291, 2.7893, 2.8766, 2.8119, 2.5639, 2.7453, 2.5766]])\n",
            "T63:  tensor([[12576.2959,   851.1133,   527.6375,  1384.0718,   585.9517,    89.5768,\n",
            "           121.1412],\n",
            "        [  104.5658,   545.0223,  1268.9657,   680.9609,    51.2595,   351.1183,\n",
            "            58.9797]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.2040, 2.3656, 2.3403, 2.2306, 2.2054, 2.2168, 2.3096],\n",
            "        [2.4808, 2.3961, 2.3476, 2.4257, 2.3636, 2.3530, 2.2890]])\n",
            "T63:  tensor([[  61.4305,  458.0169,  338.4523,   86.7082,   62.5639,   72.5944,\n",
            "          233.5237],\n",
            "        [1563.4674,  598.1218,  337.5892,  841.0275,  408.4244,  360.0551,\n",
            "          165.3477]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.8988, 2.7510, 2.8857, 2.8864, 2.6485, 2.8926, 2.8343],\n",
            "        [2.9279, 3.0135, 2.7905, 2.9832, 3.0018, 2.8436, 2.6919]])\n",
            "T63:  tensor([[1167.2970,  275.3491, 1031.4684, 1037.4100,   94.6864, 1100.5657,\n",
            "          629.8264],\n",
            "        [ 937.0814, 2056.4033,  247.4070, 1561.7803, 1850.0842,  418.3620,\n",
            "           89.6861]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.7846, 1.7840, 1.7968, 1.7483, 1.8397, 1.7429, 1.8037],\n",
            "        [2.9949, 2.7065, 2.9847, 2.6368, 2.5850, 2.8515, 2.9459]])\n",
            "T63:  tensor([[  31.4582,   31.1490,   38.4335,   17.0615,   76.8962,   15.5613,\n",
            "           43.0506],\n",
            "        [3432.0037,  211.4507, 3129.7122,  101.1505,   57.4281,  902.1359,\n",
            "         2195.8748]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[1.8321, 1.8021, 1.9388, 2.0016, 1.8347, 1.8671, 1.8399],\n",
            "        [2.2250, 2.2372, 2.2865, 2.2951, 2.2414, 2.3272, 2.3463]])\n",
            "T63:  tensor([[ 65.6455,  40.4640, 338.1299, 835.9993,  68.5318, 114.1596,  74.4291],\n",
            "        [ 73.8094,  86.3087, 160.8796, 178.8481,  91.1471, 264.7206, 333.4324]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.7733, 1.7717, 1.8149, 1.8171, 1.8413, 1.7586, 1.8213],\n",
            "        [2.1793, 2.2494, 2.1602, 2.3215, 2.1964, 2.2781, 2.4306]])\n",
            "T63:  tensor([[ 25.1966,  24.5257,  49.9470,  51.7527,  76.2956,  19.6558,  55.3868],\n",
            "        [ 42.4355, 105.5467,  32.8770, 258.3780,  53.2288, 151.5048, 931.8931]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.8508, 1.9135, 1.9343, 1.8393, 1.8303, 1.8440, 1.8798],\n",
            "        [1.7604, 1.8265, 1.7872, 1.7670, 1.7971, 1.7621, 1.8512]])\n",
            "T63:  tensor([[ 96.7290, 253.5088, 345.5898,  80.7248,  69.9166,  86.8662, 151.8650],\n",
            "        [ 20.5304,  60.9497,  32.1363,  22.9500,  37.8042,  21.1338,  90.2494]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[1.7684, 1.8393, 1.7649, 1.8141, 1.8087, 1.7664, 1.7838],\n",
            "        [2.4070, 2.2643, 2.2582, 2.1839, 2.3368, 2.2058, 2.3775]])\n",
            "T63:  tensor([[ 22.4704,  71.6314,  21.1779,  47.7837,  43.7656,  21.7321,  29.0763],\n",
            "        [654.5482, 116.9776, 108.4420,  41.3792, 285.8834,  55.2653, 463.7471]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3447, 2.2962, 2.2227, 2.2817, 2.2762, 2.2050, 2.2953],\n",
            "        [1.8452, 1.8154, 1.9016, 1.9863, 1.9859, 1.7790, 1.8192]])\n",
            "T63:  tensor([[439.0818, 244.2080,  96.9700, 204.1864, 190.7728,  77.0704, 241.5535],\n",
            "        [ 90.0862,  55.9878, 215.5773, 746.6633, 742.9227,  30.8187,  59.4861]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.7648, 2.8694, 2.6647, 2.7291, 2.6933, 2.6456, 2.7183],\n",
            "        [2.2577, 2.2548, 2.3178, 2.3375, 2.1615, 2.3322, 2.2403]])\n",
            "T63:  tensor([[282.5455, 789.2931, 100.3164, 196.6355, 135.5680,  81.7664, 175.8595],\n",
            "        [120.1591, 115.8180, 253.0623, 321.1310,  34.3418, 301.1081,  96.3198]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8763, 1.8132, 1.8190, 1.8491, 1.7399, 1.8378, 1.8115],\n",
            "        [2.2367, 2.2498, 2.1625, 2.1799, 2.3359, 2.3147, 2.3300]])\n",
            "T63:  tensor([[129.6644,  47.7383,  52.4182,  84.7312,  14.0146,  70.9268,  46.4280],\n",
            "        [ 85.2685, 100.8453,  32.2201,  40.6216, 292.5284, 226.4257, 272.4040]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.6956, 2.9184, 2.9761, 2.7597, 3.2452, 2.9079, 2.7797],\n",
            "        [2.9162, 2.9882, 2.9457, 2.6635, 2.7162, 2.7048, 2.7705]])\n",
            "T63:  tensor([[   97.1199,   889.4783,  1518.6230,   188.4368, 15443.7734,   805.2471,\n",
            "           230.7038],\n",
            "        [ 2782.2932,  5345.2646,  3645.2627,   232.2023,   400.0711,   356.0693,\n",
            "           690.5161]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.4035, 2.2269, 2.3551, 2.2750, 2.4030, 2.1817, 2.3816],\n",
            "        [2.6257, 2.4498, 2.7684, 2.7113, 2.6044, 2.5808, 2.8624]])\n",
            "T63:  tensor([[ 653.2779,   75.6367,  369.8395,  139.2081,  649.5117,   41.8896,\n",
            "          506.3347],\n",
            "        [ 600.3265,   85.6139, 2557.8665, 1451.1577,  479.1576,  371.7014,\n",
            "         6288.0288]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.7992, 3.0159, 2.5236, 2.8123, 2.8526, 2.8547, 2.7893],\n",
            "        [2.2349, 2.2937, 2.2196, 2.3103, 2.2114, 2.2055, 2.1449]])\n",
            "T63:  tensor([[ 347.4730, 2677.6375,   18.2445,  395.8982,  586.6520,  598.7230,\n",
            "          315.1250],\n",
            "        [  92.1673,  193.2093,   75.6693,  236.8612,   68.0335,   63.0176,\n",
            "           28.1284]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.8058, 1.7652, 1.8117, 1.8169, 1.8174, 1.8341, 1.8406],\n",
            "        [1.7753, 1.7833, 1.8198, 1.8006, 1.7964, 1.7882, 1.9114]])\n",
            "T63:  tensor([[ 44.8893,  22.9191,  49.4694,  53.8026,  54.1974,  70.8561,  78.6124],\n",
            "        [ 25.5111,  29.1368,  52.9937,  38.7411,  36.1594,  31.5915, 220.7914]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.5524, 2.4607, 2.3971, 2.4791, 2.2997, 2.2095, 2.2131],\n",
            "        [2.7741, 2.5911, 2.7300, 2.8360, 2.8923, 2.6673, 2.8126]])\n",
            "T63:  tensor([[3212.1050, 1177.7795,  569.7551, 1445.9553,  177.6094,   56.5673,\n",
            "           59.2961],\n",
            "        [ 281.6706,   40.8153,  180.0373,  520.1573,  893.2990,   93.4023,\n",
            "          413.3921]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.4495, 2.2273, 2.2804, 2.5294, 2.5252, 2.4400, 2.3742],\n",
            "        [2.7689, 2.8083, 2.8098, 3.0174, 2.8103, 2.7654, 2.9953]])\n",
            "T63:  tensor([[1258.6433,   87.0092,  170.4158, 3033.3872, 2897.4690, 1130.8845,\n",
            "          529.3042],\n",
            "        [ 232.2382,  344.6800,  349.6777, 2471.7976,  351.6184,  224.1928,\n",
            "         2024.9862]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.6451, 2.7376, 2.6553, 2.7075, 2.7503, 2.6594, 2.6907],\n",
            "        [2.1731, 2.1117, 2.3102, 2.2043, 2.3296, 2.2694, 2.2121]])\n",
            "T63:  tensor([[107.5458, 282.0766, 119.8805, 207.0945, 320.6807, 125.1966, 173.9751],\n",
            "        [ 45.6299,  19.8209, 261.5907,  68.8047, 330.8672, 158.1674,  76.2138]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[3.1203, 2.5303, 2.6079, 2.7623, 2.9822, 3.0895, 3.0677],\n",
            "        [1.8588, 1.7775, 1.8350, 1.8680, 1.6869, 1.8627, 1.8122]])\n",
            "T63:  tensor([[5372.2666,   15.4679,   37.1857,  190.7325, 1584.3634, 4117.0874,\n",
            "         3402.7617],\n",
            "        [ 104.7825,   28.1843,   71.9081,  120.8453,    5.8661,  111.2777,\n",
            "           49.8137]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.8639, 3.0990, 2.8832, 2.9336, 3.1081, 3.0702, 2.7426],\n",
            "        [1.8153, 1.7550, 1.7963, 1.7787, 1.8358, 1.8650, 1.8412]])\n",
            "T63:  tensor([[ 631.6281, 5381.2554,  759.8422, 1222.6226, 5818.9502, 4190.6196,\n",
            "          189.5455],\n",
            "        [  48.7753,   17.9408,   35.8001,   26.7349,   67.8563,  107.5021,\n",
            "           73.9842]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.7211, 2.6881, 3.2954, 2.7537, 2.9612, 3.0317, 2.6003],\n",
            "        [2.2884, 2.4179, 2.2901, 2.1695, 2.1931, 2.2335, 2.2789]])\n",
            "T63:  tensor([[  138.5624,    98.2066, 25117.6562,   193.8432,  1443.3939,  2732.8323,\n",
            "            38.0322],\n",
            "        [  181.1966,   847.8349,   185.0213,    39.2490,    53.6864,    90.7222,\n",
            "           161.1454]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[1.7403, 1.8551, 1.8033, 1.7764, 1.7975, 1.7379, 1.8300],\n",
            "        [1.7330, 1.7600, 1.8223, 1.7092, 1.8704, 1.9114, 1.7911]])\n",
            "T63:  tensor([[ 14.2172,  93.8745,  40.9380,  26.2351,  37.1985,  13.6533,  63.0208],\n",
            "        [ 12.6475,  20.0671,  56.1478,   8.3417, 120.2539, 224.7261,  33.7715]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2211, 2.3912, 2.2264, 2.1892, 2.2654, 2.3135, 2.1309],\n",
            "        [2.8800, 2.6870, 3.0268, 2.6406, 2.7090, 2.8293, 2.6667]])\n",
            "T63:  tensor([[  73.5624,  592.6625,   78.7844,   48.4786,  129.3203,  234.7237,\n",
            "           22.1041],\n",
            "        [ 840.6432,  122.0465, 3245.0195,   74.4701,  153.5550,  515.4480,\n",
            "           98.5079]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.3126, 2.3297, 2.1660, 2.1770, 2.3064, 2.5981, 2.4400],\n",
            "        [2.3061, 2.4165, 2.4411, 2.2902, 2.3719, 2.4097, 2.2485]])\n",
            "T63:  tensor([[ 193.6279,  238.2397,   29.4973,   34.1943,  179.3085, 4853.4092,\n",
            "          869.3672],\n",
            "        [ 226.6898,  838.3837, 1109.5878,  186.4127,  499.2657,  775.9604,\n",
            "          110.6074]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.2107, 2.3564, 2.1890, 2.2654, 2.2303, 2.1999, 2.3296],\n",
            "        [2.6232, 2.6304, 2.7214, 2.8852, 2.6359, 2.7826, 2.7234]])\n",
            "T63:  tensor([[  78.3107,  476.8159,   58.9316,  157.3404,  100.8069,   68.0467,\n",
            "          345.9332],\n",
            "        [  83.5414,   90.2934,  235.2149, 1181.5201,   95.8156,  436.4502,\n",
            "          240.0105]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.3356, 2.5888, 2.3459, 2.2588, 2.1767, 2.2149, 2.3423],\n",
            "        [2.2239, 2.1081, 2.2522, 2.2327, 2.2539, 2.2349, 2.1829]])\n",
            "T63:  tensor([[ 276.6311, 4745.6050,  313.2504,  107.1080,   36.8825,   61.0469,\n",
            "          299.8218],\n",
            "        [  81.2799,   17.2273,  116.6656,   91.0259,  119.3439,   93.7104,\n",
            "           47.5678]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3527, 2.3300, 2.2122, 2.4054, 2.4326, 2.1804, 2.5725],\n",
            "        [1.8020, 1.8087, 1.7474, 1.8207, 1.7923, 1.7700, 1.7421]])\n",
            "T63:  tensor([[ 368.9768,  281.0196,   64.2065,  684.7264,  935.4504,   42.2656,\n",
            "         4328.8916],\n",
            "        [  40.5263,   45.1689,   16.2457,   54.8665,   34.5481,   23.8436,\n",
            "           14.8425]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.9138, 2.7322, 2.6793, 2.8126, 2.9096, 2.7411, 2.8034],\n",
            "        [1.8059, 1.7927, 1.9554, 1.7828, 1.9132, 1.8847, 1.8004]])\n",
            "T63:  tensor([[1483.0715,  251.9960,  145.7522,  563.3600, 1425.9121,  275.9839,\n",
            "          514.5670],\n",
            "        [  42.2884,   34.0631,  424.3177,   28.9118,  226.9832,  147.3485,\n",
            "           38.6890]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.1839, 2.3782, 2.2874, 2.1295, 2.3451, 2.0816, 2.2563],\n",
            "        [2.3084, 2.2933, 2.2944, 2.2338, 2.2035, 2.3455, 2.1648]])\n",
            "T63:  tensor([[ 45.1360, 508.5145, 169.9903,  21.6678, 343.5704,  11.0874, 115.1698],\n",
            "        [189.5570, 157.3804, 159.5799,  74.2831,  50.1040, 297.2524,  29.9837]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.7428, 2.6119, 2.8093, 2.7275, 2.6885, 2.6838, 2.8694],\n",
            "        [2.8980, 2.8225, 2.8668, 2.8166, 2.6840, 2.8940, 2.7980]])\n",
            "T63:  tensor([[ 367.8494,   93.2544,  712.7070,  314.8683,  210.6111,  200.5361,\n",
            "         1273.6935],\n",
            "        [1285.7594,  623.5588,  956.1505,  588.2309,  153.5270, 1238.1917,\n",
            "          490.0960]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.8113, 1.8386, 1.7944, 1.7473, 1.8088, 1.8053, 1.7719],\n",
            "        [2.4928, 2.3866, 2.4018, 2.3883, 2.4880, 2.2944, 2.4643]])\n",
            "T63:  tensor([[  44.5108,   69.0971,   33.7494,   15.3233,   42.7372,   40.3928,\n",
            "           23.2499],\n",
            "        [2181.0332,  656.4351,  783.1085,  669.8236, 2068.7505,  217.6928,\n",
            "         1591.6550]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.8981, 2.7952, 2.9455, 3.0620, 2.6134, 2.6747, 2.6932],\n",
            "        [2.4793, 2.4925, 2.4962, 2.4735, 2.2748, 2.2085, 2.2459]])\n",
            "T63:  tensor([[ 808.0883,  297.2006, 1260.3885, 3607.1426,   44.3445,   85.9650,\n",
            "          104.4921],\n",
            "        [1549.1602, 1791.9789, 1866.6019, 1451.1849,  139.5078,   59.8019,\n",
            "           96.8586]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[3.0148, 2.7707, 2.6569, 2.8459, 2.7386, 2.5944, 2.7363],\n",
            "        [2.7318, 3.0672, 2.7990, 3.0483, 2.8892, 2.5104, 2.9072]])\n",
            "T63:  tensor([[4860.0288,  487.6572,  151.1059, 1019.4927,  352.8260,   76.9348,\n",
            "          344.6211],\n",
            "        [ 209.6479, 4986.6309,  412.8982, 4226.9624,  989.3270,   18.7934,\n",
            "         1171.6777]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.2737, 2.3310, 2.2213, 2.2890, 2.2034, 2.2230, 2.1890],\n",
            "        [2.2332, 2.3037, 2.3090, 2.1591, 2.3124, 2.2453, 2.3058]])\n",
            "T63:  tensor([[138.2792, 279.7841,  71.0456, 167.2744,  56.2254,  72.5686,  46.5308],\n",
            "        [114.8552, 277.2840, 295.6526,  43.5759, 308.2100, 134.0538, 284.4094]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.2468, 2.2338, 2.2649, 2.2825, 2.2470, 2.3693, 2.3589],\n",
            "        [1.6353, 1.6109, 1.6811, 1.6501, 1.6561, 1.6215, 1.5711]])\n",
            "T63:  tensor([[ 99.6728,  84.3270, 125.2887, 156.0393,  99.8560, 446.7290, 395.2826],\n",
            "        [ 18.2416,  11.6430,  41.2210,  23.7997,  26.5242,  14.1687,   5.4824]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[1.7549, 1.8488, 1.9070, 1.8847, 1.9714, 1.8483, 1.8288],\n",
            "        [1.8401, 1.6948, 1.7388, 1.7020, 1.8168, 1.8118, 1.8141]])\n",
            "T63:  tensor([[ 18.6277,  86.6488, 212.4908, 151.4783, 549.7902,  85.9540,  63.0083],\n",
            "        [ 73.6074,   6.3633,  13.7678,   7.2301,  50.6853,  46.6835,  48.5125]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.9866, 2.7923, 2.6941, 2.8348, 2.8454, 2.9337, 2.6667],\n",
            "        [2.0873, 2.5178, 2.3965, 2.2996, 2.3174, 2.1424, 2.2909]])\n",
            "T63:  tensor([[2794.0383,  442.8122,  162.6523,  672.7222,  745.2859, 1721.3395,\n",
            "          121.7959],\n",
            "        [  12.1792, 2485.7317,  637.1841,  200.2831,  248.8386,   26.1927,\n",
            "          179.8540]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.7665, 2.6379, 2.6049, 2.6001, 2.8045, 2.5663, 2.7916],\n",
            "        [2.4709, 2.9360, 2.7997, 2.7819, 2.7840, 3.0540, 2.7607]])\n",
            "T63:  tensor([[ 697.6160,  186.0512,  130.6196,  124.0646, 1013.6368,   85.6146,\n",
            "          893.8021],\n",
            "        [  12.4081, 1600.0154,  434.1330,  363.9183,  371.5475, 4631.3208,\n",
            "          294.0813]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.9318, 3.0792, 2.6999, 2.8001, 2.7869, 2.7613, 2.7829],\n",
            "        [2.6111, 2.8622, 2.6988, 3.1123, 2.7327, 2.6496, 2.5315]])\n",
            "T63:  tensor([[1691.0500, 6350.7432,  172.7757,  478.3533,  419.5952,  324.3330,\n",
            "          403.2863],\n",
            "        [  39.0961,  520.5609,  100.3887, 5085.1948,  142.8828,   59.4723,\n",
            "           15.9179]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.7646, 3.0568, 2.6722, 2.8209, 2.8587, 2.8416, 3.0814],\n",
            "        [2.6004, 2.6938, 2.6694, 3.1136, 2.9275, 2.8504, 2.8200]])\n",
            "T63:  tensor([[ 262.4449, 4102.6987,  100.9798,  460.0009,  664.1872,  562.7693,\n",
            "         5087.0073],\n",
            "        [  50.7560,  138.3129,  106.9821, 7277.3408, 1384.3823,  666.7404,\n",
            "          496.1777]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.7575, 1.8239, 1.7170, 1.8281, 1.7473, 1.7389, 1.7917],\n",
            "        [2.4711, 2.2559, 2.2828, 2.3533, 2.4741, 2.3996, 2.3656]])\n",
            "T63:  tensor([[  19.0401,   56.9506,    9.4546,   60.9425,   16.0020,   13.8363,\n",
            "           33.7085],\n",
            "        [1365.5535,  106.0316,  148.7428,  350.9797, 1412.5117,  605.4913,\n",
            "          406.4346]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.3955, 2.3542, 2.4680, 2.1961, 2.3837, 2.5126, 2.4746],\n",
            "        [2.3167, 2.4775, 2.4906, 2.4167, 2.2890, 2.1751, 2.3929]])\n",
            "T63:  tensor([[ 691.2993,  425.2753, 1575.8436,   59.0908,  601.9166, 2571.4536,\n",
            "         1695.0808],\n",
            "        [ 260.1234, 1682.2510, 1943.9636,  847.7499,  185.1243,   42.9274,\n",
            "          643.7529]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[2.3294, 2.1904, 2.3181, 2.2969, 2.2412, 2.3845, 2.3131],\n",
            "        [2.2518, 2.2837, 2.3898, 2.2251, 2.2553, 2.2281, 2.3242]])\n",
            "T63:  tensor([[262.9335,  45.3531, 229.1323, 176.7627,  87.8244, 506.9402, 215.5899],\n",
            "        [102.3560, 152.9387, 548.9060,  72.7516, 107.0156,  75.6018, 251.5637]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.9291, 1.8530, 1.9174, 1.9256, 1.8460, 1.8746, 1.9167],\n",
            "        [1.8647, 1.7592, 1.7640, 1.7167, 1.8262, 1.7546, 1.7724]])\n",
            "T63:  tensor([[305.0531,  95.4207, 255.9151, 289.6375,  85.4707, 133.5647, 253.4965],\n",
            "        [114.0006,  20.5487,  22.2975,   9.8795,  61.9975,  19.0014,  25.6593]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.6880, 2.8711, 2.9163, 2.7388, 2.7247, 2.8468, 2.6184],\n",
            "        [1.8143, 1.8009, 1.8157, 2.0251, 1.8962, 1.8014, 1.8958]])\n",
            "T63:  tensor([[ 114.7858,  721.3156, 1107.2737,  194.5013,  168.3273,  570.9280,\n",
            "           54.4302],\n",
            "        [  49.4654,   39.7863,   50.5872, 1166.1786,  179.0014,   40.1216,\n",
            "          177.9027]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3105, 2.2559, 2.1980, 2.2745, 2.1414, 2.3701, 2.2460],\n",
            "        [1.8731, 1.8681, 1.7948, 1.7910, 1.8585, 1.9852, 1.8754]])\n",
            "T63:  tensor([[262.6666, 133.4000,  63.4289, 168.3833,  29.7972, 536.2826, 117.7392],\n",
            "        [125.6414, 116.1932,  35.9896,  33.8149, 100.1816, 665.0378, 130.2722]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[2.5175, 2.7651, 2.7144, 2.8178, 3.1108, 2.8295, 3.0428],\n",
            "        [1.7778, 1.8063, 1.7583, 1.7813, 1.7512, 1.8051, 1.7656]])\n",
            "T63:  tensor([[  19.5479,  282.0799,  168.0845,  476.5864, 6978.8198,  534.2543,\n",
            "         3864.6968],\n",
            "        [  27.1245,   43.4201,   19.5393,   28.7535,   17.3406,   42.6159,\n",
            "           22.1130]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.8197, 2.6458, 2.9110, 2.8529, 2.5722, 2.6621, 2.8330],\n",
            "        [1.7708, 1.8093, 1.7842, 1.7861, 1.8472, 1.7293, 1.7805]])\n",
            "T63:  tensor([[ 686.6521,  116.6355, 1639.7577,  946.6986,   52.3481,  138.7027,\n",
            "          781.7133],\n",
            "        [  24.8907,   47.0008,   31.1349,   32.1247,   86.2859,   12.2644,\n",
            "           29.2589]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.7832, 3.0564, 3.0955, 2.7453, 2.4731, 3.0555, 2.9080],\n",
            "        [2.2593, 2.4051, 2.2823, 2.2357, 2.3009, 2.2939, 2.2796]])\n",
            "T63:  tensor([[ 233.2153, 3055.6870, 4300.8403,  158.6801,    7.8021, 3030.8237,\n",
            "          787.4758],\n",
            "        [ 127.1153,  737.5905,  169.5760,   94.1913,  213.5325,  195.8729,\n",
            "          164.0970]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.2775, 2.7260, 2.3942, 2.3597, 2.3354, 2.6029, 2.3362],\n",
            "        [2.8799, 2.6216, 2.7689, 2.7243, 2.6884, 2.7838, 2.6312]])\n",
            "T63:  tensor([[  152.3908, 21127.1641,   621.0123,   414.1216,   309.4652,  6130.6792,\n",
            "           312.6094],\n",
            "        [  784.1204,    56.3894,   263.9261,   167.5761,   115.2400,   306.5721,\n",
            "            62.5683]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.8774, 1.9269, 1.8965, 1.7908, 1.7771, 1.8885, 1.8741],\n",
            "        [2.3046, 2.3460, 2.5087, 2.4075, 2.2348, 2.1886, 2.3543]])\n",
            "T63:  tensor([[ 138.9429,  294.2661,  186.3710,   34.8921,   27.8321,  164.9346,\n",
            "          132.0484],\n",
            "        [ 181.8059,  300.4125, 1934.3501,  619.6862,   75.5984,   41.3842,\n",
            "          331.7845]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.7826, 1.8238, 1.7996, 1.7400, 1.8072, 1.7954, 1.7350],\n",
            "        [1.8983, 1.8614, 1.7936, 1.8909, 1.8308, 1.8519, 1.9509]])\n",
            "T63:  tensor([[ 28.8411,  56.5594,  38.2315,  14.0219,  43.2343,  35.6635,  12.8782],\n",
            "        [179.3485, 101.5230,  34.2100, 160.2799,  62.5563,  87.5310, 392.9659]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.2665, 2.2931, 2.3487, 2.2769, 2.3317, 2.3039, 2.3905],\n",
            "        [2.7152, 2.7597, 2.7359, 2.8648, 2.6692, 2.6425, 2.9390]])\n",
            "T63:  tensor([[ 114.7279,  159.9882,  314.9604,  130.7160,  256.5019,  182.7608,\n",
            "          515.9045],\n",
            "        [ 184.0996,  289.9222,  227.7226,  814.8196,  113.7688,   85.6038,\n",
            "         1638.1016]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.9763, 1.7471, 1.9144, 1.8764, 1.9199, 1.8182, 1.8241],\n",
            "        [2.4211, 2.3803, 2.4360, 2.6117, 2.2885, 2.2289, 2.4195]])\n",
            "T63:  tensor([[ 584.2858,   16.1327,  235.3828,  131.8837,  255.3224,   52.5743,\n",
            "           57.8628],\n",
            "        [ 812.9478,  506.1334,  963.3533, 6455.6411,  167.2966,   78.7842,\n",
            "          797.4894]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[1.8416, 1.7503, 1.7687, 1.8045, 1.8314, 1.7677, 1.8155],\n",
            "        [1.7406, 1.7559, 1.7546, 1.7483, 1.8078, 1.7983, 1.8007]])\n",
            "T63:  tensor([[79.1955, 17.6358, 24.0896, 43.5948, 67.2607, 23.6835, 52.0995],\n",
            "        [14.4423, 18.7604, 18.3481, 16.4905, 44.4811, 38.0797, 39.5990]])\n",
            "label:  tensor([100., 100.])\n",
            "beta:  tensor([[2.5447, 2.6012, 2.6695, 2.8123, 2.6345, 2.6570, 2.7944],\n",
            "        [2.7551, 2.6270, 2.5940, 2.7097, 2.7326, 2.8202, 2.7677]])\n",
            "T63:  tensor([[ 54.2122, 101.2068, 209.7404, 886.7107, 144.7839, 183.9341, 744.1687],\n",
            "        [503.8527, 133.6268,  93.5586, 318.1093, 401.7181, 957.1485, 571.4391]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.8246, 2.8880, 2.8259, 2.7655, 3.0252, 2.9614, 3.0338],\n",
            "        [2.8356, 2.9504, 2.8675, 2.7978, 2.7986, 2.7725, 2.9550]])\n",
            "T63:  tensor([[ 531.4149,  978.6219,  537.8834,  295.5736, 3447.1914, 1939.3519,\n",
            "         3720.8477],\n",
            "        [ 469.7672, 1397.6676,  639.6600,  323.8134,  326.4636,  251.3487,\n",
            "         1457.2091]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[1.7133, 1.8268, 1.8033, 1.8387, 1.7921, 1.7656, 1.7781],\n",
            "        [2.0676, 2.5034, 2.4318, 2.4664, 2.1682, 2.2654, 2.3167]])\n",
            "T63:  tensor([[   9.2265,   62.0724,   42.4774,   75.0992,   35.3292,   22.7097,\n",
            "           27.9951],\n",
            "        [   7.9551, 1862.9487,  837.0339, 1236.5651,   32.2187,  113.9725,\n",
            "          215.1492]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.8184, 2.5488, 2.9018, 2.7979, 2.9163, 2.8683, 3.0864],\n",
            "        [2.2836, 2.3585, 2.1798, 2.2962, 2.2332, 2.1733, 2.2273]])\n",
            "T63:  tensor([[ 596.7472,   35.2213, 1326.5483,  487.6187, 1518.7937,  965.5946,\n",
            "         6978.3159],\n",
            "        [ 187.8099,  465.6701,   49.6491,  219.4117,   99.5199,   45.5174,\n",
            "           92.2449]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.2838, 2.2437, 2.3892, 2.3575, 2.2479, 2.2686, 2.3000],\n",
            "        [2.9722, 2.6908, 2.9501, 3.0976, 2.8515, 2.9550, 3.1917]])\n",
            "T63:  tensor([[  141.0292,    84.9943,   503.1368,   346.0420,    89.6572,   116.6448,\n",
            "           172.4252],\n",
            "        [ 2144.3008,   137.6842,  1751.7494,  6509.4282,   692.2574,  1833.4067,\n",
            "         14397.7393]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7854, 1.7635, 1.8562, 1.8174, 1.7776, 1.7302, 1.6722],\n",
            "        [3.2373, 2.7648, 2.8790, 2.6341, 2.5767, 2.6825, 2.8936]])\n",
            "T63:  tensor([[2.9874e+01, 2.0726e+01, 9.3721e+01, 5.0486e+01, 2.6263e+01, 1.1711e+01,\n",
            "         4.1744e+00],\n",
            "        [1.7705e+04, 2.4620e+02, 7.5558e+02, 6.2730e+01, 3.3347e+01, 1.0526e+02,\n",
            "         8.6843e+02]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.8990, 2.7296, 2.9323, 2.5753, 3.0166, 2.6803, 2.9623],\n",
            "        [1.8109, 1.8002, 1.7609, 1.7535, 1.7962, 1.8355, 1.7986]])\n",
            "T63:  tensor([[ 784.3438,  146.8135, 1072.6318,   27.8732, 2320.1138,   87.6508,\n",
            "         1415.9225],\n",
            "        [  45.5673,   38.2683,   19.8762,   17.5263,   35.7942,   67.7130,\n",
            "           37.2677]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[1.8146, 1.7596, 1.7978, 1.7645, 1.7431, 1.7912, 1.8135],\n",
            "        [2.8421, 2.7828, 2.9536, 2.6572, 2.7371, 2.7795, 3.0714]])\n",
            "T63:  tensor([[  50.7847,   20.4444,   38.6160,   22.1843,   15.4310,   34.6611,\n",
            "           49.8899],\n",
            "        [ 558.7873,  311.4184, 1603.1887,   84.9256,  195.8616,  301.2148,\n",
            "         4606.7305]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.6559, 2.8397, 2.7826, 2.6529, 2.7249, 2.8989, 2.8368],\n",
            "        [1.7759, 1.7421, 1.7855, 1.7187, 1.8130, 1.7837, 1.7611]])\n",
            "T63:  tensor([[  99.8007,  646.6110,  368.6144,   96.6436,  205.5083, 1138.1826,\n",
            "          628.3994],\n",
            "        [  26.4777,   14.9258,   31.0283,    9.9401,   48.7413,   30.1266,\n",
            "           20.6127]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.9128, 2.8529, 2.7473, 2.8937, 2.6220, 2.8203, 2.7029],\n",
            "        [1.8778, 1.7771, 1.7957, 1.7289, 1.7987, 1.7799, 1.8626]])\n",
            "T63:  tensor([[1561.6349,  885.3375,  312.5070, 1306.0032,   84.0816,  645.8322,\n",
            "          198.2745],\n",
            "        [ 145.8762,   29.0634,   39.5028,   12.8098,   41.5521,   30.4364,\n",
            "          115.3529]])\n",
            "label:  tensor([  1., 100.])\n",
            "beta:  tensor([[2.3649, 2.3382, 2.2247, 2.2480, 2.2021, 2.2900, 2.2491],\n",
            "        [2.2572, 2.4432, 2.1819, 2.2894, 2.3215, 2.2385, 2.3868]])\n",
            "T63:  tensor([[ 410.4197,  298.2083,   72.4788,   97.7533,   54.0293,  165.5678,\n",
            "           99.0741],\n",
            "        [ 157.5364, 1441.7313,   59.6295,  235.3014,  348.1956,  124.2584,\n",
            "          755.6256]])\n",
            "label:  tensor([10., 10.])\n",
            "beta:  tensor([[1.7341, 1.7914, 1.7727, 1.7674, 1.8564, 1.8138, 1.8010],\n",
            "        [2.1467, 2.5419, 2.5777, 2.4383, 2.4699, 2.3033, 2.4046]])\n",
            "T63:  tensor([[  13.3650,   35.1336,   25.7416,   23.5668,   99.9957,   50.6868,\n",
            "           41.1821],\n",
            "        [  24.4893, 2869.8916, 4197.0283,  914.2599, 1305.3472,  185.5661,\n",
            "          621.4521]])\n",
            "label:  tensor([100.,  10.])\n",
            "beta:  tensor([[2.9472, 2.8260, 2.9733, 2.8202, 2.8733, 2.7733, 3.0463],\n",
            "        [2.6249, 2.8149, 2.7656, 2.5907, 2.8137, 2.7748, 2.7055]])\n",
            "T63:  tensor([[1511.9375,  477.3654, 1921.0327,  450.9672,  754.2961,  283.0398,\n",
            "         3697.4829],\n",
            "        [  64.8127,  461.6615,  282.5677,   44.5587,  456.3344,  309.9508,\n",
            "          152.6994]])\n",
            "label:  tensor([1., 1.])\n",
            "beta:  tensor([[2.7815, 2.6787, 2.8167, 2.8263, 2.7502, 3.0011, 2.7389],\n",
            "        [2.2785, 2.1597, 2.2919, 2.3307, 2.3744, 2.2016, 2.3099]])\n",
            "T63:  tensor([[ 286.4302,   99.2317,  406.5265,  446.5647,  208.7054, 2311.9927,\n",
            "          185.8695],\n",
            "        [ 160.1099,   34.3599,  188.9505,  303.6465,  511.0779,   59.9316,\n",
            "          235.6340]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.2405, 2.2317, 2.2600, 2.2942, 2.3003, 2.2456, 2.2958],\n",
            "        [2.9112, 3.0597, 2.7636, 2.9441, 2.7305, 2.7052, 2.8789]])\n",
            "T63:  tensor([[  90.1062,   80.4039,  115.3570,  176.8997,  190.7444,   96.0945,\n",
            "          180.4470],\n",
            "        [1281.0684, 4912.5317,  305.2634, 1739.8082,  218.1460,  167.9746,\n",
            "          944.4731]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[2.2352, 2.3086, 2.2871, 2.2951, 2.2708, 2.3033, 2.2496],\n",
            "        [2.8081, 3.0762, 3.0184, 3.0350, 2.7519, 2.6814, 3.0209]])\n",
            "T63:  tensor([[  91.9961,  230.6467,  176.8771,  195.3368,  144.4297,  216.2008,\n",
            "          110.5528],\n",
            "        [ 507.1323, 6025.3657, 3625.4343, 4202.1880,  289.4455,  139.8692,\n",
            "         3707.5713]])\n",
            "label:  tensor([10.,  1.])\n",
            "beta:  tensor([[1.7662, 1.7848, 1.7187, 1.7717, 1.7835, 1.7427, 1.8459],\n",
            "        [2.8914, 2.6137, 2.6061, 3.1028, 2.8559, 2.6162, 3.0357]])\n",
            "T63:  tensor([[  24.2141,   33.0076,   10.7062,   26.5500,   32.3013,   16.2407,\n",
            "           88.6698],\n",
            "        [1175.6421,   70.6451,   65.0765, 7858.9541,  838.9687,   72.5942,\n",
            "         4385.0845]])\n",
            "label:  tensor([100.,   1.])\n",
            "beta:  tensor([[2.2776, 2.2828, 2.2147, 2.3168, 2.2718, 2.3469, 2.4016],\n",
            "        [1.9046, 1.7380, 1.9343, 1.8374, 1.9575, 1.8308, 1.7770]])\n",
            "T63:  tensor([[125.0306, 133.4235,  56.0236, 203.1131, 116.3245, 292.5408, 557.9451],\n",
            "        [201.8096,  13.7004, 314.7542,  71.0769, 442.0771,  63.9995,  26.5591]])\n",
            "label:  tensor([ 10., 100.])\n",
            "beta:  tensor([[3.0052, 2.9085, 2.7566, 2.8412, 2.9618, 2.8043, 2.8843],\n",
            "        [2.3782, 2.3311, 2.2361, 2.1296, 2.3557, 2.2705, 2.3410]])\n",
            "T63:  tensor([[1907.5399,  780.6682,  175.5138,  408.5945, 1283.5706,  283.9840,\n",
            "          620.1491],\n",
            "        [ 442.7904,  252.3645,   77.2955,   18.7346,  339.5344,  119.5441,\n",
            "          284.4629]])\n",
            "label:  tensor([ 1., 10.])\n",
            "beta:  tensor([[2.8690, 2.7597, 2.9954, 2.9286, 2.5916, 2.7862, 3.0537],\n",
            "        [2.2474, 2.3175, 2.2597, 2.4063, 2.2093, 2.3103, 2.1253]])\n",
            "T63:  tensor([[ 534.9375,  181.1657, 1745.2021,  943.3266,   30.2076,  236.9999,\n",
            "         2943.4353],\n",
            "        [ 117.9007,  281.6234,  137.7548,  805.0701,   72.3322,  257.8509,\n",
            "           23.5308]])\n",
            "label:  tensor([ 1., 10.])\n",
            "average reconstruction error: 28464.152976\n"
          ]
        }
      ],
      "source": [
        "# Set VAE to evaluation mode\n",
        "vae.eval()\n",
        "\n",
        "# As we did in training mode, simply store the loss and number of batches\n",
        "# Now we only need one number because we don't have epochs\n",
        "test_loss, number_of_batches = 0.0, 0\n",
        "learned_logmu1, learned_logmu2, learned_logvar1, learned_logvar2, learned_pi, learned_a, learned_b = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "for test_images, area, current in trainloader:\n",
        "\n",
        "  # Do not track gradients\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Send images to the GPU/CPU\n",
        "    test_images = test_images.to(device)\n",
        "    # Feed images through the VAE to obtain their reconstruction\n",
        "    beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi, a, b = vae(test_images, area, current)\n",
        "\n",
        "    # Compute reconstruction loss\n",
        "    loss, mse_loss = vae_loss(images, beta, T63, logmu1, logmu2, logvar1, logvar2, latent_pi)\n",
        "\n",
        "    print(\"beta: \", beta)\n",
        "    print(\"T63: \", T63)\n",
        "    print(\"label: \", area)\n",
        "\n",
        "    # Cumulative loss & Number of batches\n",
        "    test_loss += loss.item()\n",
        "    learned_logmu1 += torch.sum(logmu1)\n",
        "    learned_logmu2 += torch.sum(logmu2)\n",
        "    learned_logvar1 += torch.sum(logvar1)\n",
        "    learned_logvar2 += torch.sum(logvar2)\n",
        "    learned_pi += torch.sum(latent_pi)\n",
        "    learned_a += torch.sum(a)\n",
        "    learned_b += torch.sum(b)\n",
        "    number_of_batches += 1\n",
        "\n",
        "# Now divide by number of batches to get average loss per batch\n",
        "test_loss /= number_of_batches\n",
        "learned_logmu1 /= number_of_batches*input_size*batch_size\n",
        "learned_logmu2 /= number_of_batches*input_size*batch_size\n",
        "learned_logvar1 /= number_of_batches*input_size*batch_size\n",
        "learned_logvar2 /= number_of_batches*input_size*batch_size\n",
        "learned_pi /= number_of_batches*input_size*batch_size\n",
        "learned_a /= number_of_batches*input_size*batch_size\n",
        "learned_b /= number_of_batches*input_size*batch_size\n",
        "learned_mu1=torch.exp(learned_logmu1).item()\n",
        "learned_mu2=torch.exp(learned_logmu2).item()\n",
        "learned_var1=torch.exp(learned_logvar1).item()\n",
        "learned_var2=torch.exp(learned_logvar2).item()\n",
        "learned_pi = learned_pi.item()\n",
        "print('average reconstruction error: %f' % (test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcgC0ulGH1R6",
        "outputId": "7f850ed5-7d49-4532-c51d-886719cd37ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30.70289421081543, 29.346506118774414) (1.9689852256123737, 1.3932461841530313) (0.07373599708080292, 0.9262640029191971) tensor(0.6782) tensor(0.3776)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ernest=False\n",
        "area_choose=0\n",
        "\n",
        "if area_choose == 0:\n",
        "  generate_area=100.0\n",
        "  num_of_points=area1_point\n",
        "  censor_percent=censor_percent_0\n",
        "elif area_choose == 1:\n",
        "  generate_area=10.0\n",
        "  num_of_points=area2_point\n",
        "  censor_percent=censor_percent_1\n",
        "else:\n",
        "  generate_area=1.0\n",
        "  num_of_points=area3_point\n",
        "  censor_percent=censor_percent_2\n",
        "\n",
        "ernest_a=torch.tensor(-0.2)\n",
        "ernest_b=torch.tensor(0.07/0.3)\n",
        "ernest_std=torch.tensor(2.23)\n",
        "ernest_mean=torch.tensor(26)\n",
        "means = learned_mu1, learned_mu2\n",
        "if ernest==True:\n",
        "  stdevs = torch.log(ernest_std)\n",
        "else:\n",
        "  stdevs = learned_var1**0.5, learned_var2**0.5\n",
        "weights = learned_pi, 1-learned_pi\n",
        "print(means, stdevs, weights, learned_a, learned_b)\n",
        "def draw_gmm(means, stdevs, weights, x, label='pdf'):\n",
        "  pdfs = [p * ss.norm.pdf(x, mu, sd) for mu, sd, p in zip(means, stdevs, weights)]\n",
        "  density = np.sum(np.array(pdfs), axis=0)\n",
        "  plt.xlabel('dielectric thickness [nm]',fontdict = {'fontsize' : 25})\n",
        "  plt.ylabel('Probability Density',fontdict = {'fontsize' : 25})\n",
        "  plt.plot(x, density, label=label)\n",
        "  plt.legend(prop={'size': 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "eioq57q-OJXc",
        "outputId": "5ea5d0ed-1766-461e-a9c3-eb64f294b67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30.70289421081543, 29.346506118774414)\n",
            "tensor(0.6782)\n",
            "tensor(0.3776)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "know_means = 26,26\n",
        "know_stdevs = 2.23, 2.23\n",
        "know_weights = 1, 1\n",
        "know_x = np.arange(20., 50., 0.0001)\n",
        "plt.axvline(x=32, ls=':', label='nominal thickness', color='red')\n",
        "draw_gmm(means,stdevs, weights,know_x,label='learned distribution')\n",
        "print(means)\n",
        "print(learned_a)\n",
        "print(learned_b)\n",
        "# draw_gmm(know_means,know_stdevs, know_weights,know_x, label='Convolution-based Method')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdMuQj9lMej9"
      },
      "source": [
        "## Data Generation (Sampling Latent Space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SyyZOJ__O8D"
      },
      "outputs": [],
      "source": [
        "from reliability.Distributions import Weibull_Distribution\n",
        "from reliability.Fitters import Fit_Weibull_Mixture\n",
        "from reliability.Other_functions import histogram\n",
        "# generate data\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "  # Sample from standard normal distribution\n",
        "  total_recon = []\n",
        "  for j in range(200):\n",
        "    gmm_mu = learned_mu1*learned_pi+learned_mu2*(1-learned_pi)\n",
        "    recon_beta_list = []\n",
        "    recon_t63_list = []\n",
        "    reconstructions = []\n",
        "    eps1 = torch.randn(batch_size, input_size)\n",
        "    eps2 = torch.randn(batch_size, input_size)\n",
        "    first_z = eps1.mul(torch.exp(0.5*(learned_logvar2))).add_(learned_mu1)\n",
        "    second_z = eps2.mul(torch.exp(0.5*(learned_logvar2))).add_(learned_mu2)\n",
        "    first_z = eps1.mul(torch.exp(0.5*(learned_logvar2))).add_(learned_mu1)\n",
        "    second_z = eps2.mul(torch.exp(0.5*(learned_logvar2))).add_(learned_mu2)\n",
        "    mixed_z = torch.tensor([])\n",
        "    select = torch.rand(batch_size, 1)\n",
        "    for i in range(batch_size):\n",
        "      if select[i][0] < learned_pi:\n",
        "        mixed_z = torch.cat((mixed_z, first_z[i]), 0)\n",
        "      else:\n",
        "        mixed_z = torch.cat((mixed_z, second_z[i]), 0)\n",
        "    mixed_z = mixed_z.reshape(batch_size, input_size)\n",
        "    ernest_z=eps1.mul(torch.exp(0.5*ernest_std)).add_(ernest_mean)\n",
        "    for i in range(50):\n",
        "      # eps = torch.randn(batch_size, input_size)\n",
        "      # first_z = eps.mul(torch.exp(0.5*learned_logvar1)).add_(learned_mu1)\n",
        "      # second_z = eps.mul(torch.exp(0.5*learned_logvar2)).add_(learned_mu2)\n",
        "      # mixed_z = first_z.mul(learned_pi).add_(second_z.mul(1-learned_pi))\n",
        "      # ernest_z=eps.mul(torch.exp(0.5*ernest_std)).add_(ernest_mean)\n",
        "      if ernest == True:\n",
        "        recon_beta, recon_t63 = vae.decoder(ernest_z.view(-1, input_size), ernest_a, ernest_b, ernest_mean, torch.tensor([generate_area, generate_area]))\n",
        "      else:\n",
        "        recon_beta, recon_t63 = vae.decoder(mixed_z.view(-1, input_size), learned_a, learned_b, gmm_mu, torch.tensor([generate_area, generate_area]))\n",
        "      recon_beta_list+=recon_beta.numpy().flatten().tolist()\n",
        "      recon_t63_list+=recon_t63.numpy().flatten().tolist()\n",
        "    for i in range(len(recon_beta_list)):\n",
        "      m = Weibull_Distribution(alpha=recon_t63_list[i], beta=recon_beta_list[i]).random_samples(1)\n",
        "      reconstructions+=m.tolist()\n",
        "      # Reconstruct images from sampled latent vectors\n",
        "    total_recon.append(np.sort(np.array(reconstructions)))\n",
        "  total_recon = np.array(total_recon)\n",
        "  sidx = total_recon.argsort(axis=0)\n",
        "  total_recon = total_recon[sidx, np.arange(sidx.shape[1])]\n",
        "  left_bound = total_recon[0]\n",
        "  right_bound = total_recon[199]\n",
        "  left_bound_95 = total_recon[9]\n",
        "  right_bound_95 = total_recon[189]\n",
        "    \n",
        "\n",
        "from reliability.Probability_plotting import Weibull_probability_plot\n",
        "from reliability.Probability_plotting import plot_points\n",
        "from reliability.Probability_plotting import plotting_positions\n",
        "area1=np.sort(df.iloc[:area1_point,area_choose].values.flatten())\n",
        "left_points = np.ones(round(len(reconstructions)*censor_percent/(1+censor_percent)))*(0.2)\n",
        "uncensored_reconstruction=np.sort(np.concatenate((left_points, np.array(reconstructions))))\n",
        "left_bound=np.concatenate((left_points,left_bound))\n",
        "right_bound=np.concatenate((left_points,right_bound))\n",
        "left_bound_95 = np.concatenate((left_points,left_bound_95))\n",
        "right_bound_95 = np.concatenate((left_points,right_bound_95))\n",
        "reconstruction_results= Weibull_probability_plot(failures=uncensored_reconstruction,show_fitted_distribution=False, color='red',show_scatter_points=False)\n",
        "experiment_result= Weibull_probability_plot(failures=area1,show_fitted_distribution=False, show_scatter_points=False, color='blue')\n",
        "# plot_points(failures=uncensored_reconstruction, color='red', marker='^', s=100)\n",
        "plot_points(failures=area1, color='blue', marker='^', s=100)\n",
        "left_x, left_y=plotting_positions(failures=left_bound_95)\n",
        "right_x, right_y=plotting_positions(failures=right_bound_95)\n",
        "plt.plot(left_x, left_y, c='dodgerblue', linewidth=5)\n",
        "plt.plot(right_x, right_y, c='crimson', linewidth=5)\n",
        "\n",
        "xfill = np.sort(np.concatenate([left_x, right_x]))\n",
        "y1fill = np.interp(xfill, left_x, left_y)\n",
        "y2fill = np.interp(xfill, right_x, right_y)\n",
        "plt.fill_between(xfill, y1fill, y2fill, where=y1fill < y2fill, interpolate=True, color='dodgerblue', alpha=0.2)\n",
        "plt.fill_between(xfill, y1fill, y2fill, where=y1fill > y2fill, interpolate=True, color='crimson', alpha=0.2)\n",
        "\n",
        "plt.xlim(left=0.3)\n",
        "plt.xlabel('Time [s]',fontdict = {'fontsize' : 25})\n",
        "plt.ylabel('Fraction Failed',fontdict = {'fontsize' : 25})\n",
        "plt.title('')\n",
        "plt.legend('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_result= Weibull_probability_plot(failures=area1,show_fitted_distribution=False, show_scatter_points=False, color='blue')"
      ],
      "metadata": {
        "id": "MCTcpK5ip4_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.exp(0.5*learned_logvar1)*4)\n",
        "print(torch.exp(0.5*learned_logvar1+1.38629))\n",
        "eps = torch.randn(batch_size, input_size)\n",
        "test1 = eps.mul(torch.exp(0.5*learned_logvar1)/4).add_(learned_mu1)*4\n",
        "test2 = eps.mul(torch.exp(0.5*learned_logvar1)).add_(learned_mu1)\n",
        "print(test1)\n",
        "print(test2)\n",
        "test_beta, test_t63 = vae.decoder(mixed_z.view(-1, input_size), learned_a, learned_b, gmm_mu, torch.tensor([generate_area, generate_area]))\n",
        "print(\"test_beta: \", test_beta)\n",
        "print(\"test_t63: \", test_t63)"
      ],
      "metadata": {
        "id": "BC1hll1YP3kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gwvblHciqK6"
      },
      "outputs": [],
      "source": [
        "from reliability.Probability_plotting import Weibull_probability_plot\n",
        "from reliability.Probability_plotting import plot_points\n",
        "area1=df.iloc[:area1_point,1].values.flatten()\n",
        "left_points = np.ones(round(len(reconstructions)*censor_percent/(1+censor_percent)))*(0.2)\n",
        "uncensored_reconstruction=np.concatenate((left_points, np.array(reconstructions)))\n",
        "reconstruction_results= Weibull_probability_plot(failures=uncensored_reconstruction,show_fitted_distribution=False, color='red',show_scatter_points=False)\n",
        "experiment_result= Weibull_probability_plot(failures=area1, show_scatter_points=False, color='blue')\n",
        "plot_points(failures=uncensored_reconstruction, color='red', marker='^', s=100)\n",
        "plot_points(failures=area1, color='blue', marker='^', s=100)\n",
        "plt.xlim(left=0.3)\n",
        "plt.xlabel('Time [s]',fontdict = {'fontsize' : 25})\n",
        "plt.ylabel('Fraction Failed',fontdict = {'fontsize' : 25})\n",
        "plt.title('')\n",
        "plt.legend('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1CEjp9yv7Et"
      },
      "outputs": [],
      "source": [
        "temp1 = uncensored_reconstruction\n",
        "temp2 = area1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0QxmcRhrqP3"
      },
      "outputs": [],
      "source": [
        "reconstruction_results= Weibull_probability_plot(failures=uncensored_reconstruction,show_fitted_distribution=False, color='red',show_scatter_points=False)\n",
        "experiment_result= Weibull_probability_plot(failures=area1, show_scatter_points=False, color='orange')\n",
        "reconstruction_results2= Weibull_probability_plot(failures=temp1,show_fitted_distribution=False, color='red',show_scatter_points=False)\n",
        "experiment_result2= Weibull_probability_plot(failures=temp2, show_scatter_points=False, color='blue')\n",
        "# plot_points(failures=uncensored_reconstruction, color='black', marker='^', s=100)\n",
        "plot_points(failures=area1, color='orange', marker='^', s=100)\n",
        "# plot_points(failures=temp1, color='red', marker='^', s=100)\n",
        "plot_points(failures=temp2, color='blue', marker='^', s=100)\n",
        "plt.xlim(left=0.3)\n",
        "plt.xlabel('Time [s]',fontdict = {'fontsize' : 30})\n",
        "plt.ylabel('Fraction Failed',fontdict = {'fontsize' : 30})\n",
        "plt.title('')\n",
        "plt.legend('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrOoBR61Ngp9"
      },
      "source": [
        "## Display Latent Space\n",
        "Here we can see what the latent space looks like by visualize a grid on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9ereWK2NYS3"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  # Create empty (x, y) grid\n",
        "  latent_x = np.linspace(-1.5, 1.5, 20)\n",
        "  latent_y = np.linspace(-1.5, 1.5, 20)\n",
        "  latents = torch.FloatTensor(len(latent_x), len(latent_y), latent_dim)\n",
        "  # Fill up the grid\n",
        "  for i, lx in enumerate(latent_x):\n",
        "    for j, ly in enumerate(latent_y):\n",
        "      latents[j, i, 0] = lx\n",
        "      latents[j, i, 1] = ly\n",
        "  # Flatten the grid\n",
        "  latents = latents.view(-1, latent_dim)\n",
        "  # Send to GPU\n",
        "  latents = latents.to(device)\n",
        "  # Find their representation\n",
        "  reconstructions = vae.decoder(latents).reshape(-1, 1, 28,28)\n",
        "  reconstructions = reconstructions.cpu()\n",
        "  # Finally, plot\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  plt.imshow(np.transpose(make_grid(reconstructions.data[:400], 20, 5).clamp(0, 1).numpy(), (1, 2, 0))) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcHztFpvUplQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}